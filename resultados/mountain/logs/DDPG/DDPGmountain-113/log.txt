Num timesteps: 1000
Best mean reward: -inf - Last mean reward per episode: -14.49
Saving new best model to ./prueba.pkl
Num timesteps: 2000
Best mean reward: -14.49 - Last mean reward per episode: -19.38
Num timesteps: 3000
Best mean reward: -14.49 - Last mean reward per episode: 10.04
Saving new best model to ./prueba.pkl
Num timesteps: 4000
Best mean reward: 10.04 - Last mean reward per episode: -6.02
Num timesteps: 5000
Best mean reward: 10.04 - Last mean reward per episode: -7.69
Num timesteps: 6000
Best mean reward: 10.04 - Last mean reward per episode: -12.71
Num timesteps: 7000
Best mean reward: 10.04 - Last mean reward per episode: -15.31
Num timesteps: 8000
Best mean reward: 10.04 - Last mean reward per episode: -17.60
Num timesteps: 9000
Best mean reward: 10.04 - Last mean reward per episode: -21.26
Num timesteps: 10000
Best mean reward: 10.04 - Last mean reward per episode: -19.79
--------------------------------------
| reference_Q_mean        | 0.0801   |
| reference_Q_std         | 0.193    |
| reference_action_mean   | 0.113    |
| reference_action_std    | 0.163    |
| reference_actor_Q_mean  | 0.114    |
| reference_actor_Q_std   | 0.232    |
| rollout/Q_mean          | 0.0691   |
| rollout/actions_mean    | -0.0278  |
| rollout/actions_std     | 0.547    |
| rollout/episode_steps   | 976      |
| rollout/episodes        | 10       |
| rollout/return          | -19.8    |
| rollout/return_history  | -19.8    |
| total/duration          | 18       |
| total/episodes          | 10       |
| total/epochs            | 1        |
| total/steps             | 9998     |
| total/steps_per_second  | 554      |
| train/loss_actor        | -0.117   |
| train/loss_critic       | 0.00232  |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 11000
Best mean reward: 10.04 - Last mean reward per episode: -22.12
Num timesteps: 12000
Best mean reward: 10.04 - Last mean reward per episode: -22.19
Num timesteps: 13000
Best mean reward: 10.04 - Last mean reward per episode: -24.40
Num timesteps: 14000
Best mean reward: 10.04 - Last mean reward per episode: -23.49
Num timesteps: 15000
Best mean reward: 10.04 - Last mean reward per episode: -26.64
Num timesteps: 16000
Best mean reward: 10.04 - Last mean reward per episode: -26.48
Num timesteps: 17000
Best mean reward: 10.04 - Last mean reward per episode: -25.81
Num timesteps: 18000
Best mean reward: 10.04 - Last mean reward per episode: -26.30
Num timesteps: 19000
Best mean reward: 10.04 - Last mean reward per episode: -26.38
Num timesteps: 20000
Best mean reward: 10.04 - Last mean reward per episode: -25.70
--------------------------------------
| reference_Q_mean        | 0.222    |
| reference_Q_std         | 0.502    |
| reference_action_mean   | -0.0342  |
| reference_action_std    | 0.0691   |
| reference_actor_Q_mean  | 0.256    |
| reference_actor_Q_std   | 0.539    |
| rollout/Q_mean          | 0.11     |
| rollout/actions_mean    | -0.0454  |
| rollout/actions_std     | 0.556    |
| rollout/episode_steps   | 988      |
| rollout/episodes        | 20       |
| rollout/return          | -25.7    |
| rollout/return_history  | -25.7    |
| total/duration          | 36.7     |
| total/episodes          | 20       |
| total/epochs            | 1        |
| total/steps             | 19998    |
| total/steps_per_second  | 545      |
| train/loss_actor        | -0.188   |
| train/loss_critic       | 0.734    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 21000
Best mean reward: 10.04 - Last mean reward per episode: -25.37
Num timesteps: 22000
Best mean reward: 10.04 - Last mean reward per episode: -25.37
Num timesteps: 23000
Best mean reward: 10.04 - Last mean reward per episode: -25.38
Num timesteps: 24000
Best mean reward: 10.04 - Last mean reward per episode: -26.36
Num timesteps: 25000
Best mean reward: 10.04 - Last mean reward per episode: -28.20
Num timesteps: 26000
Best mean reward: 10.04 - Last mean reward per episode: -29.42
Num timesteps: 27000
Best mean reward: 10.04 - Last mean reward per episode: -28.72
Num timesteps: 28000
Best mean reward: 10.04 - Last mean reward per episode: -28.17
Num timesteps: 29000
Best mean reward: 10.04 - Last mean reward per episode: -28.12
Num timesteps: 30000
Best mean reward: 10.04 - Last mean reward per episode: -29.40
--------------------------------------
| reference_Q_mean        | 0.175    |
| reference_Q_std         | 0.698    |
| reference_action_mean   | 0.0531   |
| reference_action_std    | 0.105    |
| reference_actor_Q_mean  | 0.211    |
| reference_actor_Q_std   | 0.75     |
| rollout/Q_mean          | 0.126    |
| rollout/actions_mean    | -0.0927  |
| rollout/actions_std     | 0.566    |
| rollout/episode_steps   | 991      |
| rollout/episodes        | 30       |
| rollout/return          | -29.4    |
| rollout/return_history  | -29.4    |
| total/duration          | 54.3     |
| total/episodes          | 30       |
| total/epochs            | 1        |
| total/steps             | 29998    |
| total/steps_per_second  | 552      |
| train/loss_actor        | -0.241   |
| train/loss_critic       | 0.0329   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 31000
Best mean reward: 10.04 - Last mean reward per episode: -29.25
Num timesteps: 32000
Best mean reward: 10.04 - Last mean reward per episode: -30.22
Num timesteps: 33000
Best mean reward: 10.04 - Last mean reward per episode: -30.50
Num timesteps: 34000
Best mean reward: 10.04 - Last mean reward per episode: -30.99
Num timesteps: 35000
Best mean reward: 10.04 - Last mean reward per episode: -30.49
Num timesteps: 36000
Best mean reward: 10.04 - Last mean reward per episode: -30.26
Num timesteps: 37000
Best mean reward: 10.04 - Last mean reward per episode: -29.99
Num timesteps: 38000
Best mean reward: 10.04 - Last mean reward per episode: -29.91
Num timesteps: 39000
Best mean reward: 10.04 - Last mean reward per episode: -29.60
Num timesteps: 40000
Best mean reward: 10.04 - Last mean reward per episode: -27.44
--------------------------------------
| reference_Q_mean        | 0.233    |
| reference_Q_std         | 0.867    |
| reference_action_mean   | 0.198    |
| reference_action_std    | 0.0944   |
| reference_actor_Q_mean  | 0.268    |
| reference_actor_Q_std   | 0.862    |
| rollout/Q_mean          | 0.162    |
| rollout/actions_mean    | -0.0562  |
| rollout/actions_std     | 0.568    |
| rollout/episode_steps   | 988      |
| rollout/episodes        | 40       |
| rollout/return          | -27.4    |
| rollout/return_history  | -27.4    |
| total/duration          | 71.7     |
| total/episodes          | 40       |
| total/epochs            | 1        |
| total/steps             | 39998    |
| total/steps_per_second  | 558      |
| train/loss_actor        | -0.262   |
| train/loss_critic       | 0.00575  |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 41000
Best mean reward: 10.04 - Last mean reward per episode: -27.05
Num timesteps: 42000
Best mean reward: 10.04 - Last mean reward per episode: -27.34
Num timesteps: 43000
Best mean reward: 10.04 - Last mean reward per episode: -27.09
Num timesteps: 44000
Best mean reward: 10.04 - Last mean reward per episode: -24.56
Num timesteps: 45000
Best mean reward: 10.04 - Last mean reward per episode: -24.94
Num timesteps: 46000
Best mean reward: 10.04 - Last mean reward per episode: -22.60
Num timesteps: 47000
Best mean reward: 10.04 - Last mean reward per episode: -23.19
Num timesteps: 48000
Best mean reward: 10.04 - Last mean reward per episode: -24.18
Num timesteps: 49000
Best mean reward: 10.04 - Last mean reward per episode: -24.49
Num timesteps: 50000
Best mean reward: 10.04 - Last mean reward per episode: -25.42
--------------------------------------
| reference_Q_mean        | 0.287    |
| reference_Q_std         | 1.72     |
| reference_action_mean   | 0.296    |
| reference_action_std    | 0.0942   |
| reference_actor_Q_mean  | 0.318    |
| reference_actor_Q_std   | 1.71     |
| rollout/Q_mean          | 0.239    |
| rollout/actions_mean    | -0.00885 |
| rollout/actions_std     | 0.583    |
| rollout/episode_steps   | 974      |
| rollout/episodes        | 51       |
| rollout/return          | -25.4    |
| rollout/return_history  | -25.4    |
| total/duration          | 88.8     |
| total/episodes          | 51       |
| total/epochs            | 1        |
| total/steps             | 49998    |
| total/steps_per_second  | 563      |
| train/loss_actor        | -0.349   |
| train/loss_critic       | 0.0101   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 51000
Best mean reward: 10.04 - Last mean reward per episode: -25.30
Num timesteps: 52000
Best mean reward: 10.04 - Last mean reward per episode: -25.63
Num timesteps: 53000
Best mean reward: 10.04 - Last mean reward per episode: -26.09
Num timesteps: 54000
Best mean reward: 10.04 - Last mean reward per episode: -25.79
Num timesteps: 55000
Best mean reward: 10.04 - Last mean reward per episode: -25.99
Num timesteps: 56000
Best mean reward: 10.04 - Last mean reward per episode: -26.66
Num timesteps: 57000
Best mean reward: 10.04 - Last mean reward per episode: -26.54
Num timesteps: 58000
Best mean reward: 10.04 - Last mean reward per episode: -27.06
Num timesteps: 59000
Best mean reward: 10.04 - Last mean reward per episode: -25.27
Num timesteps: 60000
Best mean reward: 10.04 - Last mean reward per episode: -25.26
--------------------------------------
| reference_Q_mean        | 0.36     |
| reference_Q_std         | 2.73     |
| reference_action_mean   | -0.0175  |
| reference_action_std    | 0.157    |
| reference_actor_Q_mean  | 0.391    |
| reference_actor_Q_std   | 2.72     |
| rollout/Q_mean          | 0.255    |
| rollout/actions_mean    | 0.0473   |
| rollout/actions_std     | 0.583    |
| rollout/episode_steps   | 976      |
| rollout/episodes        | 61       |
| rollout/return          | -25.3    |
| rollout/return_history  | -25.3    |
| total/duration          | 106      |
| total/episodes          | 61       |
| total/epochs            | 1        |
| total/steps             | 59998    |
| total/steps_per_second  | 567      |
| train/loss_actor        | -0.283   |
| train/loss_critic       | 0.00705  |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 61000
Best mean reward: 10.04 - Last mean reward per episode: -25.62
Num timesteps: 62000
Best mean reward: 10.04 - Last mean reward per episode: -26.14
Num timesteps: 63000
Best mean reward: 10.04 - Last mean reward per episode: -26.23
Num timesteps: 64000
Best mean reward: 10.04 - Last mean reward per episode: -26.04
Num timesteps: 65000
Best mean reward: 10.04 - Last mean reward per episode: -25.87
Num timesteps: 66000
Best mean reward: 10.04 - Last mean reward per episode: -26.54
Num timesteps: 67000
Best mean reward: 10.04 - Last mean reward per episode: -27.29
Num timesteps: 68000
Best mean reward: 10.04 - Last mean reward per episode: -27.15
Num timesteps: 69000
Best mean reward: 10.04 - Last mean reward per episode: -27.44
Num timesteps: 70000
Best mean reward: 10.04 - Last mean reward per episode: -27.70
--------------------------------------
| reference_Q_mean        | 0.433    |
| reference_Q_std         | 3.96     |
| reference_action_mean   | -0.078   |
| reference_action_std    | 0.214    |
| reference_actor_Q_mean  | 0.469    |
| reference_actor_Q_std   | 3.96     |
| rollout/Q_mean          | 0.248    |
| rollout/actions_mean    | 0.0432   |
| rollout/actions_std     | 0.595    |
| rollout/episode_steps   | 980      |
| rollout/episodes        | 71       |
| rollout/return          | -27.7    |
| rollout/return_history  | -27.7    |
| total/duration          | 123      |
| total/episodes          | 71       |
| total/epochs            | 1        |
| total/steps             | 69998    |
| total/steps_per_second  | 570      |
| train/loss_actor        | -0.401   |
| train/loss_critic       | 0.0081   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 71000
Best mean reward: 10.04 - Last mean reward per episode: -25.13
Num timesteps: 72000
Best mean reward: 10.04 - Last mean reward per episode: -23.65
Num timesteps: 73000
Best mean reward: 10.04 - Last mean reward per episode: -22.28
Num timesteps: 74000
Best mean reward: 10.04 - Last mean reward per episode: -22.37
Num timesteps: 75000
Best mean reward: 10.04 - Last mean reward per episode: -22.72
Num timesteps: 76000
Best mean reward: 10.04 - Last mean reward per episode: -23.11
Num timesteps: 77000
Best mean reward: 10.04 - Last mean reward per episode: -22.08
Num timesteps: 78000
Best mean reward: 10.04 - Last mean reward per episode: -20.93
Num timesteps: 79000
Best mean reward: 10.04 - Last mean reward per episode: -21.06
Num timesteps: 80000
Best mean reward: 10.04 - Last mean reward per episode: -20.27
--------------------------------------
| reference_Q_mean        | 0.552    |
| reference_Q_std         | 5.05     |
| reference_action_mean   | 0.29     |
| reference_action_std    | 0.122    |
| reference_actor_Q_mean  | 0.589    |
| reference_actor_Q_std   | 5.04     |
| rollout/Q_mean          | 0.469    |
| rollout/actions_mean    | 0.0651   |
| rollout/actions_std     | 0.599    |
| rollout/episode_steps   | 951      |
| rollout/episodes        | 84       |
| rollout/return          | -20.3    |
| rollout/return_history  | -20.3    |
| total/duration          | 140      |
| total/episodes          | 84       |
| total/epochs            | 1        |
| total/steps             | 79998    |
| total/steps_per_second  | 571      |
| train/loss_actor        | -0.672   |
| train/loss_critic       | 0.0165   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 81000
Best mean reward: 10.04 - Last mean reward per episode: -21.09
Num timesteps: 82000
Best mean reward: 10.04 - Last mean reward per episode: -21.48
Num timesteps: 83000
Best mean reward: 10.04 - Last mean reward per episode: -21.78
Num timesteps: 84000
Best mean reward: 10.04 - Last mean reward per episode: -22.05
Num timesteps: 85000
Best mean reward: 10.04 - Last mean reward per episode: -22.80
Num timesteps: 86000
Best mean reward: 10.04 - Last mean reward per episode: -23.02
Num timesteps: 87000
Best mean reward: 10.04 - Last mean reward per episode: -23.06
Num timesteps: 88000
Best mean reward: 10.04 - Last mean reward per episode: -23.61
Num timesteps: 89000
Best mean reward: 10.04 - Last mean reward per episode: -23.95
Num timesteps: 90000
Best mean reward: 10.04 - Last mean reward per episode: -23.94
--------------------------------------
| reference_Q_mean        | 0.679    |
| reference_Q_std         | 5.91     |
| reference_action_mean   | 0.283    |
| reference_action_std    | 0.185    |
| reference_actor_Q_mean  | 0.729    |
| reference_actor_Q_std   | 5.91     |
| rollout/Q_mean          | 0.429    |
| rollout/actions_mean    | 0.0969   |
| rollout/actions_std     | 0.612    |
| rollout/episode_steps   | 956      |
| rollout/episodes        | 94       |
| rollout/return          | -23.9    |
| rollout/return_history  | -23.9    |
| total/duration          | 158      |
| total/episodes          | 94       |
| total/epochs            | 1        |
| total/steps             | 89998    |
| total/steps_per_second  | 569      |
| train/loss_actor        | -0.923   |
| train/loss_critic       | 0.0382   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 91000
Best mean reward: 10.04 - Last mean reward per episode: -23.86
Num timesteps: 92000
Best mean reward: 10.04 - Last mean reward per episode: -23.91
Num timesteps: 93000
Best mean reward: 10.04 - Last mean reward per episode: -23.97
Num timesteps: 94000
Best mean reward: 10.04 - Last mean reward per episode: -24.30
Num timesteps: 95000
Best mean reward: 10.04 - Last mean reward per episode: -24.39
Num timesteps: 96000
Best mean reward: 10.04 - Last mean reward per episode: -24.74
Num timesteps: 97000
Best mean reward: 10.04 - Last mean reward per episode: -24.73
Num timesteps: 98000
Best mean reward: 10.04 - Last mean reward per episode: -23.63
Num timesteps: 99000
Best mean reward: 10.04 - Last mean reward per episode: -24.97
Num timesteps: 100000
Best mean reward: 10.04 - Last mean reward per episode: -24.67
--------------------------------------
| reference_Q_mean        | 0.91     |
| reference_Q_std         | 6.41     |
| reference_action_mean   | 0.119    |
| reference_action_std    | 0.187    |
| reference_actor_Q_mean  | 0.982    |
| reference_actor_Q_std   | 6.41     |
| rollout/Q_mean          | 0.435    |
| rollout/actions_mean    | 0.103    |
| rollout/actions_std     | 0.61     |
| rollout/episode_steps   | 956      |
| rollout/episodes        | 104      |
| rollout/return          | -24      |
| rollout/return_history  | -24.7    |
| total/duration          | 178      |
| total/episodes          | 104      |
| total/epochs            | 1        |
| total/steps             | 99998    |
| total/steps_per_second  | 563      |
| train/loss_actor        | -0.763   |
| train/loss_critic       | 0.0422   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 101000
Best mean reward: 10.04 - Last mean reward per episode: -25.13
Num timesteps: 102000
Best mean reward: 10.04 - Last mean reward per episode: -24.08
Num timesteps: 103000
Best mean reward: 10.04 - Last mean reward per episode: -23.82
Num timesteps: 104000
Best mean reward: 10.04 - Last mean reward per episode: -22.75
Num timesteps: 105000
Best mean reward: 10.04 - Last mean reward per episode: -22.49
Num timesteps: 106000
Best mean reward: 10.04 - Last mean reward per episode: -23.09
Num timesteps: 107000
Best mean reward: 10.04 - Last mean reward per episode: -23.18
Num timesteps: 108000
Best mean reward: 10.04 - Last mean reward per episode: -23.58
Num timesteps: 109000
Best mean reward: 10.04 - Last mean reward per episode: -23.40
Num timesteps: 110000
Best mean reward: 10.04 - Last mean reward per episode: -22.23
--------------------------------------
| reference_Q_mean        | 1.19     |
| reference_Q_std         | 6.87     |
| reference_action_mean   | 0.0818   |
| reference_action_std    | 0.251    |
| reference_actor_Q_mean  | 1.31     |
| reference_actor_Q_std   | 6.91     |
| rollout/Q_mean          | 0.493    |
| rollout/actions_mean    | 0.112    |
| rollout/actions_std     | 0.61     |
| rollout/episode_steps   | 952      |
| rollout/episodes        | 115      |
| rollout/return          | -22.8    |
| rollout/return_history  | -22.2    |
| total/duration          | 196      |
| total/episodes          | 115      |
| total/epochs            | 1        |
| total/steps             | 109998   |
| total/steps_per_second  | 561      |
| train/loss_actor        | -1.25    |
| train/loss_critic       | 0.0547   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 111000
Best mean reward: 10.04 - Last mean reward per episode: -21.32
Num timesteps: 112000
Best mean reward: 10.04 - Last mean reward per episode: -21.57
Num timesteps: 113000
Best mean reward: 10.04 - Last mean reward per episode: -20.19
Num timesteps: 114000
Best mean reward: 10.04 - Last mean reward per episode: -20.71
Num timesteps: 115000
Best mean reward: 10.04 - Last mean reward per episode: -19.67
Num timesteps: 116000
Best mean reward: 10.04 - Last mean reward per episode: -18.90
Num timesteps: 117000
Best mean reward: 10.04 - Last mean reward per episode: -14.50
Num timesteps: 118000
Best mean reward: 10.04 - Last mean reward per episode: -13.49
Num timesteps: 119000
Best mean reward: 10.04 - Last mean reward per episode: -11.38
Num timesteps: 120000
Best mean reward: 10.04 - Last mean reward per episode: -7.13
--------------------------------------
| reference_Q_mean        | 1.78     |
| reference_Q_std         | 7.88     |
| reference_action_mean   | -0.0774  |
| reference_action_std    | 0.368    |
| reference_actor_Q_mean  | 2.07     |
| reference_actor_Q_std   | 8.11     |
| rollout/Q_mean          | 0.884    |
| rollout/actions_mean    | 0.0886   |
| rollout/actions_std     | 0.613    |
| rollout/episode_steps   | 906      |
| rollout/episodes        | 132      |
| rollout/return          | -12.7    |
| rollout/return_history  | -7.13    |
| total/duration          | 215      |
| total/episodes          | 132      |
| total/epochs            | 1        |
| total/steps             | 119998   |
| total/steps_per_second  | 558      |
| train/loss_actor        | -2.68    |
| train/loss_critic       | 0.175    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 121000
Best mean reward: 10.04 - Last mean reward per episode: -4.80
Num timesteps: 122000
Best mean reward: 10.04 - Last mean reward per episode: -1.95
Num timesteps: 123000
Best mean reward: 10.04 - Last mean reward per episode: 0.46
Num timesteps: 124000
Best mean reward: 10.04 - Last mean reward per episode: 3.87
Num timesteps: 125000
Best mean reward: 10.04 - Last mean reward per episode: 7.90
Num timesteps: 126000
Best mean reward: 10.04 - Last mean reward per episode: 13.54
Saving new best model to ./prueba.pkl
Num timesteps: 127000
Best mean reward: 13.54 - Last mean reward per episode: 18.38
Saving new best model to ./prueba.pkl
Num timesteps: 128000
Best mean reward: 18.38 - Last mean reward per episode: 30.24
Saving new best model to ./prueba.pkl
Num timesteps: 129000
Best mean reward: 30.24 - Last mean reward per episode: 37.64
Saving new best model to ./prueba.pkl
Num timesteps: 130000
Best mean reward: 37.64 - Last mean reward per episode: 46.66
Saving new best model to ./prueba.pkl
--------------------------------------
| reference_Q_mean        | 3.37     |
| reference_Q_std         | 12.1     |
| reference_action_mean   | -0.0924  |
| reference_action_std    | 0.74     |
| reference_actor_Q_mean  | 3.93     |
| reference_actor_Q_std   | 12.9     |
| rollout/Q_mean          | 2.46     |
| rollout/actions_mean    | 0.0744   |
| rollout/actions_std     | 0.623    |
| rollout/episode_steps   | 710      |
| rollout/episodes        | 183      |
| rollout/return          | 15.7     |
| rollout/return_history  | 46.7     |
| total/duration          | 233      |
| total/episodes          | 183      |
| total/epochs            | 1        |
| total/steps             | 129998   |
| total/steps_per_second  | 558      |
| train/loss_actor        | -6.81    |
| train/loss_critic       | 0.406    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 131000
Best mean reward: 46.66 - Last mean reward per episode: 57.19
Saving new best model to ./prueba.pkl
Num timesteps: 132000
Best mean reward: 57.19 - Last mean reward per episode: 70.34
Saving new best model to ./prueba.pkl
Num timesteps: 133000
Best mean reward: 70.34 - Last mean reward per episode: 75.97
Saving new best model to ./prueba.pkl
Num timesteps: 134000
Best mean reward: 75.97 - Last mean reward per episode: 82.91
Saving new best model to ./prueba.pkl
Num timesteps: 135000
Best mean reward: 82.91 - Last mean reward per episode: 88.55
Saving new best model to ./prueba.pkl
Num timesteps: 136000
Best mean reward: 88.55 - Last mean reward per episode: 90.36
Saving new best model to ./prueba.pkl
Num timesteps: 137000
Best mean reward: 90.36 - Last mean reward per episode: 91.35
Saving new best model to ./prueba.pkl
Num timesteps: 138000
Best mean reward: 91.35 - Last mean reward per episode: 91.64
Saving new best model to ./prueba.pkl
Num timesteps: 139000
Best mean reward: 91.64 - Last mean reward per episode: 91.71
Saving new best model to ./prueba.pkl
Num timesteps: 140000
Best mean reward: 91.71 - Last mean reward per episode: 91.75
Saving new best model to ./prueba.pkl
--------------------------------------
| reference_Q_mean        | 7.03     |
| reference_Q_std         | 17.3     |
| reference_action_mean   | 0.761    |
| reference_action_std    | 0.538    |
| reference_actor_Q_mean  | 8.16     |
| reference_actor_Q_std   | 18.6     |
| rollout/Q_mean          | 4.79     |
| rollout/actions_mean    | 0.102    |
| rollout/actions_std     | 0.634    |
| rollout/episode_steps   | 536      |
| rollout/episodes        | 261      |
| rollout/return          | 38.4     |
| rollout/return_history  | 91.7     |
| total/duration          | 251      |
| total/episodes          | 261      |
| total/epochs            | 1        |
| total/steps             | 139998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -17.9    |
| train/loss_critic       | 0.829    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 141000
Best mean reward: 91.75 - Last mean reward per episode: 91.80
Saving new best model to ./prueba.pkl
Num timesteps: 142000
Best mean reward: 91.80 - Last mean reward per episode: 91.68
Num timesteps: 143000
Best mean reward: 91.80 - Last mean reward per episode: 91.53
Num timesteps: 144000
Best mean reward: 91.80 - Last mean reward per episode: 91.33
Num timesteps: 145000
Best mean reward: 91.80 - Last mean reward per episode: 91.44
Num timesteps: 146000
Best mean reward: 91.80 - Last mean reward per episode: 91.58
Num timesteps: 147000
Best mean reward: 91.80 - Last mean reward per episode: 91.61
Num timesteps: 148000
Best mean reward: 91.80 - Last mean reward per episode: 91.58
Num timesteps: 149000
Best mean reward: 91.80 - Last mean reward per episode: 91.50
Num timesteps: 150000
Best mean reward: 91.80 - Last mean reward per episode: 91.56
--------------------------------------
| reference_Q_mean        | 11.4     |
| reference_Q_std         | 24.1     |
| reference_action_mean   | 0.66     |
| reference_action_std    | 0.714    |
| reference_actor_Q_mean  | 12.4     |
| reference_actor_Q_std   | 25.1     |
| rollout/Q_mean          | 7.14     |
| rollout/actions_mean    | 0.127    |
| rollout/actions_std     | 0.647    |
| rollout/episode_steps   | 430      |
| rollout/episodes        | 349      |
| rollout/return          | 51.8     |
| rollout/return_history  | 91.6     |
| total/duration          | 269      |
| total/episodes          | 349      |
| total/epochs            | 1        |
| total/steps             | 149998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -30.5    |
| train/loss_critic       | 10.4     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 151000
Best mean reward: 91.80 - Last mean reward per episode: 91.62
Num timesteps: 152000
Best mean reward: 91.80 - Last mean reward per episode: 91.65
Num timesteps: 153000
Best mean reward: 91.80 - Last mean reward per episode: 91.71
Num timesteps: 154000
Best mean reward: 91.80 - Last mean reward per episode: 91.69
Num timesteps: 155000
Best mean reward: 91.80 - Last mean reward per episode: 91.64
Num timesteps: 156000
Best mean reward: 91.80 - Last mean reward per episode: 91.61
Num timesteps: 157000
Best mean reward: 91.80 - Last mean reward per episode: 91.63
Num timesteps: 158000
Best mean reward: 91.80 - Last mean reward per episode: 91.72
Num timesteps: 159000
Best mean reward: 91.80 - Last mean reward per episode: 91.75
Num timesteps: 160000
Best mean reward: 91.80 - Last mean reward per episode: 91.75
--------------------------------------
| reference_Q_mean        | 14.6     |
| reference_Q_std         | 28.5     |
| reference_action_mean   | 0.0396   |
| reference_action_std    | 0.925    |
| reference_actor_Q_mean  | 15.4     |
| reference_actor_Q_std   | 29.1     |
| rollout/Q_mean          | 9.44     |
| rollout/actions_mean    | 0.143    |
| rollout/actions_std     | 0.659    |
| rollout/episode_steps   | 363      |
| rollout/episodes        | 441      |
| rollout/return          | 60.1     |
| rollout/return_history  | 91.7     |
| total/duration          | 287      |
| total/episodes          | 441      |
| total/epochs            | 1        |
| total/steps             | 159998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -41.2    |
| train/loss_critic       | 2.59     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 161000
Best mean reward: 91.80 - Last mean reward per episode: 91.95
Saving new best model to ./prueba.pkl
Num timesteps: 162000
Best mean reward: 91.95 - Last mean reward per episode: 92.13
Saving new best model to ./prueba.pkl
Num timesteps: 163000
Best mean reward: 92.13 - Last mean reward per episode: 92.31
Saving new best model to ./prueba.pkl
Num timesteps: 164000
Best mean reward: 92.31 - Last mean reward per episode: 92.50
Saving new best model to ./prueba.pkl
Num timesteps: 165000
Best mean reward: 92.50 - Last mean reward per episode: 91.06
Num timesteps: 166000
Best mean reward: 92.50 - Last mean reward per episode: 91.20
Num timesteps: 167000
Best mean reward: 92.50 - Last mean reward per episode: 91.43
Num timesteps: 168000
Best mean reward: 92.50 - Last mean reward per episode: 91.22
Num timesteps: 169000
Best mean reward: 92.50 - Last mean reward per episode: 91.27
Num timesteps: 170000
Best mean reward: 92.50 - Last mean reward per episode: 91.40
--------------------------------------
| reference_Q_mean        | 18.7     |
| reference_Q_std         | 30.4     |
| reference_action_mean   | -0.232   |
| reference_action_std    | 0.87     |
| reference_actor_Q_mean  | 19.6     |
| reference_actor_Q_std   | 30.2     |
| rollout/Q_mean          | 12.3     |
| rollout/actions_mean    | 0.146    |
| rollout/actions_std     | 0.671    |
| rollout/episode_steps   | 316      |
| rollout/episodes        | 538      |
| rollout/return          | 65.8     |
| rollout/return_history  | 91.4     |
| total/duration          | 305      |
| total/episodes          | 538      |
| total/epochs            | 1        |
| total/steps             | 169998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -49.2    |
| train/loss_critic       | 0.877    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 171000
Best mean reward: 92.50 - Last mean reward per episode: 91.43
Num timesteps: 172000
Best mean reward: 92.50 - Last mean reward per episode: 91.33
Num timesteps: 173000
Best mean reward: 92.50 - Last mean reward per episode: 91.06
Num timesteps: 174000
Best mean reward: 92.50 - Last mean reward per episode: 91.07
Num timesteps: 175000
Best mean reward: 92.50 - Last mean reward per episode: 91.16
Num timesteps: 176000
Best mean reward: 92.50 - Last mean reward per episode: 92.74
Saving new best model to ./prueba.pkl
Num timesteps: 177000
Best mean reward: 92.74 - Last mean reward per episode: 92.79
Saving new best model to ./prueba.pkl
Num timesteps: 178000
Best mean reward: 92.79 - Last mean reward per episode: 93.15
Saving new best model to ./prueba.pkl
Num timesteps: 179000
Best mean reward: 93.15 - Last mean reward per episode: 93.31
Saving new best model to ./prueba.pkl
Num timesteps: 180000
Best mean reward: 93.31 - Last mean reward per episode: 93.32
Saving new best model to ./prueba.pkl
--------------------------------------
| reference_Q_mean        | 24.8     |
| reference_Q_std         | 31.2     |
| reference_action_mean   | -0.445   |
| reference_action_std    | 0.76     |
| reference_actor_Q_mean  | 25.9     |
| reference_actor_Q_std   | 30.9     |
| rollout/Q_mean          | 15.6     |
| rollout/actions_mean    | 0.136    |
| rollout/actions_std     | 0.683    |
| rollout/episode_steps   | 279      |
| rollout/episodes        | 646      |
| rollout/return          | 70.4     |
| rollout/return_history  | 93.3     |
| total/duration          | 323      |
| total/episodes          | 646      |
| total/epochs            | 1        |
| total/steps             | 179998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -57.7    |
| train/loss_critic       | 0.914    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 181000
Best mean reward: 93.32 - Last mean reward per episode: 93.70
Saving new best model to ./prueba.pkl
Num timesteps: 182000
Best mean reward: 93.70 - Last mean reward per episode: 93.68
Num timesteps: 183000
Best mean reward: 93.70 - Last mean reward per episode: 93.45
Num timesteps: 184000
Best mean reward: 93.70 - Last mean reward per episode: 93.19
Num timesteps: 185000
Best mean reward: 93.70 - Last mean reward per episode: 93.09
Num timesteps: 186000
Best mean reward: 93.70 - Last mean reward per episode: 92.44
Num timesteps: 187000
Best mean reward: 93.70 - Last mean reward per episode: 91.99
Num timesteps: 188000
Best mean reward: 93.70 - Last mean reward per episode: 91.67
Num timesteps: 189000
Best mean reward: 93.70 - Last mean reward per episode: 91.33
Num timesteps: 190000
Best mean reward: 93.70 - Last mean reward per episode: 91.06
--------------------------------------
| reference_Q_mean        | 31.6     |
| reference_Q_std         | 30.8     |
| reference_action_mean   | -0.596   |
| reference_action_std    | 0.642    |
| reference_actor_Q_mean  | 32.2     |
| reference_actor_Q_std   | 30.9     |
| rollout/Q_mean          | 18.7     |
| rollout/actions_mean    | 0.113    |
| rollout/actions_std     | 0.697    |
| rollout/episode_steps   | 263      |
| rollout/episodes        | 723      |
| rollout/return          | 72.5     |
| rollout/return_history  | 91.1     |
| total/duration          | 341      |
| total/episodes          | 723      |
| total/epochs            | 1        |
| total/steps             | 189998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -65.7    |
| train/loss_critic       | 1.13     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 191000
Best mean reward: 93.70 - Last mean reward per episode: 90.79
Num timesteps: 192000
Best mean reward: 93.70 - Last mean reward per episode: 90.39
Num timesteps: 193000
Best mean reward: 93.70 - Last mean reward per episode: 90.18
Num timesteps: 194000
Best mean reward: 93.70 - Last mean reward per episode: 89.97
Num timesteps: 195000
Best mean reward: 93.70 - Last mean reward per episode: 89.72
Num timesteps: 196000
Best mean reward: 93.70 - Last mean reward per episode: 89.78
Num timesteps: 197000
Best mean reward: 93.70 - Last mean reward per episode: 89.80
Num timesteps: 198000
Best mean reward: 93.70 - Last mean reward per episode: 89.73
Num timesteps: 199000
Best mean reward: 93.70 - Last mean reward per episode: 90.49
Num timesteps: 200000
Best mean reward: 93.70 - Last mean reward per episode: 90.64
--------------------------------------
| reference_Q_mean        | 38.4     |
| reference_Q_std         | 28.4     |
| reference_action_mean   | -0.625   |
| reference_action_std    | 0.659    |
| reference_actor_Q_mean  | 40.5     |
| reference_actor_Q_std   | 28.4     |
| rollout/Q_mean          | 21.7     |
| rollout/actions_mean    | 0.0933   |
| rollout/actions_std     | 0.71     |
| rollout/episode_steps   | 248      |
| rollout/episodes        | 806      |
| rollout/return          | 74.4     |
| rollout/return_history  | 90.6     |
| total/duration          | 359      |
| total/episodes          | 806      |
| total/epochs            | 1        |
| total/steps             | 199998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -72      |
| train/loss_critic       | 0.838    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 201000
Best mean reward: 93.70 - Last mean reward per episode: 90.75
Num timesteps: 202000
Best mean reward: 93.70 - Last mean reward per episode: 91.01
Num timesteps: 203000
Best mean reward: 93.70 - Last mean reward per episode: 91.26
Num timesteps: 204000
Best mean reward: 93.70 - Last mean reward per episode: 91.40
Num timesteps: 205000
Best mean reward: 93.70 - Last mean reward per episode: 91.42
Num timesteps: 206000
Best mean reward: 93.70 - Last mean reward per episode: 91.04
Num timesteps: 207000
Best mean reward: 93.70 - Last mean reward per episode: 91.17
Num timesteps: 208000
Best mean reward: 93.70 - Last mean reward per episode: 89.58
Num timesteps: 209000
Best mean reward: 93.70 - Last mean reward per episode: 89.19
Num timesteps: 210000
Best mean reward: 93.70 - Last mean reward per episode: 89.30
--------------------------------------
| reference_Q_mean        | 50.6     |
| reference_Q_std         | 24.1     |
| reference_action_mean   | -0.635   |
| reference_action_std    | 0.682    |
| reference_actor_Q_mean  | 54.7     |
| reference_actor_Q_std   | 24.4     |
| rollout/Q_mean          | 24.4     |
| rollout/actions_mean    | 0.0723   |
| rollout/actions_std     | 0.721    |
| rollout/episode_steps   | 238      |
| rollout/episodes        | 883      |
| rollout/return          | 75.6     |
| rollout/return_history  | 89.3     |
| total/duration          | 377      |
| total/episodes          | 883      |
| total/epochs            | 1        |
| total/steps             | 209998   |
| total/steps_per_second  | 558      |
| train/loss_actor        | -76.7    |
| train/loss_critic       | 0.944    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 211000
Best mean reward: 93.70 - Last mean reward per episode: 89.47
Num timesteps: 212000
Best mean reward: 93.70 - Last mean reward per episode: 89.73
Num timesteps: 213000
Best mean reward: 93.70 - Last mean reward per episode: 89.87
Num timesteps: 214000
Best mean reward: 93.70 - Last mean reward per episode: 90.01
Num timesteps: 215000
Best mean reward: 93.70 - Last mean reward per episode: 90.04
Num timesteps: 216000
Best mean reward: 93.70 - Last mean reward per episode: 90.47
Num timesteps: 217000
Best mean reward: 93.70 - Last mean reward per episode: 91.07
Num timesteps: 218000
Best mean reward: 93.70 - Last mean reward per episode: 93.23
Num timesteps: 219000
Best mean reward: 93.70 - Last mean reward per episode: 93.27
Num timesteps: 220000
Best mean reward: 93.70 - Last mean reward per episode: 93.45
--------------------------------------
| reference_Q_mean        | 60.9     |
| reference_Q_std         | 18.4     |
| reference_action_mean   | -0.591   |
| reference_action_std    | 0.73     |
| reference_actor_Q_mean  | 66.1     |
| reference_actor_Q_std   | 20.1     |
| rollout/Q_mean          | 27       |
| rollout/actions_mean    | 0.0666   |
| rollout/actions_std     | 0.73     |
| rollout/episode_steps   | 219      |
| rollout/episodes        | 1e+03    |
| rollout/return          | 77.8     |
| rollout/return_history  | 93.5     |
| total/duration          | 395      |
| total/episodes          | 1e+03    |
| total/epochs            | 1        |
| total/steps             | 219998   |
| total/steps_per_second  | 558      |
| train/loss_actor        | -77.7    |
| train/loss_critic       | 0.587    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 221000
Best mean reward: 93.70 - Last mean reward per episode: 93.51
Num timesteps: 222000
Best mean reward: 93.70 - Last mean reward per episode: 93.46
Num timesteps: 223000
Best mean reward: 93.70 - Last mean reward per episode: 93.30
Num timesteps: 224000
Best mean reward: 93.70 - Last mean reward per episode: 93.30
Num timesteps: 225000
Best mean reward: 93.70 - Last mean reward per episode: 93.21
Num timesteps: 226000
Best mean reward: 93.70 - Last mean reward per episode: 93.21
Num timesteps: 227000
Best mean reward: 93.70 - Last mean reward per episode: 93.14
Num timesteps: 228000
Best mean reward: 93.70 - Last mean reward per episode: 93.24
Num timesteps: 229000
Best mean reward: 93.70 - Last mean reward per episode: 93.02
Num timesteps: 230000
Best mean reward: 93.70 - Last mean reward per episode: 93.03
--------------------------------------
| reference_Q_mean        | 67.5     |
| reference_Q_std         | 13.1     |
| reference_action_mean   | -0.613   |
| reference_action_std    | 0.724    |
| reference_actor_Q_mean  | 70.7     |
| reference_actor_Q_std   | 14.5     |
| rollout/Q_mean          | 29.2     |
| rollout/actions_mean    | 0.0602   |
| rollout/actions_std     | 0.737    |
| rollout/episode_steps   | 207      |
| rollout/episodes        | 1.11e+03 |
| rollout/return          | 79.2     |
| rollout/return_history  | 93       |
| total/duration          | 413      |
| total/episodes          | 1.11e+03 |
| total/epochs            | 1        |
| total/steps             | 229998   |
| total/steps_per_second  | 558      |
| train/loss_actor        | -76.6    |
| train/loss_critic       | 0.708    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 231000
Best mean reward: 93.70 - Last mean reward per episode: 93.04
Num timesteps: 232000
Best mean reward: 93.70 - Last mean reward per episode: 93.36
Num timesteps: 233000
Best mean reward: 93.70 - Last mean reward per episode: 93.12
Num timesteps: 234000
Best mean reward: 93.70 - Last mean reward per episode: 93.20
Num timesteps: 235000
Best mean reward: 93.70 - Last mean reward per episode: 93.22
Num timesteps: 236000
Best mean reward: 93.70 - Last mean reward per episode: 93.20
Num timesteps: 237000
Best mean reward: 93.70 - Last mean reward per episode: 93.36
Num timesteps: 238000
Best mean reward: 93.70 - Last mean reward per episode: 93.40
Num timesteps: 239000
Best mean reward: 93.70 - Last mean reward per episode: 93.19
Num timesteps: 240000
Best mean reward: 93.70 - Last mean reward per episode: 93.27
--------------------------------------
| reference_Q_mean        | 67.8     |
| reference_Q_std         | 10.3     |
| reference_action_mean   | -0.538   |
| reference_action_std    | 0.803    |
| reference_actor_Q_mean  | 70       |
| reference_actor_Q_std   | 11.2     |
| rollout/Q_mean          | 31.2     |
| rollout/actions_mean    | 0.0571   |
| rollout/actions_std     | 0.743    |
| rollout/episode_steps   | 196      |
| rollout/episodes        | 1.23e+03 |
| rollout/return          | 80.6     |
| rollout/return_history  | 93.3     |
| total/duration          | 430      |
| total/episodes          | 1.23e+03 |
| total/epochs            | 1        |
| total/steps             | 239998   |
| total/steps_per_second  | 558      |
| train/loss_actor        | -75.5    |
| train/loss_critic       | 0.633    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 241000
Best mean reward: 93.70 - Last mean reward per episode: 93.10
Num timesteps: 242000
Best mean reward: 93.70 - Last mean reward per episode: 93.28
Num timesteps: 243000
Best mean reward: 93.70 - Last mean reward per episode: 93.31
Num timesteps: 244000
Best mean reward: 93.70 - Last mean reward per episode: 93.33
Num timesteps: 245000
Best mean reward: 93.70 - Last mean reward per episode: 93.32
Num timesteps: 246000
Best mean reward: 93.70 - Last mean reward per episode: 93.25
Num timesteps: 247000
Best mean reward: 93.70 - Last mean reward per episode: 93.32
Num timesteps: 248000
Best mean reward: 93.70 - Last mean reward per episode: 93.30
Num timesteps: 249000
Best mean reward: 93.70 - Last mean reward per episode: 93.38
Num timesteps: 250000
Best mean reward: 93.70 - Last mean reward per episode: 93.34
--------------------------------------
| reference_Q_mean        | 66       |
| reference_Q_std         | 8.93     |
| reference_action_mean   | -0.826   |
| reference_action_std    | 0.541    |
| reference_actor_Q_mean  | 67.5     |
| reference_actor_Q_std   | 9.02     |
| rollout/Q_mean          | 33       |
| rollout/actions_mean    | 0.059    |
| rollout/actions_std     | 0.75     |
| rollout/episode_steps   | 186      |
| rollout/episodes        | 1.34e+03 |
| rollout/return          | 81.7     |
| rollout/return_history  | 93.3     |
| total/duration          | 448      |
| total/episodes          | 1.34e+03 |
| total/epochs            | 1        |
| total/steps             | 249998   |
| total/steps_per_second  | 558      |
| train/loss_actor        | -74.4    |
| train/loss_critic       | 0.704    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 251000
Best mean reward: 93.70 - Last mean reward per episode: 93.37
Num timesteps: 252000
Best mean reward: 93.70 - Last mean reward per episode: 93.35
Num timesteps: 253000
Best mean reward: 93.70 - Last mean reward per episode: 93.46
Num timesteps: 254000
Best mean reward: 93.70 - Last mean reward per episode: 93.48
Num timesteps: 255000
Best mean reward: 93.70 - Last mean reward per episode: 93.55
Num timesteps: 256000
Best mean reward: 93.70 - Last mean reward per episode: 93.56
Num timesteps: 257000
Best mean reward: 93.70 - Last mean reward per episode: 93.61
Num timesteps: 258000
Best mean reward: 93.70 - Last mean reward per episode: 93.64
Num timesteps: 259000
Best mean reward: 93.70 - Last mean reward per episode: 93.68
Num timesteps: 260000
Best mean reward: 93.70 - Last mean reward per episode: 93.64
--------------------------------------
| reference_Q_mean        | 63.7     |
| reference_Q_std         | 7.57     |
| reference_action_mean   | -0.585   |
| reference_action_std    | 0.753    |
| reference_actor_Q_mean  | 64.2     |
| reference_actor_Q_std   | 7.59     |
| rollout/Q_mean          | 34.5     |
| rollout/actions_mean    | 0.0604   |
| rollout/actions_std     | 0.754    |
| rollout/episode_steps   | 178      |
| rollout/episodes        | 1.46e+03 |
| rollout/return          | 82.7     |
| rollout/return_history  | 93.6     |
| total/duration          | 466      |
| total/episodes          | 1.46e+03 |
| total/epochs            | 1        |
| total/steps             | 259998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -73.3    |
| train/loss_critic       | 0.482    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 261000
Best mean reward: 93.70 - Last mean reward per episode: 93.67
Num timesteps: 262000
Best mean reward: 93.70 - Last mean reward per episode: 93.65
Num timesteps: 263000
Best mean reward: 93.70 - Last mean reward per episode: 93.74
Saving new best model to ./prueba.pkl
Num timesteps: 264000
Best mean reward: 93.74 - Last mean reward per episode: 93.78
Saving new best model to ./prueba.pkl
Num timesteps: 265000
Best mean reward: 93.78 - Last mean reward per episode: 93.76
Num timesteps: 266000
Best mean reward: 93.78 - Last mean reward per episode: 93.71
Num timesteps: 267000
Best mean reward: 93.78 - Last mean reward per episode: 93.67
Num timesteps: 268000
Best mean reward: 93.78 - Last mean reward per episode: 93.66
Num timesteps: 269000
Best mean reward: 93.78 - Last mean reward per episode: 93.50
Num timesteps: 270000
Best mean reward: 93.78 - Last mean reward per episode: 93.39
--------------------------------------
| reference_Q_mean        | 61.2     |
| reference_Q_std         | 6.73     |
| reference_action_mean   | -0.448   |
| reference_action_std    | 0.816    |
| reference_actor_Q_mean  | 61.5     |
| reference_actor_Q_std   | 6.69     |
| rollout/Q_mean          | 36       |
| rollout/actions_mean    | 0.0657   |
| rollout/actions_std     | 0.757    |
| rollout/episode_steps   | 172      |
| rollout/episodes        | 1.57e+03 |
| rollout/return          | 83.4     |
| rollout/return_history  | 93.4     |
| total/duration          | 484      |
| total/episodes          | 1.57e+03 |
| total/epochs            | 1        |
| total/steps             | 269998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -72.5    |
| train/loss_critic       | 0.209    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 271000
Best mean reward: 93.78 - Last mean reward per episode: 93.15
Num timesteps: 272000
Best mean reward: 93.78 - Last mean reward per episode: 93.03
Num timesteps: 273000
Best mean reward: 93.78 - Last mean reward per episode: 91.54
Num timesteps: 274000
Best mean reward: 93.78 - Last mean reward per episode: 91.62
Num timesteps: 275000
Best mean reward: 93.78 - Last mean reward per episode: 91.70
Num timesteps: 276000
Best mean reward: 93.78 - Last mean reward per episode: 91.67
Num timesteps: 277000
Best mean reward: 93.78 - Last mean reward per episode: 90.02
Num timesteps: 278000
Best mean reward: 93.78 - Last mean reward per episode: 90.06
Num timesteps: 279000
Best mean reward: 93.78 - Last mean reward per episode: 90.00
Num timesteps: 280000
Best mean reward: 93.78 - Last mean reward per episode: 90.15
--------------------------------------
| reference_Q_mean        | 59.1     |
| reference_Q_std         | 5.71     |
| reference_action_mean   | -0.431   |
| reference_action_std    | 0.797    |
| reference_actor_Q_mean  | 59.1     |
| reference_actor_Q_std   | 5.91     |
| rollout/Q_mean          | 37.2     |
| rollout/actions_mean    | 0.0651   |
| rollout/actions_std     | 0.761    |
| rollout/episode_steps   | 168      |
| rollout/episodes        | 1.66e+03 |
| rollout/return          | 83.7     |
| rollout/return_history  | 90.2     |
| total/duration          | 502      |
| total/episodes          | 1.66e+03 |
| total/epochs            | 1        |
| total/steps             | 279998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -71.3    |
| train/loss_critic       | 2.01     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 281000
Best mean reward: 93.78 - Last mean reward per episode: 90.31
Num timesteps: 282000
Best mean reward: 93.78 - Last mean reward per episode: 90.42
Num timesteps: 283000
Best mean reward: 93.78 - Last mean reward per episode: 91.94
Num timesteps: 284000
Best mean reward: 93.78 - Last mean reward per episode: 90.72
Num timesteps: 285000
Best mean reward: 93.78 - Last mean reward per episode: 90.70
Num timesteps: 286000
Best mean reward: 93.78 - Last mean reward per episode: 90.64
Num timesteps: 287000
Best mean reward: 93.78 - Last mean reward per episode: 92.32
Num timesteps: 288000
Best mean reward: 93.78 - Last mean reward per episode: 92.38
Num timesteps: 289000
Best mean reward: 93.78 - Last mean reward per episode: 92.42
Num timesteps: 290000
Best mean reward: 93.78 - Last mean reward per episode: 90.98
--------------------------------------
| reference_Q_mean        | 57       |
| reference_Q_std         | 5.88     |
| reference_action_mean   | 0.388    |
| reference_action_std    | 0.721    |
| reference_actor_Q_mean  | 57.2     |
| reference_actor_Q_std   | 5.96     |
| rollout/Q_mean          | 38.4     |
| rollout/actions_mean    | 0.0674   |
| rollout/actions_std     | 0.761    |
| rollout/episode_steps   | 166      |
| rollout/episodes        | 1.75e+03 |
| rollout/return          | 84.1     |
| rollout/return_history  | 91       |
| total/duration          | 520      |
| total/episodes          | 1.75e+03 |
| total/epochs            | 1        |
| total/steps             | 289998   |
| total/steps_per_second  | 557      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.129    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 291000
Best mean reward: 93.78 - Last mean reward per episode: 90.78
Num timesteps: 292000
Best mean reward: 93.78 - Last mean reward per episode: 90.57
Num timesteps: 293000
Best mean reward: 93.78 - Last mean reward per episode: 90.55
Num timesteps: 294000
Best mean reward: 93.78 - Last mean reward per episode: 90.22
Num timesteps: 295000
Best mean reward: 93.78 - Last mean reward per episode: 90.04
Num timesteps: 296000
Best mean reward: 93.78 - Last mean reward per episode: 87.98
Num timesteps: 297000
Best mean reward: 93.78 - Last mean reward per episode: 87.78
Num timesteps: 298000
Best mean reward: 93.78 - Last mean reward per episode: 88.80
Num timesteps: 299000
Best mean reward: 93.78 - Last mean reward per episode: 88.73
Num timesteps: 300000
Best mean reward: 93.78 - Last mean reward per episode: 88.23
--------------------------------------
| reference_Q_mean        | 55.7     |
| reference_Q_std         | 5.84     |
| reference_action_mean   | 0.497    |
| reference_action_std    | 0.734    |
| reference_actor_Q_mean  | 56.1     |
| reference_actor_Q_std   | 5.95     |
| rollout/Q_mean          | 39.2     |
| rollout/actions_mean    | 0.0777   |
| rollout/actions_std     | 0.762    |
| rollout/episode_steps   | 166      |
| rollout/episodes        | 1.81e+03 |
| rollout/return          | 84.2     |
| rollout/return_history  | 88.2     |
| total/duration          | 539      |
| total/episodes          | 1.81e+03 |
| total/epochs            | 1        |
| total/steps             | 299998   |
| total/steps_per_second  | 556      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 1.57     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 301000
Best mean reward: 93.78 - Last mean reward per episode: 88.03
Num timesteps: 302000
Best mean reward: 93.78 - Last mean reward per episode: 87.93
Num timesteps: 303000
Best mean reward: 93.78 - Last mean reward per episode: 87.64
Num timesteps: 304000
Best mean reward: 93.78 - Last mean reward per episode: 87.47
Num timesteps: 305000
Best mean reward: 93.78 - Last mean reward per episode: 87.25
Num timesteps: 306000
Best mean reward: 93.78 - Last mean reward per episode: 88.84
Num timesteps: 307000
Best mean reward: 93.78 - Last mean reward per episode: 88.91
Num timesteps: 308000
Best mean reward: 93.78 - Last mean reward per episode: 89.26
Num timesteps: 309000
Best mean reward: 93.78 - Last mean reward per episode: 91.22
Num timesteps: 310000
Best mean reward: 93.78 - Last mean reward per episode: 91.45
--------------------------------------
| reference_Q_mean        | 54.8     |
| reference_Q_std         | 6.16     |
| reference_action_mean   | 0.175    |
| reference_action_std    | 0.915    |
| reference_actor_Q_mean  | 55.2     |
| reference_actor_Q_std   | 6.27     |
| rollout/Q_mean          | 40.1     |
| rollout/actions_mean    | 0.0861   |
| rollout/actions_std     | 0.763    |
| rollout/episode_steps   | 164      |
| rollout/episodes        | 1.89e+03 |
| rollout/return          | 84.5     |
| rollout/return_history  | 91.5     |
| total/duration          | 557      |
| total/episodes          | 1.89e+03 |
| total/epochs            | 1        |
| total/steps             | 309998   |
| total/steps_per_second  | 556      |
| train/loss_actor        | -67.4    |
| train/loss_critic       | 0.121    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 311000
Best mean reward: 93.78 - Last mean reward per episode: 91.85
Num timesteps: 312000
Best mean reward: 93.78 - Last mean reward per episode: 91.88
Num timesteps: 313000
Best mean reward: 93.78 - Last mean reward per episode: 91.93
Num timesteps: 314000
Best mean reward: 93.78 - Last mean reward per episode: 92.06
Num timesteps: 315000
Best mean reward: 93.78 - Last mean reward per episode: 92.27
Num timesteps: 316000
Best mean reward: 93.78 - Last mean reward per episode: 92.36
Num timesteps: 317000
Best mean reward: 93.78 - Last mean reward per episode: 92.53
Num timesteps: 318000
Best mean reward: 93.78 - Last mean reward per episode: 92.52
Num timesteps: 319000
Best mean reward: 93.78 - Last mean reward per episode: 92.74
Num timesteps: 320000
Best mean reward: 93.78 - Last mean reward per episode: 92.82
--------------------------------------
| reference_Q_mean        | 53.8     |
| reference_Q_std         | 6.49     |
| reference_action_mean   | -0.0963  |
| reference_action_std    | 0.878    |
| reference_actor_Q_mean  | 54.1     |
| reference_actor_Q_std   | 6.57     |
| rollout/Q_mean          | 41       |
| rollout/actions_mean    | 0.0938   |
| rollout/actions_std     | 0.765    |
| rollout/episode_steps   | 161      |
| rollout/episodes        | 1.99e+03 |
| rollout/return          | 84.9     |
| rollout/return_history  | 92.8     |
| total/duration          | 575      |
| total/episodes          | 1.99e+03 |
| total/epochs            | 1        |
| total/steps             | 319998   |
| total/steps_per_second  | 556      |
| train/loss_actor        | -66.6    |
| train/loss_critic       | 0.196    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 321000
Best mean reward: 93.78 - Last mean reward per episode: 92.84
Num timesteps: 322000
Best mean reward: 93.78 - Last mean reward per episode: 92.53
Num timesteps: 323000
Best mean reward: 93.78 - Last mean reward per episode: 92.69
Num timesteps: 324000
Best mean reward: 93.78 - Last mean reward per episode: 92.64
Num timesteps: 325000
Best mean reward: 93.78 - Last mean reward per episode: 92.69
Num timesteps: 326000
Best mean reward: 93.78 - Last mean reward per episode: 92.76
Num timesteps: 327000
Best mean reward: 93.78 - Last mean reward per episode: 92.43
Num timesteps: 328000
Best mean reward: 93.78 - Last mean reward per episode: 92.51
Num timesteps: 329000
Best mean reward: 93.78 - Last mean reward per episode: 92.56
Num timesteps: 330000
Best mean reward: 93.78 - Last mean reward per episode: 92.62
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 6.73     |
| reference_action_mean   | -0.158   |
| reference_action_std    | 0.849    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.75     |
| rollout/Q_mean          | 41.8     |
| rollout/actions_mean    | 0.0947   |
| rollout/actions_std     | 0.766    |
| rollout/episode_steps   | 159      |
| rollout/episodes        | 2.08e+03 |
| rollout/return          | 85.2     |
| rollout/return_history  | 92.6     |
| total/duration          | 593      |
| total/episodes          | 2.08e+03 |
| total/epochs            | 1        |
| total/steps             | 329998   |
| total/steps_per_second  | 556      |
| train/loss_actor        | -66.2    |
| train/loss_critic       | 0.241    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 331000
Best mean reward: 93.78 - Last mean reward per episode: 92.79
Num timesteps: 332000
Best mean reward: 93.78 - Last mean reward per episode: 92.94
Num timesteps: 333000
Best mean reward: 93.78 - Last mean reward per episode: 93.12
Num timesteps: 334000
Best mean reward: 93.78 - Last mean reward per episode: 93.21
Num timesteps: 335000
Best mean reward: 93.78 - Last mean reward per episode: 93.63
Num timesteps: 336000
Best mean reward: 93.78 - Last mean reward per episode: 93.63
Num timesteps: 337000
Best mean reward: 93.78 - Last mean reward per episode: 93.54
Num timesteps: 338000
Best mean reward: 93.78 - Last mean reward per episode: 93.45
Num timesteps: 339000
Best mean reward: 93.78 - Last mean reward per episode: 93.39
Num timesteps: 340000
Best mean reward: 93.78 - Last mean reward per episode: 93.30
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 6.76     |
| reference_action_mean   | -0.547   |
| reference_action_std    | 0.777    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 6.79     |
| rollout/Q_mean          | 42.6     |
| rollout/actions_mean    | 0.0978   |
| rollout/actions_std     | 0.769    |
| rollout/episode_steps   | 155      |
| rollout/episodes        | 2.2e+03  |
| rollout/return          | 85.7     |
| rollout/return_history  | 93.3     |
| total/duration          | 611      |
| total/episodes          | 2.2e+03  |
| total/epochs            | 1        |
| total/steps             | 339998   |
| total/steps_per_second  | 556      |
| train/loss_actor        | -65.8    |
| train/loss_critic       | 0.201    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 341000
Best mean reward: 93.78 - Last mean reward per episode: 92.95
Num timesteps: 342000
Best mean reward: 93.78 - Last mean reward per episode: 93.02
Num timesteps: 343000
Best mean reward: 93.78 - Last mean reward per episode: 92.99
Num timesteps: 344000
Best mean reward: 93.78 - Last mean reward per episode: 92.88
Num timesteps: 345000
Best mean reward: 93.78 - Last mean reward per episode: 92.93
Num timesteps: 346000
Best mean reward: 93.78 - Last mean reward per episode: 92.87
Num timesteps: 347000
Best mean reward: 93.78 - Last mean reward per episode: 92.96
Num timesteps: 348000
Best mean reward: 93.78 - Last mean reward per episode: 93.01
Num timesteps: 349000
Best mean reward: 93.78 - Last mean reward per episode: 93.07
Num timesteps: 350000
Best mean reward: 93.78 - Last mean reward per episode: 93.13
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 6.74     |
| reference_action_mean   | -0.612   |
| reference_action_std    | 0.733    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 6.7      |
| rollout/Q_mean          | 43.4     |
| rollout/actions_mean    | 0.0985   |
| rollout/actions_std     | 0.772    |
| rollout/episode_steps   | 152      |
| rollout/episodes        | 2.3e+03  |
| rollout/return          | 86       |
| rollout/return_history  | 93.1     |
| total/duration          | 630      |
| total/episodes          | 2.3e+03  |
| total/epochs            | 1        |
| total/steps             | 349998   |
| total/steps_per_second  | 556      |
| train/loss_actor        | -67.6    |
| train/loss_critic       | 0.267    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 351000
Best mean reward: 93.78 - Last mean reward per episode: 93.00
Num timesteps: 352000
Best mean reward: 93.78 - Last mean reward per episode: 91.28
Num timesteps: 353000
Best mean reward: 93.78 - Last mean reward per episode: 91.27
Num timesteps: 354000
Best mean reward: 93.78 - Last mean reward per episode: 89.82
Num timesteps: 355000
Best mean reward: 93.78 - Last mean reward per episode: 89.85
Num timesteps: 356000
Best mean reward: 93.78 - Last mean reward per episode: 89.67
Num timesteps: 357000
Best mean reward: 93.78 - Last mean reward per episode: 89.86
Num timesteps: 358000
Best mean reward: 93.78 - Last mean reward per episode: 89.89
Num timesteps: 359000
Best mean reward: 93.78 - Last mean reward per episode: 89.96
Num timesteps: 360000
Best mean reward: 93.78 - Last mean reward per episode: 89.95
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 7.4      |
| reference_action_mean   | -0.529   |
| reference_action_std    | 0.776    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 7.37     |
| rollout/Q_mean          | 44.1     |
| rollout/actions_mean    | 0.0958   |
| rollout/actions_std     | 0.774    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 2.39e+03 |
| rollout/return          | 86.2     |
| rollout/return_history  | 90       |
| total/duration          | 648      |
| total/episodes          | 2.39e+03 |
| total/epochs            | 1        |
| total/steps             | 359998   |
| total/steps_per_second  | 556      |
| train/loss_actor        | -68.1    |
| train/loss_critic       | 0.263    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 361000
Best mean reward: 93.78 - Last mean reward per episode: 90.42
Num timesteps: 362000
Best mean reward: 93.78 - Last mean reward per episode: 92.20
Num timesteps: 363000
Best mean reward: 93.78 - Last mean reward per episode: 93.59
Num timesteps: 364000
Best mean reward: 93.78 - Last mean reward per episode: 93.83
Saving new best model to ./prueba.pkl
Num timesteps: 365000
Best mean reward: 93.83 - Last mean reward per episode: 93.78
Num timesteps: 366000
Best mean reward: 93.83 - Last mean reward per episode: 93.77
Num timesteps: 367000
Best mean reward: 93.83 - Last mean reward per episode: 93.75
Num timesteps: 368000
Best mean reward: 93.83 - Last mean reward per episode: 93.57
Num timesteps: 369000
Best mean reward: 93.83 - Last mean reward per episode: 93.47
Num timesteps: 370000
Best mean reward: 93.83 - Last mean reward per episode: 93.52
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 7.62     |
| reference_action_mean   | -0.626   |
| reference_action_std    | 0.704    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 7.77     |
| rollout/Q_mean          | 44.8     |
| rollout/actions_mean    | 0.097    |
| rollout/actions_std     | 0.776    |
| rollout/episode_steps   | 148      |
| rollout/episodes        | 2.5e+03  |
| rollout/return          | 86.5     |
| rollout/return_history  | 93.5     |
| total/duration          | 665      |
| total/episodes          | 2.5e+03  |
| total/epochs            | 1        |
| total/steps             | 369998   |
| total/steps_per_second  | 556      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 0.29     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 371000
Best mean reward: 93.83 - Last mean reward per episode: 93.24
Num timesteps: 372000
Best mean reward: 93.83 - Last mean reward per episode: 93.12
Num timesteps: 373000
Best mean reward: 93.83 - Last mean reward per episode: 93.17
Num timesteps: 374000
Best mean reward: 93.83 - Last mean reward per episode: 93.29
Num timesteps: 375000
Best mean reward: 93.83 - Last mean reward per episode: 93.20
Num timesteps: 376000
Best mean reward: 93.83 - Last mean reward per episode: 93.14
Num timesteps: 377000
Best mean reward: 93.83 - Last mean reward per episode: 93.04
Num timesteps: 378000
Best mean reward: 93.83 - Last mean reward per episode: 93.24
Num timesteps: 379000
Best mean reward: 93.83 - Last mean reward per episode: 93.52
Num timesteps: 380000
Best mean reward: 93.83 - Last mean reward per episode: 93.55
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 7.9      |
| reference_action_mean   | -0.592   |
| reference_action_std    | 0.733    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 8.04     |
| rollout/Q_mean          | 45.5     |
| rollout/actions_mean    | 0.0994   |
| rollout/actions_std     | 0.778    |
| rollout/episode_steps   | 145      |
| rollout/episodes        | 2.62e+03 |
| rollout/return          | 86.8     |
| rollout/return_history  | 93.6     |
| total/duration          | 683      |
| total/episodes          | 2.62e+03 |
| total/epochs            | 1        |
| total/steps             | 379998   |
| total/steps_per_second  | 556      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.281    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 381000
Best mean reward: 93.83 - Last mean reward per episode: 93.39
Num timesteps: 382000
Best mean reward: 93.83 - Last mean reward per episode: 93.43
Num timesteps: 383000
Best mean reward: 93.83 - Last mean reward per episode: 93.45
Num timesteps: 384000
Best mean reward: 93.83 - Last mean reward per episode: 93.53
Num timesteps: 385000
Best mean reward: 93.83 - Last mean reward per episode: 93.58
Num timesteps: 386000
Best mean reward: 93.83 - Last mean reward per episode: 91.82
Num timesteps: 387000
Best mean reward: 93.83 - Last mean reward per episode: 91.89
Num timesteps: 388000
Best mean reward: 93.83 - Last mean reward per episode: 90.32
Num timesteps: 389000
Best mean reward: 93.83 - Last mean reward per episode: 90.28
Num timesteps: 390000
Best mean reward: 93.83 - Last mean reward per episode: 90.33
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 7.95     |
| reference_action_mean   | -0.381   |
| reference_action_std    | 0.824    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 7.93     |
| rollout/Q_mean          | 46.1     |
| rollout/actions_mean    | 0.0959   |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 144      |
| rollout/episodes        | 2.7e+03  |
| rollout/return          | 86.9     |
| rollout/return_history  | 90.3     |
| total/duration          | 703      |
| total/episodes          | 2.7e+03  |
| total/epochs            | 1        |
| total/steps             | 389998   |
| total/steps_per_second  | 555      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.234    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 391000
Best mean reward: 93.83 - Last mean reward per episode: 90.01
Num timesteps: 392000
Best mean reward: 93.83 - Last mean reward per episode: 90.16
Num timesteps: 393000
Best mean reward: 93.83 - Last mean reward per episode: 90.14
Num timesteps: 394000
Best mean reward: 93.83 - Last mean reward per episode: 90.13
Num timesteps: 395000
Best mean reward: 93.83 - Last mean reward per episode: 90.19
Num timesteps: 396000
Best mean reward: 93.83 - Last mean reward per episode: 91.92
Num timesteps: 397000
Best mean reward: 93.83 - Last mean reward per episode: 93.57
Num timesteps: 398000
Best mean reward: 93.83 - Last mean reward per episode: 93.62
Num timesteps: 399000
Best mean reward: 93.83 - Last mean reward per episode: 93.92
Saving new best model to ./prueba.pkl
Num timesteps: 400000
Best mean reward: 93.92 - Last mean reward per episode: 94.03
Saving new best model to ./prueba.pkl
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 7.86     |
| reference_action_mean   | -0.418   |
| reference_action_std    | 0.831    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 7.88     |
| rollout/Q_mean          | 46.7     |
| rollout/actions_mean    | 0.0998   |
| rollout/actions_std     | 0.782    |
| rollout/episode_steps   | 142      |
| rollout/episodes        | 2.83e+03 |
| rollout/return          | 87.2     |
| rollout/return_history  | 94       |
| total/duration          | 722      |
| total/episodes          | 2.83e+03 |
| total/epochs            | 1        |
| total/steps             | 399998   |
| total/steps_per_second  | 554      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.379    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 401000
Best mean reward: 94.03 - Last mean reward per episode: 94.01
Num timesteps: 402000
Best mean reward: 94.03 - Last mean reward per episode: 93.97
Num timesteps: 403000
Best mean reward: 94.03 - Last mean reward per episode: 93.91
Num timesteps: 404000
Best mean reward: 94.03 - Last mean reward per episode: 93.87
Num timesteps: 405000
Best mean reward: 94.03 - Last mean reward per episode: 93.83
Num timesteps: 406000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 407000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 408000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 409000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 410000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 8.11     |
| reference_action_mean   | -0.335   |
| reference_action_std    | 0.842    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 8.02     |
| rollout/Q_mean          | 47.2     |
| rollout/actions_mean    | 0.102    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 140      |
| rollout/episodes        | 2.94e+03 |
| rollout/return          | 87.4     |
| rollout/return_history  | 93.6     |
| total/duration          | 741      |
| total/episodes          | 2.94e+03 |
| total/epochs            | 1        |
| total/steps             | 409998   |
| total/steps_per_second  | 553      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.42     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 411000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 412000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 413000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 414000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 415000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 416000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 417000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 418000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
Num timesteps: 419000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 420000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 8.36     |
| reference_action_mean   | -0.336   |
| reference_action_std    | 0.83     |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 8.06     |
| rollout/Q_mean          | 47.8     |
| rollout/actions_mean    | 0.105    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 138      |
| rollout/episodes        | 3.05e+03 |
| rollout/return          | 87.7     |
| rollout/return_history  | 93.7     |
| total/duration          | 762      |
| total/episodes          | 3.05e+03 |
| total/epochs            | 1        |
| total/steps             | 419998   |
| total/steps_per_second  | 551      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.216    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 421000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 422000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 423000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 424000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 425000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 426000
Best mean reward: 94.03 - Last mean reward per episode: 91.72
Num timesteps: 427000
Best mean reward: 94.03 - Last mean reward per episode: 91.75
Num timesteps: 428000
Best mean reward: 94.03 - Last mean reward per episode: 91.77
Num timesteps: 429000
Best mean reward: 94.03 - Last mean reward per episode: 91.89
Num timesteps: 430000
Best mean reward: 94.03 - Last mean reward per episode: 91.46
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 8.12     |
| reference_action_mean   | -0.372   |
| reference_action_std    | 0.831    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 7.63     |
| rollout/Q_mean          | 48.3     |
| rollout/actions_mean    | 0.104    |
| rollout/actions_std     | 0.785    |
| rollout/episode_steps   | 137      |
| rollout/episodes        | 3.14e+03 |
| rollout/return          | 87.8     |
| rollout/return_history  | 91.5     |
| total/duration          | 780      |
| total/episodes          | 3.14e+03 |
| total/epochs            | 1        |
| total/steps             | 429998   |
| total/steps_per_second  | 551      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.254    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 431000
Best mean reward: 94.03 - Last mean reward per episode: 91.45
Num timesteps: 432000
Best mean reward: 94.03 - Last mean reward per episode: 91.45
Num timesteps: 433000
Best mean reward: 94.03 - Last mean reward per episode: 91.46
Num timesteps: 434000
Best mean reward: 94.03 - Last mean reward per episode: 91.56
Num timesteps: 435000
Best mean reward: 94.03 - Last mean reward per episode: 91.67
Num timesteps: 436000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 437000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 438000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 439000
Best mean reward: 94.03 - Last mean reward per episode: 91.81
Num timesteps: 440000
Best mean reward: 94.03 - Last mean reward per episode: 92.16
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 8.16     |
| reference_action_mean   | -0.428   |
| reference_action_std    | 0.841    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 7.83     |
| rollout/Q_mean          | 48.7     |
| rollout/actions_mean    | 0.104    |
| rollout/actions_std     | 0.786    |
| rollout/episode_steps   | 136      |
| rollout/episodes        | 3.24e+03 |
| rollout/return          | 87.9     |
| rollout/return_history  | 92.2     |
| total/duration          | 799      |
| total/episodes          | 3.24e+03 |
| total/epochs            | 1        |
| total/steps             | 439998   |
| total/steps_per_second  | 551      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 1.28     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 441000
Best mean reward: 94.03 - Last mean reward per episode: 91.90
Num timesteps: 442000
Best mean reward: 94.03 - Last mean reward per episode: 92.05
Num timesteps: 443000
Best mean reward: 94.03 - Last mean reward per episode: 91.97
Num timesteps: 444000
Best mean reward: 94.03 - Last mean reward per episode: 91.98
Num timesteps: 445000
Best mean reward: 94.03 - Last mean reward per episode: 91.97
Num timesteps: 446000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 447000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 448000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 449000
Best mean reward: 94.03 - Last mean reward per episode: 93.85
Num timesteps: 450000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 7.92     |
| reference_action_mean   | -0.374   |
| reference_action_std    | 0.866    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.64     |
| rollout/Q_mean          | 49.2     |
| rollout/actions_mean    | 0.106    |
| rollout/actions_std     | 0.787    |
| rollout/episode_steps   | 134      |
| rollout/episodes        | 3.36e+03 |
| rollout/return          | 88.1     |
| rollout/return_history  | 93.7     |
| total/duration          | 817      |
| total/episodes          | 3.36e+03 |
| total/epochs            | 1        |
| total/steps             | 449998   |
| total/steps_per_second  | 551      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.12     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 451000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 452000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 453000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 454000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 455000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 456000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 457000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 458000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 459000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 460000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 8.09     |
| reference_action_mean   | -0.54    |
| reference_action_std    | 0.801    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.75     |
| rollout/Q_mean          | 49.7     |
| rollout/actions_mean    | 0.108    |
| rollout/actions_std     | 0.788    |
| rollout/episode_steps   | 133      |
| rollout/episodes        | 3.47e+03 |
| rollout/return          | 88.3     |
| rollout/return_history  | 93.3     |
| total/duration          | 837      |
| total/episodes          | 3.47e+03 |
| total/epochs            | 1        |
| total/steps             | 459998   |
| total/steps_per_second  | 549      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.291    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 461000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 462000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 463000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 464000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 465000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
Num timesteps: 466000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 467000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 468000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 469000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 470000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 8.21     |
| reference_action_mean   | -0.567   |
| reference_action_std    | 0.783    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 7.84     |
| rollout/Q_mean          | 50.1     |
| rollout/actions_mean    | 0.111    |
| rollout/actions_std     | 0.79     |
| rollout/episode_steps   | 131      |
| rollout/episodes        | 3.59e+03 |
| rollout/return          | 88.4     |
| rollout/return_history  | 93.6     |
| total/duration          | 858      |
| total/episodes          | 3.59e+03 |
| total/epochs            | 1        |
| total/steps             | 469998   |
| total/steps_per_second  | 548      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.307    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 471000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 472000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 473000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 474000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 475000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 476000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 477000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 478000
Best mean reward: 94.03 - Last mean reward per episode: 91.64
Num timesteps: 479000
Best mean reward: 94.03 - Last mean reward per episode: 91.60
Num timesteps: 480000
Best mean reward: 94.03 - Last mean reward per episode: 91.61
--------------------------------------
| reference_Q_mean        | 50.6     |
| reference_Q_std         | 8.32     |
| reference_action_mean   | -0.395   |
| reference_action_std    | 0.827    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 7.81     |
| rollout/Q_mean          | 50.5     |
| rollout/actions_mean    | 0.11     |
| rollout/actions_std     | 0.79     |
| rollout/episode_steps   | 131      |
| rollout/episodes        | 3.67e+03 |
| rollout/return          | 88.5     |
| rollout/return_history  | 91.6     |
| total/duration          | 877      |
| total/episodes          | 3.67e+03 |
| total/epochs            | 1        |
| total/steps             | 479998   |
| total/steps_per_second  | 547      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.281    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 481000
Best mean reward: 94.03 - Last mean reward per episode: 89.95
Num timesteps: 482000
Best mean reward: 94.03 - Last mean reward per episode: 89.91
Num timesteps: 483000
Best mean reward: 94.03 - Last mean reward per episode: 89.96
Num timesteps: 484000
Best mean reward: 94.03 - Last mean reward per episode: 89.97
Num timesteps: 485000
Best mean reward: 94.03 - Last mean reward per episode: 89.98
Num timesteps: 486000
Best mean reward: 94.03 - Last mean reward per episode: 90.15
Num timesteps: 487000
Best mean reward: 94.03 - Last mean reward per episode: 90.27
Num timesteps: 488000
Best mean reward: 94.03 - Last mean reward per episode: 90.13
Num timesteps: 489000
Best mean reward: 94.03 - Last mean reward per episode: 91.81
Num timesteps: 490000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 9.15     |
| reference_action_mean   | -0.596   |
| reference_action_std    | 0.772    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 8.67     |
| rollout/Q_mean          | 50.9     |
| rollout/actions_mean    | 0.112    |
| rollout/actions_std     | 0.792    |
| rollout/episode_steps   | 130      |
| rollout/episodes        | 3.78e+03 |
| rollout/return          | 88.6     |
| rollout/return_history  | 93.4     |
| total/duration          | 896      |
| total/episodes          | 3.78e+03 |
| total/epochs            | 1        |
| total/steps             | 489998   |
| total/steps_per_second  | 547      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 1.25     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 491000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 492000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 493000
Best mean reward: 94.03 - Last mean reward per episode: 91.91
Num timesteps: 494000
Best mean reward: 94.03 - Last mean reward per episode: 91.58
Num timesteps: 495000
Best mean reward: 94.03 - Last mean reward per episode: 91.56
Num timesteps: 496000
Best mean reward: 94.03 - Last mean reward per episode: 91.40
Num timesteps: 497000
Best mean reward: 94.03 - Last mean reward per episode: 91.41
Num timesteps: 498000
Best mean reward: 94.03 - Last mean reward per episode: 91.23
Num timesteps: 499000
Best mean reward: 94.03 - Last mean reward per episode: 91.01
Num timesteps: 500000
Best mean reward: 94.03 - Last mean reward per episode: 90.88
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 8.89     |
| reference_action_mean   | -0.607   |
| reference_action_std    | 0.774    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 8.39     |
| rollout/Q_mean          | 51.2     |
| rollout/actions_mean    | 0.109    |
| rollout/actions_std     | 0.792    |
| rollout/episode_steps   | 130      |
| rollout/episodes        | 3.86e+03 |
| rollout/return          | 88.6     |
| rollout/return_history  | 90.9     |
| total/duration          | 916      |
| total/episodes          | 3.86e+03 |
| total/epochs            | 1        |
| total/steps             | 499998   |
| total/steps_per_second  | 546      |
| train/loss_actor        | -69      |
| train/loss_critic       | 1.23     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 501000
Best mean reward: 94.03 - Last mean reward per episode: 90.98
Num timesteps: 502000
Best mean reward: 94.03 - Last mean reward per episode: 90.89
Num timesteps: 503000
Best mean reward: 94.03 - Last mean reward per episode: 90.83
Num timesteps: 504000
Best mean reward: 94.03 - Last mean reward per episode: 92.71
Num timesteps: 505000
Best mean reward: 94.03 - Last mean reward per episode: 92.65
Num timesteps: 506000
Best mean reward: 94.03 - Last mean reward per episode: 92.71
Num timesteps: 507000
Best mean reward: 94.03 - Last mean reward per episode: 92.65
Num timesteps: 508000
Best mean reward: 94.03 - Last mean reward per episode: 92.76
Num timesteps: 509000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 510000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 9.02     |
| reference_action_mean   | -0.628   |
| reference_action_std    | 0.76     |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 8.21     |
| rollout/Q_mean          | 51.6     |
| rollout/actions_mean    | 0.111    |
| rollout/actions_std     | 0.793    |
| rollout/episode_steps   | 129      |
| rollout/episodes        | 3.96e+03 |
| rollout/return          | 88.7     |
| rollout/return_history  | 92.9     |
| total/duration          | 934      |
| total/episodes          | 3.96e+03 |
| total/epochs            | 1        |
| total/steps             | 509998   |
| total/steps_per_second  | 546      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.301    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 511000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 512000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 513000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 514000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 515000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 516000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 517000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 518000
Best mean reward: 94.03 - Last mean reward per episode: 92.50
Num timesteps: 519000
Best mean reward: 94.03 - Last mean reward per episode: 92.55
Num timesteps: 520000
Best mean reward: 94.03 - Last mean reward per episode: 92.45
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 8.33     |
| reference_action_mean   | -0.705   |
| reference_action_std    | 0.691    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.57     |
| rollout/Q_mean          | 52       |
| rollout/actions_mean    | 0.111    |
| rollout/actions_std     | 0.794    |
| rollout/episode_steps   | 127      |
| rollout/episodes        | 4.08e+03 |
| rollout/return          | 88.9     |
| rollout/return_history  | 92.5     |
| total/duration          | 954      |
| total/episodes          | 4.08e+03 |
| total/epochs            | 1        |
| total/steps             | 519998   |
| total/steps_per_second  | 545      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 0.291    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 521000
Best mean reward: 94.03 - Last mean reward per episode: 92.33
Num timesteps: 522000
Best mean reward: 94.03 - Last mean reward per episode: 92.30
Num timesteps: 523000
Best mean reward: 94.03 - Last mean reward per episode: 92.27
Num timesteps: 524000
Best mean reward: 94.03 - Last mean reward per episode: 92.28
Num timesteps: 525000
Best mean reward: 94.03 - Last mean reward per episode: 92.23
Num timesteps: 526000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 527000
Best mean reward: 94.03 - Last mean reward per episode: 93.77
Num timesteps: 528000
Best mean reward: 94.03 - Last mean reward per episode: 93.99
Num timesteps: 529000
Best mean reward: 94.03 - Last mean reward per episode: 93.94
Num timesteps: 530000
Best mean reward: 94.03 - Last mean reward per episode: 93.95
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 8.22     |
| reference_action_mean   | -0.562   |
| reference_action_std    | 0.782    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.44     |
| rollout/Q_mean          | 52.3     |
| rollout/actions_mean    | 0.111    |
| rollout/actions_std     | 0.796    |
| rollout/episode_steps   | 126      |
| rollout/episodes        | 4.21e+03 |
| rollout/return          | 89       |
| rollout/return_history  | 94       |
| total/duration          | 973      |
| total/episodes          | 4.21e+03 |
| total/epochs            | 1        |
| total/steps             | 529998   |
| total/steps_per_second  | 544      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 1.16     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 531000
Best mean reward: 94.03 - Last mean reward per episode: 94.01
Num timesteps: 532000
Best mean reward: 94.03 - Last mean reward per episode: 93.91
Num timesteps: 533000
Best mean reward: 94.03 - Last mean reward per episode: 93.96
Num timesteps: 534000
Best mean reward: 94.03 - Last mean reward per episode: 93.83
Num timesteps: 535000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 536000
Best mean reward: 94.03 - Last mean reward per episode: 93.77
Num timesteps: 537000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 538000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 539000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 540000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 8.42     |
| reference_action_mean   | -0.539   |
| reference_action_std    | 0.797    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 7.87     |
| rollout/Q_mean          | 52.7     |
| rollout/actions_mean    | 0.113    |
| rollout/actions_std     | 0.797    |
| rollout/episode_steps   | 125      |
| rollout/episodes        | 4.33e+03 |
| rollout/return          | 89.1     |
| rollout/return_history  | 93.6     |
| total/duration          | 993      |
| total/episodes          | 4.33e+03 |
| total/epochs            | 1        |
| total/steps             | 539998   |
| total/steps_per_second  | 544      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.272    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 541000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 542000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 543000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 544000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 545000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 546000
Best mean reward: 94.03 - Last mean reward per episode: 93.07
Num timesteps: 547000
Best mean reward: 94.03 - Last mean reward per episode: 93.06
Num timesteps: 548000
Best mean reward: 94.03 - Last mean reward per episode: 93.04
Num timesteps: 549000
Best mean reward: 94.03 - Last mean reward per episode: 92.76
Num timesteps: 550000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 8.82     |
| reference_action_mean   | -0.438   |
| reference_action_std    | 0.88     |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 8.17     |
| rollout/Q_mean          | 53       |
| rollout/actions_mean    | 0.112    |
| rollout/actions_std     | 0.798    |
| rollout/episode_steps   | 124      |
| rollout/episodes        | 4.43e+03 |
| rollout/return          | 89.2     |
| rollout/return_history  | 92.8     |
| total/duration          | 1.01e+03 |
| total/episodes          | 4.43e+03 |
| total/epochs            | 1        |
| total/steps             | 549998   |
| total/steps_per_second  | 544      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.218    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 551000
Best mean reward: 94.03 - Last mean reward per episode: 92.90
Num timesteps: 552000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 553000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 554000
Best mean reward: 94.03 - Last mean reward per episode: 92.96
Num timesteps: 555000
Best mean reward: 94.03 - Last mean reward per episode: 92.87
Num timesteps: 556000
Best mean reward: 94.03 - Last mean reward per episode: 92.81
Num timesteps: 557000
Best mean reward: 94.03 - Last mean reward per episode: 92.69
Num timesteps: 558000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 559000
Best mean reward: 94.03 - Last mean reward per episode: 93.06
Num timesteps: 560000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 8.7      |
| reference_action_mean   | -0.62    |
| reference_action_std    | 0.754    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.49     |
| rollout/Q_mean          | 53.3     |
| rollout/actions_mean    | 0.112    |
| rollout/actions_std     | 0.8      |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 4.54e+03 |
| rollout/return          | 89.3     |
| rollout/return_history  | 93.1     |
| total/duration          | 1.03e+03 |
| total/episodes          | 4.54e+03 |
| total/epochs            | 1        |
| total/steps             | 559998   |
| total/steps_per_second  | 544      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.182    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 561000
Best mean reward: 94.03 - Last mean reward per episode: 91.63
Num timesteps: 562000
Best mean reward: 94.03 - Last mean reward per episode: 91.88
Num timesteps: 563000
Best mean reward: 94.03 - Last mean reward per episode: 91.93
Num timesteps: 564000
Best mean reward: 94.03 - Last mean reward per episode: 91.98
Num timesteps: 565000
Best mean reward: 94.03 - Last mean reward per episode: 92.00
Num timesteps: 566000
Best mean reward: 94.03 - Last mean reward per episode: 92.15
Num timesteps: 567000
Best mean reward: 94.03 - Last mean reward per episode: 92.17
Num timesteps: 568000
Best mean reward: 94.03 - Last mean reward per episode: 92.14
Num timesteps: 569000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 570000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 8.9      |
| reference_action_mean   | -0.645   |
| reference_action_std    | 0.757    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 7.43     |
| rollout/Q_mean          | 53.6     |
| rollout/actions_mean    | 0.114    |
| rollout/actions_std     | 0.801    |
| rollout/episode_steps   | 122      |
| rollout/episodes        | 4.66e+03 |
| rollout/return          | 89.4     |
| rollout/return_history  | 93.6     |
| total/duration          | 1.05e+03 |
| total/episodes          | 4.66e+03 |
| total/epochs            | 1        |
| total/steps             | 569998   |
| total/steps_per_second  | 544      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.237    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 571000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 572000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 573000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 574000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 575000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 576000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 577000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 578000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 579000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 580000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 8.9      |
| reference_action_mean   | -0.675   |
| reference_action_std    | 0.733    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.3      |
| rollout/Q_mean          | 53.9     |
| rollout/actions_mean    | 0.117    |
| rollout/actions_std     | 0.802    |
| rollout/episode_steps   | 121      |
| rollout/episodes        | 4.78e+03 |
| rollout/return          | 89.5     |
| rollout/return_history  | 93.5     |
| total/duration          | 1.07e+03 |
| total/episodes          | 4.78e+03 |
| total/epochs            | 1        |
| total/steps             | 579998   |
| total/steps_per_second  | 544      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.369    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 581000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 582000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 583000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 584000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 585000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 586000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 587000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 588000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 589000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 590000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 8.05     |
| reference_action_mean   | -0.631   |
| reference_action_std    | 0.752    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 7.65     |
| rollout/Q_mean          | 54.1     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.803    |
| rollout/episode_steps   | 121      |
| rollout/episodes        | 4.9e+03  |
| rollout/return          | 89.6     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.08e+03 |
| total/episodes          | 4.9e+03  |
| total/epochs            | 1        |
| total/steps             | 589998   |
| total/steps_per_second  | 544      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.217    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 591000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 592000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 593000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 594000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 595000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
Num timesteps: 596000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 597000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 598000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 599000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 600000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 8.31     |
| reference_action_mean   | -0.659   |
| reference_action_std    | 0.721    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 8.08     |
| rollout/Q_mean          | 54.4     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.804    |
| rollout/episode_steps   | 120      |
| rollout/episodes        | 5.02e+03 |
| rollout/return          | 89.7     |
| rollout/return_history  | 93.5     |
| total/duration          | 1.1e+03  |
| total/episodes          | 5.02e+03 |
| total/epochs            | 1        |
| total/steps             | 599998   |
| total/steps_per_second  | 543      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.219    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 601000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 602000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
Num timesteps: 603000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 604000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 605000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 606000
Best mean reward: 94.03 - Last mean reward per episode: 93.16
Num timesteps: 607000
Best mean reward: 94.03 - Last mean reward per episode: 93.20
Num timesteps: 608000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 609000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 610000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 7.93     |
| reference_action_mean   | -0.452   |
| reference_action_std    | 0.865    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 7.88     |
| rollout/Q_mean          | 54.7     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.805    |
| rollout/episode_steps   | 119      |
| rollout/episodes        | 5.13e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 93.8     |
| total/duration          | 1.13e+03 |
| total/episodes          | 5.13e+03 |
| total/epochs            | 1        |
| total/steps             | 609998   |
| total/steps_per_second  | 542      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.682    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 611000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
Num timesteps: 612000
Best mean reward: 94.03 - Last mean reward per episode: 93.81
Num timesteps: 613000
Best mean reward: 94.03 - Last mean reward per episode: 93.92
Num timesteps: 614000
Best mean reward: 94.03 - Last mean reward per episode: 93.83
Num timesteps: 615000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
Num timesteps: 616000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 617000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
Num timesteps: 618000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 619000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 620000
Best mean reward: 94.03 - Last mean reward per episode: 92.94
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 8.11     |
| reference_action_mean   | -0.424   |
| reference_action_std    | 0.894    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 8.19     |
| rollout/Q_mean          | 54.9     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.806    |
| rollout/episode_steps   | 118      |
| rollout/episodes        | 5.25e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 92.9     |
| total/duration          | 1.14e+03 |
| total/episodes          | 5.25e+03 |
| total/epochs            | 1        |
| total/steps             | 619998   |
| total/steps_per_second  | 542      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.254    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 621000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 622000
Best mean reward: 94.03 - Last mean reward per episode: 92.77
Num timesteps: 623000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 624000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 625000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 626000
Best mean reward: 94.03 - Last mean reward per episode: 91.61
Num timesteps: 627000
Best mean reward: 94.03 - Last mean reward per episode: 91.60
Num timesteps: 628000
Best mean reward: 94.03 - Last mean reward per episode: 91.03
Num timesteps: 629000
Best mean reward: 94.03 - Last mean reward per episode: 91.00
Num timesteps: 630000
Best mean reward: 94.03 - Last mean reward per episode: 90.62
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 8.12     |
| reference_action_mean   | -0.4     |
| reference_action_std    | 0.907    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 8.19     |
| rollout/Q_mean          | 55.2     |
| rollout/actions_mean    | 0.117    |
| rollout/actions_std     | 0.807    |
| rollout/episode_steps   | 118      |
| rollout/episodes        | 5.34e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 90.6     |
| total/duration          | 1.16e+03 |
| total/episodes          | 5.34e+03 |
| total/epochs            | 1        |
| total/steps             | 629998   |
| total/steps_per_second  | 542      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.227    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 631000
Best mean reward: 94.03 - Last mean reward per episode: 90.84
Num timesteps: 632000
Best mean reward: 94.03 - Last mean reward per episode: 90.82
Num timesteps: 633000
Best mean reward: 94.03 - Last mean reward per episode: 90.98
Num timesteps: 634000
Best mean reward: 94.03 - Last mean reward per episode: 91.00
Num timesteps: 635000
Best mean reward: 94.03 - Last mean reward per episode: 90.91
Num timesteps: 636000
Best mean reward: 94.03 - Last mean reward per episode: 92.61
Num timesteps: 637000
Best mean reward: 94.03 - Last mean reward per episode: 93.07
Num timesteps: 638000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 639000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 640000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 52.9     |
| reference_Q_std         | 7.22     |
| reference_action_mean   | -0.427   |
| reference_action_std    | 0.891    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 7.32     |
| rollout/Q_mean          | 55.4     |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.808    |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 5.45e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.18e+03 |
| total/episodes          | 5.45e+03 |
| total/epochs            | 1        |
| total/steps             | 639998   |
| total/steps_per_second  | 542      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.269    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 641000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 642000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 643000
Best mean reward: 94.03 - Last mean reward per episode: 93.01
Num timesteps: 644000
Best mean reward: 94.03 - Last mean reward per episode: 92.54
Num timesteps: 645000
Best mean reward: 94.03 - Last mean reward per episode: 92.58
Num timesteps: 646000
Best mean reward: 94.03 - Last mean reward per episode: 92.61
Num timesteps: 647000
Best mean reward: 94.03 - Last mean reward per episode: 92.63
Num timesteps: 648000
Best mean reward: 94.03 - Last mean reward per episode: 92.75
Num timesteps: 649000
Best mean reward: 94.03 - Last mean reward per episode: 92.77
Num timesteps: 650000
Best mean reward: 94.03 - Last mean reward per episode: 92.78
--------------------------------------
| reference_Q_mean        | 53.2     |
| reference_Q_std         | 6.77     |
| reference_action_mean   | -0.464   |
| reference_action_std    | 0.87     |
| reference_actor_Q_mean  | 53.7     |
| reference_actor_Q_std   | 6.81     |
| rollout/Q_mean          | 55.6     |
| rollout/actions_mean    | 0.117    |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 5.55e+03 |
| rollout/return          | 90       |
| rollout/return_history  | 92.8     |
| total/duration          | 1.2e+03  |
| total/episodes          | 5.55e+03 |
| total/epochs            | 1        |
| total/steps             | 649998   |
| total/steps_per_second  | 541      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.236    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 651000
Best mean reward: 94.03 - Last mean reward per episode: 92.89
Num timesteps: 652000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 653000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 654000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 655000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 656000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 657000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 658000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 659000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 660000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 7.77     |
| reference_action_mean   | -0.453   |
| reference_action_std    | 0.865    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 7.25     |
| rollout/Q_mean          | 55.9     |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 5.66e+03 |
| rollout/return          | 90       |
| rollout/return_history  | 93.3     |
| total/duration          | 1.22e+03 |
| total/episodes          | 5.66e+03 |
| total/epochs            | 1        |
| total/steps             | 659998   |
| total/steps_per_second  | 541      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.222    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 661000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 662000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 663000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 664000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 665000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 666000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 667000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 668000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 669000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 670000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 8.11     |
| reference_action_mean   | -0.533   |
| reference_action_std    | 0.819    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.3      |
| rollout/Q_mean          | 56.1     |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.811    |
| rollout/episode_steps   | 116      |
| rollout/episodes        | 5.78e+03 |
| rollout/return          | 90.1     |
| rollout/return_history  | 93.1     |
| total/duration          | 1.24e+03 |
| total/episodes          | 5.78e+03 |
| total/epochs            | 1        |
| total/steps             | 669998   |
| total/steps_per_second  | 541      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.227    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 671000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 672000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 673000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 674000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 675000
Best mean reward: 94.03 - Last mean reward per episode: 91.74
Num timesteps: 676000
Best mean reward: 94.03 - Last mean reward per episode: 91.72
Num timesteps: 677000
Best mean reward: 94.03 - Last mean reward per episode: 89.86
Num timesteps: 678000
Best mean reward: 94.03 - Last mean reward per episode: 89.92
Num timesteps: 679000
Best mean reward: 94.03 - Last mean reward per episode: 89.98
Num timesteps: 680000
Best mean reward: 94.03 - Last mean reward per episode: 90.00
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 8.46     |
| reference_action_mean   | -0.613   |
| reference_action_std    | 0.743    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.57     |
| rollout/Q_mean          | 56.3     |
| rollout/actions_mean    | 0.116    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 116      |
| rollout/episodes        | 5.88e+03 |
| rollout/return          | 90.1     |
| rollout/return_history  | 90       |
| total/duration          | 1.26e+03 |
| total/episodes          | 5.88e+03 |
| total/epochs            | 1        |
| total/steps             | 679998   |
| total/steps_per_second  | 540      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.265    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 681000
Best mean reward: 94.03 - Last mean reward per episode: 89.92
Num timesteps: 682000
Best mean reward: 94.03 - Last mean reward per episode: 89.76
Num timesteps: 683000
Best mean reward: 94.03 - Last mean reward per episode: 89.84
Num timesteps: 684000
Best mean reward: 94.03 - Last mean reward per episode: 89.90
Num timesteps: 685000
Best mean reward: 94.03 - Last mean reward per episode: 91.59
Num timesteps: 686000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 687000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 688000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 689000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 690000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 50.6     |
| reference_Q_std         | 8.56     |
| reference_action_mean   | -0.415   |
| reference_action_std    | 0.89     |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 8.13     |
| rollout/Q_mean          | 56.5     |
| rollout/actions_mean    | 0.116    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 5.99e+03 |
| rollout/return          | 90.2     |
| rollout/return_history  | 93.3     |
| total/duration          | 1.28e+03 |
| total/episodes          | 5.99e+03 |
| total/epochs            | 1        |
| total/steps             | 689998   |
| total/steps_per_second  | 540      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.306    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 691000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 692000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
Num timesteps: 693000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 694000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 695000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 696000
Best mean reward: 94.03 - Last mean reward per episode: 92.92
Num timesteps: 697000
Best mean reward: 94.03 - Last mean reward per episode: 92.94
Num timesteps: 698000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 699000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 700000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 8.07     |
| reference_action_mean   | -0.44    |
| reference_action_std    | 0.883    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 7.83     |
| rollout/Q_mean          | 56.7     |
| rollout/actions_mean    | 0.117    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 6.1e+03  |
| rollout/return          | 90.2     |
| rollout/return_history  | 93.2     |
| total/duration          | 1.3e+03  |
| total/episodes          | 6.1e+03  |
| total/epochs            | 1        |
| total/steps             | 699998   |
| total/steps_per_second  | 539      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.25     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 701000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 702000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 703000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 704000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 705000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
Num timesteps: 706000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 707000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 708000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 709000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 710000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.48     |
| reference_action_mean   | -0.373   |
| reference_action_std    | 0.914    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.4      |
| rollout/Q_mean          | 56.9     |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 114      |
| rollout/episodes        | 6.22e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.32e+03 |
| total/episodes          | 6.22e+03 |
| total/epochs            | 1        |
| total/steps             | 709998   |
| total/steps_per_second  | 538      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 2.21     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 711000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 712000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 713000
Best mean reward: 94.03 - Last mean reward per episode: 93.81
Num timesteps: 714000
Best mean reward: 94.03 - Last mean reward per episode: 93.84
Num timesteps: 715000
Best mean reward: 94.03 - Last mean reward per episode: 93.90
Num timesteps: 716000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 717000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 718000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 719000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 720000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 7.52     |
| reference_action_mean   | -0.279   |
| reference_action_std    | 0.944    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 7.48     |
| rollout/Q_mean          | 57.1     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 114      |
| rollout/episodes        | 6.34e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 93.5     |
| total/duration          | 1.34e+03 |
| total/episodes          | 6.34e+03 |
| total/epochs            | 1        |
| total/steps             | 719998   |
| total/steps_per_second  | 538      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.295    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 721000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 722000
Best mean reward: 94.03 - Last mean reward per episode: 93.20
Num timesteps: 723000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 724000
Best mean reward: 94.03 - Last mean reward per episode: 92.94
Num timesteps: 725000
Best mean reward: 94.03 - Last mean reward per episode: 92.76
Num timesteps: 726000
Best mean reward: 94.03 - Last mean reward per episode: 91.21
Num timesteps: 727000
Best mean reward: 94.03 - Last mean reward per episode: 91.49
Num timesteps: 728000
Best mean reward: 94.03 - Last mean reward per episode: 91.39
Num timesteps: 729000
Best mean reward: 94.03 - Last mean reward per episode: 90.99
Num timesteps: 730000
Best mean reward: 94.03 - Last mean reward per episode: 90.84
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.48     |
| reference_action_mean   | -0.0583  |
| reference_action_std    | 0.973    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 7.47     |
| rollout/Q_mean          | 57.2     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.815    |
| rollout/episode_steps   | 114      |
| rollout/episodes        | 6.41e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 90.8     |
| total/duration          | 1.36e+03 |
| total/episodes          | 6.41e+03 |
| total/epochs            | 1        |
| total/steps             | 729998   |
| total/steps_per_second  | 537      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.288    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 731000
Best mean reward: 94.03 - Last mean reward per episode: 90.66
Num timesteps: 732000
Best mean reward: 94.03 - Last mean reward per episode: 90.46
Num timesteps: 733000
Best mean reward: 94.03 - Last mean reward per episode: 88.94
Num timesteps: 734000
Best mean reward: 94.03 - Last mean reward per episode: 89.15
Num timesteps: 735000
Best mean reward: 94.03 - Last mean reward per episode: 89.30
Num timesteps: 736000
Best mean reward: 94.03 - Last mean reward per episode: 89.55
Num timesteps: 737000
Best mean reward: 94.03 - Last mean reward per episode: 90.89
Num timesteps: 738000
Best mean reward: 94.03 - Last mean reward per episode: 90.82
Num timesteps: 739000
Best mean reward: 94.03 - Last mean reward per episode: 91.33
Num timesteps: 740000
Best mean reward: 94.03 - Last mean reward per episode: 91.01
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 8.09     |
| reference_action_mean   | -0.342   |
| reference_action_std    | 0.923    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 8.07     |
| rollout/Q_mean          | 57.4     |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.815    |
| rollout/episode_steps   | 114      |
| rollout/episodes        | 6.50e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 91       |
| total/duration          | 1.38e+03 |
| total/episodes          | 6.50e+03 |
| total/epochs            | 1        |
| total/steps             | 739998   |
| total/steps_per_second  | 537      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.328    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 741000
Best mean reward: 94.03 - Last mean reward per episode: 91.15
Num timesteps: 742000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 743000
Best mean reward: 94.03 - Last mean reward per episode: 92.89
Num timesteps: 744000
Best mean reward: 94.03 - Last mean reward per episode: 92.90
Num timesteps: 745000
Best mean reward: 94.03 - Last mean reward per episode: 92.85
Num timesteps: 746000
Best mean reward: 94.03 - Last mean reward per episode: 92.97
Num timesteps: 747000
Best mean reward: 94.03 - Last mean reward per episode: 92.99
Num timesteps: 748000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 749000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 750000
Best mean reward: 94.03 - Last mean reward per episode: 91.86
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 8.2      |
| reference_action_mean   | -0.336   |
| reference_action_std    | 0.922    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 8.38     |
| rollout/Q_mean          | 57.5     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 6.61e+03 |
| rollout/return          | 90.4     |
| rollout/return_history  | 91.9     |
| total/duration          | 1.4e+03  |
| total/episodes          | 6.61e+03 |
| total/epochs            | 1        |
| total/steps             | 749998   |
| total/steps_per_second  | 536      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.35     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 751000
Best mean reward: 94.03 - Last mean reward per episode: 91.87
Num timesteps: 752000
Best mean reward: 94.03 - Last mean reward per episode: 91.92
Num timesteps: 753000
Best mean reward: 94.03 - Last mean reward per episode: 91.96
Num timesteps: 754000
Best mean reward: 94.03 - Last mean reward per episode: 91.85
Num timesteps: 755000
Best mean reward: 94.03 - Last mean reward per episode: 91.85
Num timesteps: 756000
Best mean reward: 94.03 - Last mean reward per episode: 91.76
Num timesteps: 757000
Best mean reward: 94.03 - Last mean reward per episode: 91.71
Num timesteps: 758000
Best mean reward: 94.03 - Last mean reward per episode: 91.80
Num timesteps: 759000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 760000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 8.36     |
| reference_action_mean   | -0.39    |
| reference_action_std    | 0.888    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 8.38     |
| rollout/Q_mean          | 57.7     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 6.72e+03 |
| rollout/return          | 90.4     |
| rollout/return_history  | 93.3     |
| total/duration          | 1.42e+03 |
| total/episodes          | 6.72e+03 |
| total/epochs            | 1        |
| total/steps             | 759998   |
| total/steps_per_second  | 535      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 1.57     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 761000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 762000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 763000
Best mean reward: 94.03 - Last mean reward per episode: 91.59
Num timesteps: 764000
Best mean reward: 94.03 - Last mean reward per episode: 91.73
Num timesteps: 765000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 766000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 767000
Best mean reward: 94.03 - Last mean reward per episode: 91.34
Num timesteps: 768000
Best mean reward: 94.03 - Last mean reward per episode: 91.28
Num timesteps: 769000
Best mean reward: 94.03 - Last mean reward per episode: 91.31
Num timesteps: 770000
Best mean reward: 94.03 - Last mean reward per episode: 91.33
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 8.36     |
| reference_action_mean   | -0.402   |
| reference_action_std    | 0.877    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 8.34     |
| rollout/Q_mean          | 57.9     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 6.82e+03 |
| rollout/return          | 90.4     |
| rollout/return_history  | 91.3     |
| total/duration          | 1.44e+03 |
| total/episodes          | 6.82e+03 |
| total/epochs            | 1        |
| total/steps             | 769998   |
| total/steps_per_second  | 535      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 1.47     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 771000
Best mean reward: 94.03 - Last mean reward per episode: 91.30
Num timesteps: 772000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 773000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 774000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 775000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 776000
Best mean reward: 94.03 - Last mean reward per episode: 93.83
Num timesteps: 777000
Best mean reward: 94.03 - Last mean reward per episode: 92.12
Num timesteps: 778000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
Num timesteps: 779000
Best mean reward: 94.03 - Last mean reward per episode: 92.17
Num timesteps: 780000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 8.2      |
| reference_action_mean   | -0.472   |
| reference_action_std    | 0.846    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 8.19     |
| rollout/Q_mean          | 58       |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 6.93e+03 |
| rollout/return          | 90.5     |
| rollout/return_history  | 92.1     |
| total/duration          | 1.46e+03 |
| total/episodes          | 6.93e+03 |
| total/epochs            | 1        |
| total/steps             | 779998   |
| total/steps_per_second  | 534      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.32     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 781000
Best mean reward: 94.03 - Last mean reward per episode: 91.99
Num timesteps: 782000
Best mean reward: 94.03 - Last mean reward per episode: 92.00
Num timesteps: 783000
Best mean reward: 94.03 - Last mean reward per episode: 91.82
Num timesteps: 784000
Best mean reward: 94.03 - Last mean reward per episode: 91.83
Num timesteps: 785000
Best mean reward: 94.03 - Last mean reward per episode: 91.80
Num timesteps: 786000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 787000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 788000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 789000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 790000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.73     |
| reference_action_mean   | -0.515   |
| reference_action_std    | 0.817    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.52     |
| rollout/Q_mean          | 58.2     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 7.05e+03 |
| rollout/return          | 90.5     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.48e+03 |
| total/episodes          | 7.05e+03 |
| total/epochs            | 1        |
| total/steps             | 789998   |
| total/steps_per_second  | 534      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 1.51     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 791000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 792000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 793000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 794000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 795000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 796000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 797000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 798000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 799000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 800000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 7.63     |
| reference_action_mean   | -0.425   |
| reference_action_std    | 0.873    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.5      |
| rollout/Q_mean          | 58.4     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 7.16e+03 |
| rollout/return          | 90.6     |
| rollout/return_history  | 93.2     |
| total/duration          | 1.5e+03  |
| total/episodes          | 7.16e+03 |
| total/epochs            | 1        |
| total/steps             | 799998   |
| total/steps_per_second  | 533      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.243    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 801000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 802000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 803000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 804000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 805000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 806000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 807000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 808000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 809000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 810000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 7.84     |
| reference_action_mean   | -0.373   |
| reference_action_std    | 0.887    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 7.56     |
| rollout/Q_mean          | 58.5     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 7.28e+03 |
| rollout/return          | 90.6     |
| rollout/return_history  | 93.7     |
| total/duration          | 1.52e+03 |
| total/episodes          | 7.28e+03 |
| total/epochs            | 1        |
| total/steps             | 809998   |
| total/steps_per_second  | 533      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.428    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 811000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 812000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 813000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 814000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 815000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 816000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 817000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 818000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 819000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 820000
Best mean reward: 94.03 - Last mean reward per episode: 93.80
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 8.09     |
| reference_action_mean   | -0.253   |
| reference_action_std    | 0.922    |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 7.36     |
| rollout/Q_mean          | 58.7     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 7.4e+03  |
| rollout/return          | 90.7     |
| rollout/return_history  | 93.8     |
| total/duration          | 1.54e+03 |
| total/episodes          | 7.4e+03  |
| total/epochs            | 1        |
| total/steps             | 819998   |
| total/steps_per_second  | 532      |
| train/loss_actor        | -71.2    |
| train/loss_critic       | 0.187    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 821000
Best mean reward: 94.03 - Last mean reward per episode: 93.90
Num timesteps: 822000
Best mean reward: 94.03 - Last mean reward per episode: 93.86
Num timesteps: 823000
Best mean reward: 94.03 - Last mean reward per episode: 93.87
Num timesteps: 824000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 825000
Best mean reward: 94.03 - Last mean reward per episode: 93.82
Num timesteps: 826000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 827000
Best mean reward: 94.03 - Last mean reward per episode: 93.80
Num timesteps: 828000
Best mean reward: 94.03 - Last mean reward per episode: 93.84
Num timesteps: 829000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 830000
Best mean reward: 94.03 - Last mean reward per episode: 93.77
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 8.49     |
| reference_action_mean   | -0.304   |
| reference_action_std    | 0.914    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 7.55     |
| rollout/Q_mean          | 58.8     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 7.52e+03 |
| rollout/return          | 90.7     |
| rollout/return_history  | 93.8     |
| total/duration          | 1.56e+03 |
| total/episodes          | 7.52e+03 |
| total/epochs            | 1        |
| total/steps             | 829998   |
| total/steps_per_second  | 532      |
| train/loss_actor        | -71.1    |
| train/loss_critic       | 0.183    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 831000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
Num timesteps: 832000
Best mean reward: 94.03 - Last mean reward per episode: 93.80
Num timesteps: 833000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 834000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 835000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 836000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 837000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 838000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 839000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 840000
Best mean reward: 94.03 - Last mean reward per episode: 91.75
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 8.25     |
| reference_action_mean   | -0.716   |
| reference_action_std    | 0.676    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.6      |
| rollout/Q_mean          | 59       |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 7.62e+03 |
| rollout/return          | 90.7     |
| rollout/return_history  | 91.7     |
| total/duration          | 1.58e+03 |
| total/episodes          | 7.62e+03 |
| total/epochs            | 1        |
| total/steps             | 839998   |
| total/steps_per_second  | 531      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.293    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 841000
Best mean reward: 94.03 - Last mean reward per episode: 91.75
Num timesteps: 842000
Best mean reward: 94.03 - Last mean reward per episode: 91.78
Num timesteps: 843000
Best mean reward: 94.03 - Last mean reward per episode: 91.82
Num timesteps: 844000
Best mean reward: 94.03 - Last mean reward per episode: 91.80
Num timesteps: 845000
Best mean reward: 94.03 - Last mean reward per episode: 91.80
Num timesteps: 846000
Best mean reward: 94.03 - Last mean reward per episode: 91.84
Num timesteps: 847000
Best mean reward: 94.03 - Last mean reward per episode: 92.11
Num timesteps: 848000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 849000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 850000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 8.11     |
| reference_action_mean   | -0.563   |
| reference_action_std    | 0.793    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.69     |
| rollout/Q_mean          | 59.1     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.821    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 7.74e+03 |
| rollout/return          | 90.8     |
| rollout/return_history  | 93.5     |
| total/duration          | 1.6e+03  |
| total/episodes          | 7.74e+03 |
| total/epochs            | 1        |
| total/steps             | 849998   |
| total/steps_per_second  | 531      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.193    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 851000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 852000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 853000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 854000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 855000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 856000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 857000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 858000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 859000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 860000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 8.05     |
| reference_action_mean   | -0.419   |
| reference_action_std    | 0.875    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.59     |
| rollout/Q_mean          | 59.2     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.821    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 7.85e+03 |
| rollout/return          | 90.8     |
| rollout/return_history  | 93.2     |
| total/duration          | 1.62e+03 |
| total/episodes          | 7.85e+03 |
| total/epochs            | 1        |
| total/steps             | 859998   |
| total/steps_per_second  | 530      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.21     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 861000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 862000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 863000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 864000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 865000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 866000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 867000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 868000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 869000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 870000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 7.88     |
| reference_action_mean   | -0.471   |
| reference_action_std    | 0.85     |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.45     |
| rollout/Q_mean          | 59.4     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.822    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 7.97e+03 |
| rollout/return          | 90.8     |
| rollout/return_history  | 93.6     |
| total/duration          | 1.64e+03 |
| total/episodes          | 7.97e+03 |
| total/epochs            | 1        |
| total/steps             | 869998   |
| total/steps_per_second  | 530      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.203    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 871000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 872000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 873000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 874000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 875000
Best mean reward: 94.03 - Last mean reward per episode: 92.58
Num timesteps: 876000
Best mean reward: 94.03 - Last mean reward per episode: 92.48
Num timesteps: 877000
Best mean reward: 94.03 - Last mean reward per episode: 92.44
Num timesteps: 878000
Best mean reward: 94.03 - Last mean reward per episode: 92.31
Num timesteps: 879000
Best mean reward: 94.03 - Last mean reward per episode: 92.34
Num timesteps: 880000
Best mean reward: 94.03 - Last mean reward per episode: 92.66
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 7.78     |
| reference_action_mean   | -0.425   |
| reference_action_std    | 0.886    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 7.45     |
| rollout/Q_mean          | 59.5     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.822    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 8.08e+03 |
| rollout/return          | 90.9     |
| rollout/return_history  | 92.7     |
| total/duration          | 1.66e+03 |
| total/episodes          | 8.08e+03 |
| total/epochs            | 1        |
| total/steps             | 879998   |
| total/steps_per_second  | 529      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.215    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 881000
Best mean reward: 94.03 - Last mean reward per episode: 92.69
Num timesteps: 882000
Best mean reward: 94.03 - Last mean reward per episode: 92.75
Num timesteps: 883000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 884000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 885000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 886000
Best mean reward: 94.03 - Last mean reward per episode: 93.89
Num timesteps: 887000
Best mean reward: 94.03 - Last mean reward per episode: 93.86
Num timesteps: 888000
Best mean reward: 94.03 - Last mean reward per episode: 93.83
Num timesteps: 889000
Best mean reward: 94.03 - Last mean reward per episode: 93.87
Num timesteps: 890000
Best mean reward: 94.03 - Last mean reward per episode: 92.39
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.35     |
| reference_action_mean   | -0.476   |
| reference_action_std    | 0.836    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 6.81     |
| rollout/Q_mean          | 59.6     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.823    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 8.19e+03 |
| rollout/return          | 90.9     |
| rollout/return_history  | 92.4     |
| total/duration          | 1.68e+03 |
| total/episodes          | 8.19e+03 |
| total/epochs            | 1        |
| total/steps             | 889998   |
| total/steps_per_second  | 529      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.251    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 891000
Best mean reward: 94.03 - Last mean reward per episode: 92.42
Num timesteps: 892000
Best mean reward: 94.03 - Last mean reward per episode: 92.53
Num timesteps: 893000
Best mean reward: 94.03 - Last mean reward per episode: 92.36
Num timesteps: 894000
Best mean reward: 94.03 - Last mean reward per episode: 92.28
Num timesteps: 895000
Best mean reward: 94.03 - Last mean reward per episode: 92.30
Num timesteps: 896000
Best mean reward: 94.03 - Last mean reward per episode: 92.28
Num timesteps: 897000
Best mean reward: 94.03 - Last mean reward per episode: 90.74
Num timesteps: 898000
Best mean reward: 94.03 - Last mean reward per episode: 90.70
Num timesteps: 899000
Best mean reward: 94.03 - Last mean reward per episode: 92.15
Num timesteps: 900000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.4      |
| reference_action_mean   | -0.458   |
| reference_action_std    | 0.846    |
| reference_actor_Q_mean  | 53.4     |
| reference_actor_Q_std   | 6.94     |
| rollout/Q_mean          | 59.8     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.823    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.3e+03  |
| rollout/return          | 90.9     |
| rollout/return_history  | 92.1     |
| total/duration          | 1.7e+03  |
| total/episodes          | 8.3e+03  |
| total/epochs            | 1        |
| total/steps             | 899998   |
| total/steps_per_second  | 528      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.143    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 901000
Best mean reward: 94.03 - Last mean reward per episode: 92.07
Num timesteps: 902000
Best mean reward: 94.03 - Last mean reward per episode: 92.06
Num timesteps: 903000
Best mean reward: 94.03 - Last mean reward per episode: 91.62
Num timesteps: 904000
Best mean reward: 94.03 - Last mean reward per episode: 91.61
Num timesteps: 905000
Best mean reward: 94.03 - Last mean reward per episode: 91.55
Num timesteps: 906000
Best mean reward: 94.03 - Last mean reward per episode: 91.42
Num timesteps: 907000
Best mean reward: 94.03 - Last mean reward per episode: 92.85
Num timesteps: 908000
Best mean reward: 94.03 - Last mean reward per episode: 92.86
Num timesteps: 909000
Best mean reward: 94.03 - Last mean reward per episode: 92.87
Num timesteps: 910000
Best mean reward: 94.03 - Last mean reward per episode: 92.88
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 7.59     |
| reference_action_mean   | -0.427   |
| reference_action_std    | 0.866    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 7.03     |
| rollout/Q_mean          | 59.9     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.824    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.41e+03 |
| rollout/return          | 90.9     |
| rollout/return_history  | 92.9     |
| total/duration          | 1.72e+03 |
| total/episodes          | 8.41e+03 |
| total/epochs            | 1        |
| total/steps             | 909998   |
| total/steps_per_second  | 528      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 1.13     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 911000
Best mean reward: 94.03 - Last mean reward per episode: 92.87
Num timesteps: 912000
Best mean reward: 94.03 - Last mean reward per episode: 91.96
Num timesteps: 913000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
Num timesteps: 914000
Best mean reward: 94.03 - Last mean reward per episode: 92.03
Num timesteps: 915000
Best mean reward: 94.03 - Last mean reward per episode: 92.06
Num timesteps: 916000
Best mean reward: 94.03 - Last mean reward per episode: 92.21
Num timesteps: 917000
Best mean reward: 94.03 - Last mean reward per episode: 92.24
Num timesteps: 918000
Best mean reward: 94.03 - Last mean reward per episode: 92.20
Num timesteps: 919000
Best mean reward: 94.03 - Last mean reward per episode: 92.14
Num timesteps: 920000
Best mean reward: 94.03 - Last mean reward per episode: 92.12
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 8.15     |
| reference_action_mean   | -0.452   |
| reference_action_std    | 0.851    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.63     |
| rollout/Q_mean          | 60       |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.824    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.52e+03 |
| rollout/return          | 90.9     |
| rollout/return_history  | 92.1     |
| total/duration          | 1.74e+03 |
| total/episodes          | 8.52e+03 |
| total/epochs            | 1        |
| total/steps             | 919998   |
| total/steps_per_second  | 528      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 1.2      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 921000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 922000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 923000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 924000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 925000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 926000
Best mean reward: 94.03 - Last mean reward per episode: 92.01
Num timesteps: 927000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
Num timesteps: 928000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
Num timesteps: 929000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
Num timesteps: 930000
Best mean reward: 94.03 - Last mean reward per episode: 92.12
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 7.91     |
| reference_action_mean   | -0.412   |
| reference_action_std    | 0.879    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 7.52     |
| rollout/Q_mean          | 60.1     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.63e+03 |
| rollout/return          | 91       |
| rollout/return_history  | 92.1     |
| total/duration          | 1.76e+03 |
| total/episodes          | 8.63e+03 |
| total/epochs            | 1        |
| total/steps             | 929998   |
| total/steps_per_second  | 527      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 1.06     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 931000
Best mean reward: 94.03 - Last mean reward per episode: 92.00
Num timesteps: 932000
Best mean reward: 94.03 - Last mean reward per episode: 91.98
Num timesteps: 933000
Best mean reward: 94.03 - Last mean reward per episode: 91.95
Num timesteps: 934000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 935000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 936000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 937000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 938000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 939000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 940000
Best mean reward: 94.03 - Last mean reward per episode: 93.16
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 8.01     |
| reference_action_mean   | -0.384   |
| reference_action_std    | 0.869    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 7.32     |
| rollout/Q_mean          | 60.2     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.74e+03 |
| rollout/return          | 91       |
| rollout/return_history  | 93.2     |
| total/duration          | 1.78e+03 |
| total/episodes          | 8.74e+03 |
| total/epochs            | 1        |
| total/steps             | 939998   |
| total/steps_per_second  | 527      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.776    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 941000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 942000
Best mean reward: 94.03 - Last mean reward per episode: 92.89
Num timesteps: 943000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 944000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 945000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 946000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 947000
Best mean reward: 94.03 - Last mean reward per episode: 93.20
Num timesteps: 948000
Best mean reward: 94.03 - Last mean reward per episode: 91.70
Num timesteps: 949000
Best mean reward: 94.03 - Last mean reward per episode: 91.97
Num timesteps: 950000
Best mean reward: 94.03 - Last mean reward per episode: 92.28
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 7.66     |
| reference_action_mean   | -0.438   |
| reference_action_std    | 0.841    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 6.93     |
| rollout/Q_mean          | 60.3     |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 8.84e+03 |
| rollout/return          | 91       |
| rollout/return_history  | 92.3     |
| total/duration          | 1.8e+03  |
| total/episodes          | 8.84e+03 |
| total/epochs            | 1        |
| total/steps             | 949998   |
| total/steps_per_second  | 527      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 1.51     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 951000
Best mean reward: 94.03 - Last mean reward per episode: 92.26
Num timesteps: 952000
Best mean reward: 94.03 - Last mean reward per episode: 92.22
Num timesteps: 953000
Best mean reward: 94.03 - Last mean reward per episode: 92.21
Num timesteps: 954000
Best mean reward: 94.03 - Last mean reward per episode: 92.21
Num timesteps: 955000
Best mean reward: 94.03 - Last mean reward per episode: 92.21
Num timesteps: 956000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 957000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 958000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 959000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 960000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 7.73     |
| reference_action_mean   | -0.256   |
| reference_action_std    | 0.888    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.09     |
| rollout/Q_mean          | 60.4     |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.826    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 8.96e+03 |
| rollout/return          | 91       |
| rollout/return_history  | 93.6     |
| total/duration          | 1.82e+03 |
| total/episodes          | 8.96e+03 |
| total/epochs            | 1        |
| total/steps             | 959998   |
| total/steps_per_second  | 526      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 1.57     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 961000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 962000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 963000
Best mean reward: 94.03 - Last mean reward per episode: 93.20
Num timesteps: 964000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 965000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 966000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 967000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 968000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 969000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 970000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 7.54     |
| reference_action_mean   | -0.205   |
| reference_action_std    | 0.904    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.12     |
| rollout/Q_mean          | 60.5     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.826    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 9.08e+03 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.5     |
| total/duration          | 1.84e+03 |
| total/episodes          | 9.08e+03 |
| total/epochs            | 1        |
| total/steps             | 969998   |
| total/steps_per_second  | 526      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 0.262    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 971000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 972000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 973000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 974000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 975000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 976000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 977000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 978000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 979000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 980000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 7.59     |
| reference_action_mean   | -0.297   |
| reference_action_std    | 0.888    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 6.96     |
| rollout/Q_mean          | 60.6     |
| rollout/actions_mean    | 0.119    |
| rollout/actions_std     | 0.826    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 9.19e+03 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.2     |
| total/duration          | 1.86e+03 |
| total/episodes          | 9.19e+03 |
| total/epochs            | 1        |
| total/steps             | 979998   |
| total/steps_per_second  | 526      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.141    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 981000
Best mean reward: 94.03 - Last mean reward per episode: 93.01
Num timesteps: 982000
Best mean reward: 94.03 - Last mean reward per episode: 92.92
Num timesteps: 983000
Best mean reward: 94.03 - Last mean reward per episode: 92.82
Num timesteps: 984000
Best mean reward: 94.03 - Last mean reward per episode: 92.73
Num timesteps: 985000
Best mean reward: 94.03 - Last mean reward per episode: 92.73
Num timesteps: 986000
Best mean reward: 94.03 - Last mean reward per episode: 92.80
Num timesteps: 987000
Best mean reward: 94.03 - Last mean reward per episode: 92.91
Num timesteps: 988000
Best mean reward: 94.03 - Last mean reward per episode: 93.07
Num timesteps: 989000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 990000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 7.89     |
| reference_action_mean   | -0.432   |
| reference_action_std    | 0.869    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 7.24     |
| rollout/Q_mean          | 60.7     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.827    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 9.3e+03  |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.88e+03 |
| total/episodes          | 9.3e+03  |
| total/epochs            | 1        |
| total/steps             | 989998   |
| total/steps_per_second  | 526      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.182    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 991000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 992000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 993000
Best mean reward: 94.03 - Last mean reward per episode: 91.83
Num timesteps: 994000
Best mean reward: 94.03 - Last mean reward per episode: 91.70
Num timesteps: 995000
Best mean reward: 94.03 - Last mean reward per episode: 91.71
Num timesteps: 996000
Best mean reward: 94.03 - Last mean reward per episode: 91.94
Num timesteps: 997000
Best mean reward: 94.03 - Last mean reward per episode: 91.98
Num timesteps: 998000
Best mean reward: 94.03 - Last mean reward per episode: 91.80
Num timesteps: 999000
Best mean reward: 94.03 - Last mean reward per episode: 91.80
Num timesteps: 1000000
Best mean reward: 94.03 - Last mean reward per episode: 91.71
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 7.78     |
| reference_action_mean   | -0.378   |
| reference_action_std    | 0.903    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 6.97     |
| rollout/Q_mean          | 60.8     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.827    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 9.41e+03 |
| rollout/return          | 91.1     |
| rollout/return_history  | 91.7     |
| total/duration          | 1.9e+03  |
| total/episodes          | 9.41e+03 |
| total/epochs            | 1        |
| total/steps             | 999998   |
| total/steps_per_second  | 525      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.124    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1001000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1002000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1003000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1004000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1005000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 1006000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1007000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1008000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 1009000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1010000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 7.62     |
| reference_action_mean   | -0.586   |
| reference_action_std    | 0.779    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.26     |
| rollout/Q_mean          | 60.9     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.828    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 9.54e+03 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.92e+03 |
| total/episodes          | 9.54e+03 |
| total/epochs            | 1        |
| total/steps             | 1009998  |
| total/steps_per_second  | 525      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.151    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1011000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1012000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1013000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1014000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1015000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 1016000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 1017000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 1018000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 1019000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 1020000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 7.69     |
| reference_action_mean   | -0.334   |
| reference_action_std    | 0.909    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.13     |
| rollout/Q_mean          | 61       |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.828    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 9.66e+03 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.6     |
| total/duration          | 1.94e+03 |
| total/episodes          | 9.66e+03 |
| total/epochs            | 1        |
| total/steps             | 1019998  |
| total/steps_per_second  | 525      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 1.06     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1021000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1022000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1023000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 1024000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 1025000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1026000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1027000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 1028000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 1029000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 1030000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 7.59     |
| reference_action_mean   | -0.514   |
| reference_action_std    | 0.831    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.18     |
| rollout/Q_mean          | 61.1     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 9.77e+03 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.3     |
| total/duration          | 1.96e+03 |
| total/episodes          | 9.77e+03 |
| total/epochs            | 1        |
| total/steps             | 1029998  |
| total/steps_per_second  | 525      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.174    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1031000
Best mean reward: 94.03 - Last mean reward per episode: 93.20
Num timesteps: 1032000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 1033000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 1034000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 1035000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 1036000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1037000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1038000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 1039000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 1040000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 7.96     |
| reference_action_mean   | -0.553   |
| reference_action_std    | 0.805    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.42     |
| rollout/Q_mean          | 61.2     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 9.88e+03 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.98e+03 |
| total/episodes          | 9.88e+03 |
| total/epochs            | 1        |
| total/steps             | 1039998  |
| total/steps_per_second  | 525      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.216    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1041000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1042000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1043000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 1044000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 1045000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
Num timesteps: 1046000
Best mean reward: 94.03 - Last mean reward per episode: 92.80
Num timesteps: 1047000
Best mean reward: 94.03 - Last mean reward per episode: 92.82
Num timesteps: 1048000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 1049000
Best mean reward: 94.03 - Last mean reward per episode: 93.07
Num timesteps: 1050000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.82     |
| reference_action_mean   | -0.611   |
| reference_action_std    | 0.771    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 7.07     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.121    |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 9.98e+03 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.1     |
| total/duration          | 2e+03    |
| total/episodes          | 9.98e+03 |
| total/epochs            | 1        |
| total/steps             | 1049998  |
| total/steps_per_second  | 524      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.254    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1051000
Best mean reward: 94.03 - Last mean reward per episode: 92.77
Num timesteps: 1052000
Best mean reward: 94.03 - Last mean reward per episode: 92.54
Num timesteps: 1053000
Best mean reward: 94.03 - Last mean reward per episode: 92.46
Num timesteps: 1054000
Best mean reward: 94.03 - Last mean reward per episode: 92.84
Num timesteps: 1055000
Best mean reward: 94.03 - Last mean reward per episode: 92.89
Num timesteps: 1056000
Best mean reward: 94.03 - Last mean reward per episode: 92.70
Num timesteps: 1057000
Best mean reward: 94.03 - Last mean reward per episode: 92.57
Num timesteps: 1058000
Best mean reward: 94.03 - Last mean reward per episode: 92.46
Num timesteps: 1059000
Best mean reward: 94.03 - Last mean reward per episode: 92.57
Num timesteps: 1060000
Best mean reward: 94.03 - Last mean reward per episode: 92.91
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 7.94     |
| reference_action_mean   | -0.618   |
| reference_action_std    | 0.76     |
| reference_actor_Q_mean  | 53.4     |
| reference_actor_Q_std   | 6.81     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.01e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 92.9     |
| total/duration          | 2.02e+03 |
| total/episodes          | 1.01e+04 |
| total/epochs            | 1        |
| total/steps             | 1059998  |
| total/steps_per_second  | 524      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.249    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1061000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 1062000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 1063000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 1064000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1065000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1066000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 1067000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 1068000
Best mean reward: 94.03 - Last mean reward per episode: 92.00
Num timesteps: 1069000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 1070000
Best mean reward: 94.03 - Last mean reward per episode: 91.75
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 7.68     |
| reference_action_mean   | -0.491   |
| reference_action_std    | 0.848    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.85     |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.121    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.02e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 91.8     |
| total/duration          | 2.04e+03 |
| total/episodes          | 1.02e+04 |
| total/epochs            | 1        |
| total/steps             | 1069998  |
| total/steps_per_second  | 524      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.247    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1071000
Best mean reward: 94.03 - Last mean reward per episode: 91.68
Num timesteps: 1072000
Best mean reward: 94.03 - Last mean reward per episode: 91.55
Num timesteps: 1073000
Best mean reward: 94.03 - Last mean reward per episode: 91.51
Num timesteps: 1074000
Best mean reward: 94.03 - Last mean reward per episode: 91.61
Num timesteps: 1075000
Best mean reward: 94.03 - Last mean reward per episode: 91.60
Num timesteps: 1076000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 1077000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 1078000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1079000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1080000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 7.64     |
| reference_action_mean   | 0.0377   |
| reference_action_std    | 0.965    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.22     |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.03e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.3     |
| total/duration          | 2.06e+03 |
| total/episodes          | 1.03e+04 |
| total/epochs            | 1        |
| total/steps             | 1079998  |
| total/steps_per_second  | 524      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.387    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1081000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 1082000
Best mean reward: 94.03 - Last mean reward per episode: 93.20
Num timesteps: 1083000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 1084000
Best mean reward: 94.03 - Last mean reward per episode: 92.96
Num timesteps: 1085000
Best mean reward: 94.03 - Last mean reward per episode: 92.96
Num timesteps: 1086000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
Num timesteps: 1087000
Best mean reward: 94.03 - Last mean reward per episode: 92.75
Num timesteps: 1088000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 1089000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 1090000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.24     |
| reference_action_mean   | -0.464   |
| reference_action_std    | 0.859    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.83     |
| rollout/Q_mean          | 61.6     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.04e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.3     |
| total/duration          | 2.08e+03 |
| total/episodes          | 1.04e+04 |
| total/epochs            | 1        |
| total/steps             | 1089998  |
| total/steps_per_second  | 523      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.261    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1091000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 1092000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1093000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 1094000
Best mean reward: 94.03 - Last mean reward per episode: 92.89
Num timesteps: 1095000
Best mean reward: 94.03 - Last mean reward per episode: 92.48
Num timesteps: 1096000
Best mean reward: 94.03 - Last mean reward per episode: 92.50
Num timesteps: 1097000
Best mean reward: 94.03 - Last mean reward per episode: 90.90
Num timesteps: 1098000
Best mean reward: 94.03 - Last mean reward per episode: 90.95
Num timesteps: 1099000
Best mean reward: 94.03 - Last mean reward per episode: 90.94
Num timesteps: 1100000
Best mean reward: 94.03 - Last mean reward per episode: 90.99
--------------------------------------
| reference_Q_mean        | 52.3     |
| reference_Q_std         | 7.13     |
| reference_action_mean   | -0.436   |
| reference_action_std    | 0.881    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 7.09     |
| rollout/Q_mean          | 61.7     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.831    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.05e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 91       |
| total/duration          | 2.1e+03  |
| total/episodes          | 1.05e+04 |
| total/epochs            | 1        |
| total/steps             | 1099998  |
| total/steps_per_second  | 523      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.35     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1101000
Best mean reward: 94.03 - Last mean reward per episode: 90.82
Num timesteps: 1102000
Best mean reward: 94.03 - Last mean reward per episode: 90.81
Num timesteps: 1103000
Best mean reward: 94.03 - Last mean reward per episode: 91.14
Num timesteps: 1104000
Best mean reward: 94.03 - Last mean reward per episode: 91.91
Num timesteps: 1105000
Best mean reward: 94.03 - Last mean reward per episode: 91.86
Num timesteps: 1106000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1107000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 1108000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1109000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 1110000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.42     |
| reference_action_mean   | -0.449   |
| reference_action_std    | 0.867    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.47     |
| rollout/Q_mean          | 61.7     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.831    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.06e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.6     |
| total/duration          | 2.12e+03 |
| total/episodes          | 1.06e+04 |
| total/epochs            | 1        |
| total/steps             | 1109998  |
| total/steps_per_second  | 523      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.365    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1111000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1112000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1113000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 1114000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 1115000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 1116000
Best mean reward: 94.03 - Last mean reward per episode: 92.60
Num timesteps: 1117000
Best mean reward: 94.03 - Last mean reward per episode: 92.59
Num timesteps: 1118000
Best mean reward: 94.03 - Last mean reward per episode: 92.61
Num timesteps: 1119000
Best mean reward: 94.03 - Last mean reward per episode: 92.40
Num timesteps: 1120000
Best mean reward: 94.03 - Last mean reward per episode: 92.36
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 7.56     |
| reference_action_mean   | -0.534   |
| reference_action_std    | 0.819    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.49     |
| rollout/Q_mean          | 61.8     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.831    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.07e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 92.4     |
| total/duration          | 2.14e+03 |
| total/episodes          | 1.07e+04 |
| total/epochs            | 1        |
| total/steps             | 1119998  |
| total/steps_per_second  | 523      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.272    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1121000
Best mean reward: 94.03 - Last mean reward per episode: 92.32
Num timesteps: 1122000
Best mean reward: 94.03 - Last mean reward per episode: 92.30
Num timesteps: 1123000
Best mean reward: 94.03 - Last mean reward per episode: 92.63
Num timesteps: 1124000
Best mean reward: 94.03 - Last mean reward per episode: 92.58
Num timesteps: 1125000
Best mean reward: 94.03 - Last mean reward per episode: 92.65
Num timesteps: 1126000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 1127000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 1128000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 1129000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 1130000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 7.55     |
| reference_action_mean   | -0.574   |
| reference_action_std    | 0.789    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 7.45     |
| rollout/Q_mean          | 61.9     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.09e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.3     |
| total/duration          | 2.16e+03 |
| total/episodes          | 1.09e+04 |
| total/epochs            | 1        |
| total/steps             | 1129998  |
| total/steps_per_second  | 522      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.317    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1131000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 1132000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1133000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1134000
Best mean reward: 94.03 - Last mean reward per episode: 91.71
Num timesteps: 1135000
Best mean reward: 94.03 - Last mean reward per episode: 91.67
Num timesteps: 1136000
Best mean reward: 94.03 - Last mean reward per episode: 91.63
Num timesteps: 1137000
Best mean reward: 94.03 - Last mean reward per episode: 91.60
Num timesteps: 1138000
Best mean reward: 94.03 - Last mean reward per episode: 91.57
Num timesteps: 1139000
Best mean reward: 94.03 - Last mean reward per episode: 91.28
Num timesteps: 1140000
Best mean reward: 94.03 - Last mean reward per episode: 91.27
--------------------------------------
| reference_Q_mean        | 52.6     |
| reference_Q_std         | 7.28     |
| reference_action_mean   | -0.545   |
| reference_action_std    | 0.82     |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 7.22     |
| rollout/Q_mean          | 61.9     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.09e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 91.3     |
| total/duration          | 2.18e+03 |
| total/episodes          | 1.09e+04 |
| total/epochs            | 1        |
| total/steps             | 1139998  |
| total/steps_per_second  | 522      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 2        |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1141000
Best mean reward: 94.03 - Last mean reward per episode: 91.20
Num timesteps: 1142000
Best mean reward: 94.03 - Last mean reward per episode: 91.07
Num timesteps: 1143000
Best mean reward: 94.03 - Last mean reward per episode: 91.01
Num timesteps: 1144000
Best mean reward: 94.03 - Last mean reward per episode: 89.60
Num timesteps: 1145000
Best mean reward: 94.03 - Last mean reward per episode: 91.18
Num timesteps: 1146000
Best mean reward: 94.03 - Last mean reward per episode: 90.94
Num timesteps: 1147000
Best mean reward: 94.03 - Last mean reward per episode: 90.89
Num timesteps: 1148000
Best mean reward: 94.03 - Last mean reward per episode: 90.88
Num timesteps: 1149000
Best mean reward: 94.03 - Last mean reward per episode: 90.89
Num timesteps: 1150000
Best mean reward: 94.03 - Last mean reward per episode: 91.21
--------------------------------------
| reference_Q_mean        | 52.9     |
| reference_Q_std         | 7        |
| reference_action_mean   | -0.559   |
| reference_action_std    | 0.815    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 6.99     |
| rollout/Q_mean          | 62       |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.1e+04  |
| rollout/return          | 91.4     |
| rollout/return_history  | 91.2     |
| total/duration          | 2.2e+03  |
| total/episodes          | 1.1e+04  |
| total/epochs            | 1        |
| total/steps             | 1149998  |
| total/steps_per_second  | 522      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.408    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1151000
Best mean reward: 94.03 - Last mean reward per episode: 91.29
Num timesteps: 1152000
Best mean reward: 94.03 - Last mean reward per episode: 91.38
Num timesteps: 1153000
Best mean reward: 94.03 - Last mean reward per episode: 91.42
Num timesteps: 1154000
Best mean reward: 94.03 - Last mean reward per episode: 92.73
Num timesteps: 1155000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
Num timesteps: 1156000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 1157000
Best mean reward: 94.03 - Last mean reward per episode: 93.16
Num timesteps: 1158000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
Num timesteps: 1159000
Best mean reward: 94.03 - Last mean reward per episode: 92.92
Num timesteps: 1160000
Best mean reward: 94.03 - Last mean reward per episode: 92.80
--------------------------------------
| reference_Q_mean        | 53       |
| reference_Q_std         | 7.02     |
| reference_action_mean   | -0.631   |
| reference_action_std    | 0.766    |
| reference_actor_Q_mean  | 53.7     |
| reference_actor_Q_std   | 6.88     |
| rollout/Q_mean          | 62.1     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.11e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 92.8     |
| total/duration          | 2.22e+03 |
| total/episodes          | 1.11e+04 |
| total/epochs            | 1        |
| total/steps             | 1159998  |
| total/steps_per_second  | 522      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.325    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1161000
Best mean reward: 94.03 - Last mean reward per episode: 92.79
Num timesteps: 1162000
Best mean reward: 94.03 - Last mean reward per episode: 92.88
Num timesteps: 1163000
Best mean reward: 94.03 - Last mean reward per episode: 92.97
Num timesteps: 1164000
Best mean reward: 94.03 - Last mean reward per episode: 93.06
Num timesteps: 1165000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 1166000
Best mean reward: 94.03 - Last mean reward per episode: 93.05
Num timesteps: 1167000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1168000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1169000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 1170000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
--------------------------------------
| reference_Q_mean        | 53.3     |
| reference_Q_std         | 6.4      |
| reference_action_mean   | -0.619   |
| reference_action_std    | 0.769    |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 6.56     |
| rollout/Q_mean          | 62.1     |
| rollout/actions_mean    | 0.125    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.13e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.3     |
| total/duration          | 2.24e+03 |
| total/episodes          | 1.13e+04 |
| total/epochs            | 1        |
| total/steps             | 1169998  |
| total/steps_per_second  | 522      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.337    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1171000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1172000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
Num timesteps: 1173000
Best mean reward: 94.03 - Last mean reward per episode: 93.04
Num timesteps: 1174000
Best mean reward: 94.03 - Last mean reward per episode: 92.98
Num timesteps: 1175000
Best mean reward: 94.03 - Last mean reward per episode: 92.74
Num timesteps: 1176000
Best mean reward: 94.03 - Last mean reward per episode: 92.70
Num timesteps: 1177000
Best mean reward: 94.03 - Last mean reward per episode: 92.76
Num timesteps: 1178000
Best mean reward: 94.03 - Last mean reward per episode: 92.91
Num timesteps: 1179000
Best mean reward: 94.03 - Last mean reward per episode: 93.02
Num timesteps: 1180000
Best mean reward: 94.03 - Last mean reward per episode: 93.07
--------------------------------------
| reference_Q_mean        | 53       |
| reference_Q_std         | 6.4      |
| reference_action_mean   | -0.677   |
| reference_action_std    | 0.725    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.79     |
| rollout/Q_mean          | 62.2     |
| rollout/actions_mean    | 0.125    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.14e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.1     |
| total/duration          | 2.26e+03 |
| total/episodes          | 1.14e+04 |
| total/epochs            | 1        |
| total/steps             | 1179998  |
| total/steps_per_second  | 521      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.398    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1181000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 1182000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1183000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 1184000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1185000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 1186000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 1187000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 1188000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 1189000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 1190000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
--------------------------------------
| reference_Q_mean        | 53.2     |
| reference_Q_std         | 6.49     |
| reference_action_mean   | -0.555   |
| reference_action_std    | 0.817    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 6.69     |
| rollout/Q_mean          | 62.3     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.15e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.3     |
| total/duration          | 2.29e+03 |
| total/episodes          | 1.15e+04 |
| total/epochs            | 1        |
| total/steps             | 1189998  |
| total/steps_per_second  | 520      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.274    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1191000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
Num timesteps: 1192000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1193000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 1194000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1195000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 1196000
Best mean reward: 94.03 - Last mean reward per episode: 93.87
Num timesteps: 1197000
Best mean reward: 94.03 - Last mean reward per episode: 93.87
Num timesteps: 1198000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 1199000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1200000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
--------------------------------------
| reference_Q_mean        | 53.1     |
| reference_Q_std         | 6.56     |
| reference_action_mean   | -0.525   |
| reference_action_std    | 0.835    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 6.98     |
| rollout/Q_mean          | 62.4     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.16e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.31e+03 |
| total/episodes          | 1.16e+04 |
| total/epochs            | 1        |
| total/steps             | 1199998  |
| total/steps_per_second  | 519      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.338    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1201000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1202000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 1203000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 1204000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 1205000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 1206000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 1207000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 1208000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 1209000
Best mean reward: 94.03 - Last mean reward per episode: 93.86
Num timesteps: 1210000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
--------------------------------------
| reference_Q_mean        | 52.9     |
| reference_Q_std         | 6.75     |
| reference_action_mean   | -0.573   |
| reference_action_std    | 0.785    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.23     |
| rollout/Q_mean          | 62.4     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.17e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.33e+03 |
| total/episodes          | 1.17e+04 |
| total/epochs            | 1        |
| total/steps             | 1209998  |
| total/steps_per_second  | 519      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.254    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1211000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1212000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 1213000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 1214000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 1215000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1216000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 1217000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1218000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1219000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 1220000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 6.97     |
| reference_action_mean   | -0.668   |
| reference_action_std    | 0.728    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 7.6      |
| rollout/Q_mean          | 62.5     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.18e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.6     |
| total/duration          | 2.36e+03 |
| total/episodes          | 1.18e+04 |
| total/epochs            | 1        |
| total/steps             | 1219998  |
| total/steps_per_second  | 517      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.207    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1221000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1222000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1223000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 1224000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 1225000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1226000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1227000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 1228000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1229000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 1230000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 6.97     |
| reference_action_mean   | -0.661   |
| reference_action_std    | 0.728    |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 7.71     |
| rollout/Q_mean          | 62.6     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.2e+04  |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.4     |
| total/duration          | 2.39e+03 |
| total/episodes          | 1.2e+04  |
| total/epochs            | 1        |
| total/steps             | 1229998  |
| total/steps_per_second  | 515      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.249    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1231000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 1232000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1233000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1234000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1235000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1236000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 1237000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1238000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1239000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 1240000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
--------------------------------------
| reference_Q_mean        | 53.7     |
| reference_Q_std         | 6.38     |
| reference_action_mean   | -0.525   |
| reference_action_std    | 0.824    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.3      |
| rollout/Q_mean          | 62.6     |
| rollout/actions_mean    | 0.125    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.21e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.42e+03 |
| total/episodes          | 1.21e+04 |
| total/epochs            | 1        |
| total/steps             | 1239998  |
| total/steps_per_second  | 513      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.283    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1241000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1242000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1243000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1244000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1245000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 1246000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 1247000
Best mean reward: 94.03 - Last mean reward per episode: 93.05
Num timesteps: 1248000
Best mean reward: 94.03 - Last mean reward per episode: 92.53
Num timesteps: 1249000
Best mean reward: 94.03 - Last mean reward per episode: 92.50
Num timesteps: 1250000
Best mean reward: 94.03 - Last mean reward per episode: 92.49
--------------------------------------
| reference_Q_mean        | 54.3     |
| reference_Q_std         | 6.2      |
| reference_action_mean   | -0.398   |
| reference_action_std    | 0.899    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 6.93     |
| rollout/Q_mean          | 62.7     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.22e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 92.5     |
| total/duration          | 2.44e+03 |
| total/episodes          | 1.22e+04 |
| total/epochs            | 1        |
| total/steps             | 1249998  |
| total/steps_per_second  | 512      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.387    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1251000
Best mean reward: 94.03 - Last mean reward per episode: 92.64
Num timesteps: 1252000
Best mean reward: 94.03 - Last mean reward per episode: 92.67
Num timesteps: 1253000
Best mean reward: 94.03 - Last mean reward per episode: 92.48
Num timesteps: 1254000
Best mean reward: 94.03 - Last mean reward per episode: 92.41
Num timesteps: 1255000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
Num timesteps: 1256000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1257000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1258000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1259000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1260000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
--------------------------------------
| reference_Q_mean        | 53.7     |
| reference_Q_std         | 6.57     |
| reference_action_mean   | -0.397   |
| reference_action_std    | 0.893    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 7.17     |
| rollout/Q_mean          | 62.7     |
| rollout/actions_mean    | 0.125    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.23e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.47e+03 |
| total/episodes          | 1.23e+04 |
| total/epochs            | 1        |
| total/steps             | 1259998  |
| total/steps_per_second  | 510      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.306    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1261000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1262000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1263000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1264000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 1265000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
Num timesteps: 1266000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 1267000
Best mean reward: 94.03 - Last mean reward per episode: 93.06
Num timesteps: 1268000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 1269000
Best mean reward: 94.03 - Last mean reward per episode: 92.91
Num timesteps: 1270000
Best mean reward: 94.03 - Last mean reward per episode: 92.80
--------------------------------------
| reference_Q_mean        | 53.4     |
| reference_Q_std         | 6.6      |
| reference_action_mean   | -0.539   |
| reference_action_std    | 0.821    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.32     |
| rollout/Q_mean          | 62.8     |
| rollout/actions_mean    | 0.125    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.24e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 92.8     |
| total/duration          | 2.5e+03  |
| total/episodes          | 1.24e+04 |
| total/epochs            | 1        |
| total/steps             | 1269998  |
| total/steps_per_second  | 509      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 0.379    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1271000
Best mean reward: 94.03 - Last mean reward per episode: 92.74
Num timesteps: 1272000
Best mean reward: 94.03 - Last mean reward per episode: 92.97
Num timesteps: 1273000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 1274000
Best mean reward: 94.03 - Last mean reward per episode: 92.96
Num timesteps: 1275000
Best mean reward: 94.03 - Last mean reward per episode: 93.16
Num timesteps: 1276000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1277000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 1278000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 1279000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 1280000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
--------------------------------------
| reference_Q_mean        | 53.5     |
| reference_Q_std         | 6.76     |
| reference_action_mean   | -0.509   |
| reference_action_std    | 0.845    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 7.19     |
| rollout/Q_mean          | 62.9     |
| rollout/actions_mean    | 0.125    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.25e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.8     |
| total/duration          | 2.52e+03 |
| total/episodes          | 1.25e+04 |
| total/epochs            | 1        |
| total/steps             | 1279998  |
| total/steps_per_second  | 507      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.402    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1281000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 1282000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 1283000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 1284000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 1285000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 1286000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1287000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 1288000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 1289000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 1290000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
--------------------------------------
| reference_Q_mean        | 53.1     |
| reference_Q_std         | 6.88     |
| reference_action_mean   | -0.481   |
| reference_action_std    | 0.863    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 7.13     |
| rollout/Q_mean          | 62.9     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.26e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93       |
| total/duration          | 2.55e+03 |
| total/episodes          | 1.26e+04 |
| total/epochs            | 1        |
| total/steps             | 1289998  |
| total/steps_per_second  | 506      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.371    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1291000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 1292000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 1293000
Best mean reward: 94.03 - Last mean reward per episode: 92.88
Num timesteps: 1294000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 1295000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 1296000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 1297000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 1298000
Best mean reward: 94.03 - Last mean reward per episode: 92.98
Num timesteps: 1299000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 1300000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 53       |
| reference_Q_std         | 7.05     |
| reference_action_mean   | -0.48    |
| reference_action_std    | 0.863    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.29     |
| rollout/Q_mean          | 63       |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.27e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.1     |
| total/duration          | 2.58e+03 |
| total/episodes          | 1.27e+04 |
| total/epochs            | 1        |
| total/steps             | 1299998  |
| total/steps_per_second  | 504      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.347    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1301000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 1302000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 1303000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1304000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 1305000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1306000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 1307000
Best mean reward: 94.03 - Last mean reward per episode: 93.81
Num timesteps: 1308000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 1309000
Best mean reward: 94.03 - Last mean reward per episode: 93.77
Num timesteps: 1310000
Best mean reward: 94.03 - Last mean reward per episode: 93.85
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 7.38     |
| reference_action_mean   | -0.467   |
| reference_action_std    | 0.869    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.37     |
| rollout/Q_mean          | 63       |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.28e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.8     |
| total/duration          | 2.6e+03  |
| total/episodes          | 1.28e+04 |
| total/epochs            | 1        |
| total/steps             | 1309998  |
| total/steps_per_second  | 503      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.271    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1311000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
Num timesteps: 1312000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 1313000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 1314000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 1315000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 1316000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 1317000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 1318000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 1319000
Best mean reward: 94.03 - Last mean reward per episode: 92.10
Num timesteps: 1320000
Best mean reward: 94.03 - Last mean reward per episode: 92.12
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.68     |
| reference_action_mean   | -0.419   |
| reference_action_std    | 0.893    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.69     |
| rollout/Q_mean          | 63.1     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.3e+04  |
| rollout/return          | 91.6     |
| rollout/return_history  | 92.1     |
| total/duration          | 2.63e+03 |
| total/episodes          | 1.3e+04  |
| total/epochs            | 1        |
| total/steps             | 1319998  |
| total/steps_per_second  | 502      |
| train/loss_actor        | -71      |
| train/loss_critic       | 0.376    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1321000
Best mean reward: 94.03 - Last mean reward per episode: 92.03
Num timesteps: 1322000
Best mean reward: 94.03 - Last mean reward per episode: 90.33
Num timesteps: 1323000
Best mean reward: 94.03 - Last mean reward per episode: 90.34
Num timesteps: 1324000
Best mean reward: 94.03 - Last mean reward per episode: 90.30
Num timesteps: 1325000
Best mean reward: 94.03 - Last mean reward per episode: 90.36
Num timesteps: 1326000
Best mean reward: 94.03 - Last mean reward per episode: 90.36
Num timesteps: 1327000
Best mean reward: 94.03 - Last mean reward per episode: 88.98
Num timesteps: 1328000
Best mean reward: 94.03 - Last mean reward per episode: 89.10
Num timesteps: 1329000
Best mean reward: 94.03 - Last mean reward per episode: 90.53
Num timesteps: 1330000
Best mean reward: 94.03 - Last mean reward per episode: 90.59
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 6.9      |
| reference_action_mean   | -0.532   |
| reference_action_std    | 0.822    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.15     |
| rollout/Q_mean          | 63.2     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.31e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 90.6     |
| total/duration          | 2.66e+03 |
| total/episodes          | 1.31e+04 |
| total/epochs            | 1        |
| total/steps             | 1329998  |
| total/steps_per_second  | 500      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 2.19     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1331000
Best mean reward: 94.03 - Last mean reward per episode: 92.30
Num timesteps: 1332000
Best mean reward: 94.03 - Last mean reward per episode: 92.31
Num timesteps: 1333000
Best mean reward: 94.03 - Last mean reward per episode: 92.23
Num timesteps: 1334000
Best mean reward: 94.03 - Last mean reward per episode: 92.28
Num timesteps: 1335000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 1336000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 1337000
Best mean reward: 94.03 - Last mean reward per episode: 93.77
Num timesteps: 1338000
Best mean reward: 94.03 - Last mean reward per episode: 93.85
Num timesteps: 1339000
Best mean reward: 94.03 - Last mean reward per episode: 93.88
Num timesteps: 1340000
Best mean reward: 94.03 - Last mean reward per episode: 93.98
--------------------------------------
| reference_Q_mean        | 52.9     |
| reference_Q_std         | 6.81     |
| reference_action_mean   | -0.482   |
| reference_action_std    | 0.864    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.32     |
| rollout/Q_mean          | 63.2     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.32e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 94       |
| total/duration          | 2.69e+03 |
| total/episodes          | 1.32e+04 |
| total/epochs            | 1        |
| total/steps             | 1339998  |
| total/steps_per_second  | 499      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.225    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1341000
Best mean reward: 94.03 - Last mean reward per episode: 92.40
Num timesteps: 1342000
Best mean reward: 94.03 - Last mean reward per episode: 92.32
Num timesteps: 1343000
Best mean reward: 94.03 - Last mean reward per episode: 92.29
Num timesteps: 1344000
Best mean reward: 94.03 - Last mean reward per episode: 92.05
Num timesteps: 1345000
Best mean reward: 94.03 - Last mean reward per episode: 91.96
Num timesteps: 1346000
Best mean reward: 94.03 - Last mean reward per episode: 91.97
Num timesteps: 1347000
Best mean reward: 94.03 - Last mean reward per episode: 91.47
Num timesteps: 1348000
Best mean reward: 94.03 - Last mean reward per episode: 91.45
Num timesteps: 1349000
Best mean reward: 94.03 - Last mean reward per episode: 91.51
Num timesteps: 1350000
Best mean reward: 94.03 - Last mean reward per episode: 91.51
--------------------------------------
| reference_Q_mean        | 53.2     |
| reference_Q_std         | 6.92     |
| reference_action_mean   | -0.569   |
| reference_action_std    | 0.798    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.66     |
| rollout/Q_mean          | 63.3     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.33e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 91.5     |
| total/duration          | 2.71e+03 |
| total/episodes          | 1.33e+04 |
| total/epochs            | 1        |
| total/steps             | 1349998  |
| total/steps_per_second  | 498      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.241    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1351000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 1352000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 1353000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
Num timesteps: 1354000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 1355000
Best mean reward: 94.03 - Last mean reward per episode: 93.87
Num timesteps: 1356000
Best mean reward: 94.03 - Last mean reward per episode: 93.94
Num timesteps: 1357000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 1358000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 1359000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 1360000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
--------------------------------------
| reference_Q_mean        | 52.8     |
| reference_Q_std         | 6.97     |
| reference_action_mean   | -0.499   |
| reference_action_std    | 0.841    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 7.68     |
| rollout/Q_mean          | 63.3     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.837    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.34e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93.7     |
| total/duration          | 2.74e+03 |
| total/episodes          | 1.34e+04 |
| total/epochs            | 1        |
| total/steps             | 1359998  |
| total/steps_per_second  | 496      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.767    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1361000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 1362000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1363000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 1364000
Best mean reward: 94.03 - Last mean reward per episode: 92.65
Num timesteps: 1365000
Best mean reward: 94.03 - Last mean reward per episode: 92.67
Num timesteps: 1366000
Best mean reward: 94.03 - Last mean reward per episode: 92.58
Num timesteps: 1367000
Best mean reward: 94.03 - Last mean reward per episode: 92.74
Num timesteps: 1368000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
Num timesteps: 1369000
Best mean reward: 94.03 - Last mean reward per episode: 92.87
Num timesteps: 1370000
Best mean reward: 94.03 - Last mean reward per episode: 92.79
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 7.17     |
| reference_action_mean   | -0.536   |
| reference_action_std    | 0.826    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.49     |
| rollout/Q_mean          | 63.4     |
| rollout/actions_mean    | 0.121    |
| rollout/actions_std     | 0.837    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.35e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 92.8     |
| total/duration          | 2.77e+03 |
| total/episodes          | 1.35e+04 |
| total/epochs            | 1        |
| total/steps             | 1369998  |
| total/steps_per_second  | 495      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.237    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1371000
Best mean reward: 94.03 - Last mean reward per episode: 92.90
Num timesteps: 1372000
Best mean reward: 94.03 - Last mean reward per episode: 92.91
Num timesteps: 1373000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1374000
Best mean reward: 94.03 - Last mean reward per episode: 91.71
Num timesteps: 1375000
Best mean reward: 94.03 - Last mean reward per episode: 91.70
Num timesteps: 1376000
Best mean reward: 94.03 - Last mean reward per episode: 91.60
Num timesteps: 1377000
Best mean reward: 94.03 - Last mean reward per episode: 91.59
Num timesteps: 1378000
Best mean reward: 94.03 - Last mean reward per episode: 91.53
Num timesteps: 1379000
Best mean reward: 94.03 - Last mean reward per episode: 91.60
Num timesteps: 1380000
Best mean reward: 94.03 - Last mean reward per episode: 91.59
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 7.03     |
| reference_action_mean   | -0.412   |
| reference_action_std    | 0.895    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.32     |
| rollout/Q_mean          | 63.4     |
| rollout/actions_mean    | 0.12     |
| rollout/actions_std     | 0.837    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.36e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 91.6     |
| total/duration          | 2.79e+03 |
| total/episodes          | 1.36e+04 |
| total/epochs            | 1        |
| total/steps             | 1379998  |
| total/steps_per_second  | 494      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.251    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1381000
Best mean reward: 94.03 - Last mean reward per episode: 91.91
Num timesteps: 1382000
Best mean reward: 94.03 - Last mean reward per episode: 91.89
Num timesteps: 1383000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 1384000
Best mean reward: 94.03 - Last mean reward per episode: 93.77
Num timesteps: 1385000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 1386000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1387000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1388000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 1389000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1390000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
--------------------------------------
| reference_Q_mean        | 52.8     |
| reference_Q_std         | 6.88     |
| reference_action_mean   | -0.387   |
| reference_action_std    | 0.901    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 7.19     |
| rollout/Q_mean          | 63.5     |
| rollout/actions_mean    | 0.121    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.37e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.82e+03 |
| total/episodes          | 1.37e+04 |
| total/epochs            | 1        |
| total/steps             | 1389998  |
| total/steps_per_second  | 493      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.249    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1391000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1392000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 1393000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 1394000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
Num timesteps: 1395000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 1396000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
Num timesteps: 1397000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 1398000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 1399000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1400000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
--------------------------------------
| reference_Q_mean        | 52.6     |
| reference_Q_std         | 6.9      |
| reference_action_mean   | -0.644   |
| reference_action_std    | 0.75     |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 6.88     |
| rollout/Q_mean          | 63.5     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.38e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.85e+03 |
| total/episodes          | 1.38e+04 |
| total/epochs            | 1        |
| total/steps             | 1399998  |
| total/steps_per_second  | 492      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.448    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1401000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1402000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1403000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1404000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 1405000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 1406000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 1407000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 1408000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 1409000
Best mean reward: 94.03 - Last mean reward per episode: 93.02
Num timesteps: 1410000
Best mean reward: 94.03 - Last mean reward per episode: 92.98
--------------------------------------
| reference_Q_mean        | 53       |
| reference_Q_std         | 6.88     |
| reference_action_mean   | -0.632   |
| reference_action_std    | 0.757    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.95     |
| rollout/Q_mean          | 63.6     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.39e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93       |
| total/duration          | 2.87e+03 |
| total/episodes          | 1.39e+04 |
| total/epochs            | 1        |
| total/steps             | 1409998  |
| total/steps_per_second  | 491      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.272    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1411000
Best mean reward: 94.03 - Last mean reward per episode: 93.01
Num timesteps: 1412000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 1413000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 1414000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 1415000
Best mean reward: 94.03 - Last mean reward per episode: 91.82
Num timesteps: 1416000
Best mean reward: 94.03 - Last mean reward per episode: 91.85
Num timesteps: 1417000
Best mean reward: 94.03 - Last mean reward per episode: 91.54
Num timesteps: 1418000
Best mean reward: 94.03 - Last mean reward per episode: 91.62
Num timesteps: 1419000
Best mean reward: 94.03 - Last mean reward per episode: 91.69
Num timesteps: 1420000
Best mean reward: 94.03 - Last mean reward per episode: 91.72
--------------------------------------
| reference_Q_mean        | 53.2     |
| reference_Q_std         | 6.83     |
| reference_action_mean   | -0.698   |
| reference_action_std    | 0.694    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 7.04     |
| rollout/Q_mean          | 63.6     |
| rollout/actions_mean    | 0.121    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.4e+04  |
| rollout/return          | 91.7     |
| rollout/return_history  | 91.7     |
| total/duration          | 2.9e+03  |
| total/episodes          | 1.4e+04  |
| total/epochs            | 1        |
| total/steps             | 1419998  |
| total/steps_per_second  | 489      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.295    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1421000
Best mean reward: 94.03 - Last mean reward per episode: 91.80
Num timesteps: 1422000
Best mean reward: 94.03 - Last mean reward per episode: 91.92
Num timesteps: 1423000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1424000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1425000
Best mean reward: 94.03 - Last mean reward per episode: 93.87
Num timesteps: 1426000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 1427000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 1428000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 1429000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 1430000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
--------------------------------------
| reference_Q_mean        | 53       |
| reference_Q_std         | 6.55     |
| reference_action_mean   | -0.597   |
| reference_action_std    | 0.785    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 6.9      |
| rollout/Q_mean          | 63.6     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.42e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93.6     |
| total/duration          | 2.93e+03 |
| total/episodes          | 1.42e+04 |
| total/epochs            | 1        |
| total/steps             | 1429998  |
| total/steps_per_second  | 488      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.288    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1431000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1432000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1433000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1434000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 1435000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1436000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1437000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1438000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1439000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 1440000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
--------------------------------------
| reference_Q_mean        | 53.1     |
| reference_Q_std         | 6.6      |
| reference_action_mean   | -0.565   |
| reference_action_std    | 0.804    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 6.8      |
| rollout/Q_mean          | 63.7     |
| rollout/actions_mean    | 0.122    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.43e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93.6     |
| total/duration          | 2.96e+03 |
| total/episodes          | 1.43e+04 |
| total/epochs            | 1        |
| total/steps             | 1439998  |
| total/steps_per_second  | 487      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.309    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1441000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1442000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1443000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1444000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1445000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 1446000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 1447000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 1448000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 1449000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 1450000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
--------------------------------------
| reference_Q_mean        | 53.7     |
| reference_Q_std         | 6.21     |
| reference_action_mean   | -0.493   |
| reference_action_std    | 0.84     |
| reference_actor_Q_mean  | 54.1     |
| reference_actor_Q_std   | 6.39     |
| rollout/Q_mean          | 63.7     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.44e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.6     |
| total/duration          | 2.99e+03 |
| total/episodes          | 1.44e+04 |
| total/epochs            | 1        |
| total/steps             | 1449998  |
| total/steps_per_second  | 486      |
| train/loss_actor        | -70      |
| train/loss_critic       | 1.66     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1451000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1452000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 1453000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1454000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 1455000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1456000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1457000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 1458000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 1459000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 1460000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
--------------------------------------
| reference_Q_mean        | 53.4     |
| reference_Q_std         | 6.75     |
| reference_action_mean   | -0.585   |
| reference_action_std    | 0.791    |
| reference_actor_Q_mean  | 54.6     |
| reference_actor_Q_std   | 6.54     |
| rollout/Q_mean          | 63.8     |
| rollout/actions_mean    | 0.123    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.45e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.2     |
| total/duration          | 3.01e+03 |
| total/episodes          | 1.45e+04 |
| total/epochs            | 1        |
| total/steps             | 1459998  |
| total/steps_per_second  | 484      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.25     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1461000
Best mean reward: 94.03 - Last mean reward per episode: 93.16
Num timesteps: 1462000
Best mean reward: 94.03 - Last mean reward per episode: 93.02
Num timesteps: 1463000
Best mean reward: 94.03 - Last mean reward per episode: 92.89
Num timesteps: 1464000
Best mean reward: 94.03 - Last mean reward per episode: 92.77
Num timesteps: 1465000
Best mean reward: 94.03 - Last mean reward per episode: 92.77
Num timesteps: 1466000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 1467000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 1468000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
Num timesteps: 1469000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 1470000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
--------------------------------------
| reference_Q_mean        | 53.2     |
| reference_Q_std         | 6.74     |
| reference_action_mean   | -0.596   |
| reference_action_std    | 0.776    |
| reference_actor_Q_mean  | 54.7     |
| reference_actor_Q_std   | 6.42     |
| rollout/Q_mean          | 63.8     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.46e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.1     |
| total/duration          | 3.04e+03 |
| total/episodes          | 1.46e+04 |
| total/epochs            | 1        |
| total/steps             | 1469998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.347    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1471000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 1472000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1473000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 1474000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1475000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 1476000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 1477000
Best mean reward: 94.03 - Last mean reward per episode: 93.80
Num timesteps: 1478000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
Num timesteps: 1479000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 1480000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
--------------------------------------
| reference_Q_mean        | 53.3     |
| reference_Q_std         | 6.84     |
| reference_action_mean   | -0.519   |
| reference_action_std    | 0.826    |
| reference_actor_Q_mean  | 54.8     |
| reference_actor_Q_std   | 6.66     |
| rollout/Q_mean          | 63.9     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.48e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.7     |
| total/duration          | 3.06e+03 |
| total/episodes          | 1.48e+04 |
| total/epochs            | 1        |
| total/steps             | 1479998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.441    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1481000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 1482000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 1483000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1484000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 1485000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 1486000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 1487000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 1488000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 1489000
Best mean reward: 94.03 - Last mean reward per episode: 93.16
Num timesteps: 1490000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
--------------------------------------
| reference_Q_mean        | 53.4     |
| reference_Q_std         | 6.76     |
| reference_action_mean   | -0.373   |
| reference_action_std    | 0.897    |
| reference_actor_Q_mean  | 54.6     |
| reference_actor_Q_std   | 6.58     |
| rollout/Q_mean          | 63.9     |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.49e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.1     |
| total/duration          | 3.09e+03 |
| total/episodes          | 1.49e+04 |
| total/epochs            | 1        |
| total/steps             | 1489998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.422    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1491000
Best mean reward: 94.03 - Last mean reward per episode: 91.46
Num timesteps: 1492000
Best mean reward: 94.03 - Last mean reward per episode: 91.46
Num timesteps: 1493000
Best mean reward: 94.03 - Last mean reward per episode: 91.97
Num timesteps: 1494000
Best mean reward: 94.03 - Last mean reward per episode: 92.00
Num timesteps: 1495000
Best mean reward: 94.03 - Last mean reward per episode: 91.51
Num timesteps: 1496000
Best mean reward: 94.03 - Last mean reward per episode: 91.58
Num timesteps: 1497000
Best mean reward: 94.03 - Last mean reward per episode: 91.57
Num timesteps: 1498000
Best mean reward: 94.03 - Last mean reward per episode: 90.09
Num timesteps: 1499000
Best mean reward: 94.03 - Last mean reward per episode: 90.21
Num timesteps: 1500000
Best mean reward: 94.03 - Last mean reward per episode: 91.88
--------------------------------------
| reference_Q_mean        | 53.3     |
| reference_Q_std         | 6.79     |
| reference_action_mean   | -0.331   |
| reference_action_std    | 0.912    |
| reference_actor_Q_mean  | 54.3     |
| reference_actor_Q_std   | 6.71     |
| rollout/Q_mean          | 64       |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.5e+04  |
| rollout/return          | 91.8     |
| rollout/return_history  | 91.9     |
| total/duration          | 3.11e+03 |
| total/episodes          | 1.5e+04  |
| total/epochs            | 1        |
| total/steps             | 1499998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.22     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1501000
Best mean reward: 94.03 - Last mean reward per episode: 91.77
Num timesteps: 1502000
Best mean reward: 94.03 - Last mean reward per episode: 91.74
Num timesteps: 1503000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 1504000
Best mean reward: 94.03 - Last mean reward per episode: 92.15
Num timesteps: 1505000
Best mean reward: 94.03 - Last mean reward per episode: 92.05
Num timesteps: 1506000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1507000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 1508000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 1509000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 1510000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
--------------------------------------
| reference_Q_mean        | 53.2     |
| reference_Q_std         | 6.81     |
| reference_action_mean   | -0.236   |
| reference_action_std    | 0.953    |
| reference_actor_Q_mean  | 54.1     |
| reference_actor_Q_std   | 6.76     |
| rollout/Q_mean          | 64       |
| rollout/actions_mean    | 0.124    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.51e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.13e+03 |
| total/episodes          | 1.51e+04 |
| total/epochs            | 1        |
| total/steps             | 1509998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.241    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1511000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 1512000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 1513000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 1514000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 1515000
Best mean reward: 94.03 - Last mean reward per episode: 93.02
Num timesteps: 1516000
Best mean reward: 94.03 - Last mean reward per episode: 93.02
Num timesteps: 1517000
Best mean reward: 94.03 - Last mean reward per episode: 93.01
Num timesteps: 1518000
Best mean reward: 94.03 - Last mean reward per episode: 93.06
Num timesteps: 1519000
Best mean reward: 94.03 - Last mean reward per episode: 93.06
Num timesteps: 1520000
Best mean reward: 94.03 - Last mean reward per episode: 93.05
--------------------------------------
| reference_Q_mean        | 53.7     |
| reference_Q_std         | 6.54     |
| reference_action_mean   | -0.255   |
| reference_action_std    | 0.938    |
| reference_actor_Q_mean  | 53.8     |
| reference_actor_Q_std   | 6.81     |
| rollout/Q_mean          | 64       |
| rollout/actions_mean    | 0.125    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.52e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93       |
| total/duration          | 3.15e+03 |
| total/episodes          | 1.52e+04 |
| total/epochs            | 1        |
| total/steps             | 1519998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.3      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1521000
Best mean reward: 94.03 - Last mean reward per episode: 93.06
Num timesteps: 1522000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1523000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 1524000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1525000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1526000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 1527000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
Num timesteps: 1528000
Best mean reward: 94.03 - Last mean reward per episode: 92.99
Num timesteps: 1529000
Best mean reward: 94.03 - Last mean reward per episode: 92.99
Num timesteps: 1530000
Best mean reward: 94.03 - Last mean reward per episode: 92.94
--------------------------------------
| reference_Q_mean        | 53.1     |
| reference_Q_std         | 7        |
| reference_action_mean   | -0.239   |
| reference_action_std    | 0.958    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 7.31     |
| rollout/Q_mean          | 64.1     |
| rollout/actions_mean    | 0.125    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.53e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 92.9     |
| total/duration          | 3.17e+03 |
| total/episodes          | 1.53e+04 |
| total/epochs            | 1        |
| total/steps             | 1529998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 1.18     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1531000
Best mean reward: 94.03 - Last mean reward per episode: 92.87
Num timesteps: 1532000
Best mean reward: 94.03 - Last mean reward per episode: 92.90
Num timesteps: 1533000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
Num timesteps: 1534000
Best mean reward: 94.03 - Last mean reward per episode: 92.92
Num timesteps: 1535000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 1536000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1537000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1538000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1539000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 1540000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 7.7      |
| reference_action_mean   | -0.0962  |
| reference_action_std    | 0.962    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 7.99     |
| rollout/Q_mean          | 64.1     |
| rollout/actions_mean    | 0.126    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.9     |
| rollout/episodes        | 1.54e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.4     |
| total/duration          | 3.19e+03 |
| total/episodes          | 1.54e+04 |
| total/epochs            | 1        |
| total/steps             | 1539998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.455    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1541000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1542000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1543000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1544000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1545000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 1546000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1547000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1548000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1549000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1550000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 7.57     |
| reference_action_mean   | -0.532   |
| reference_action_std    | 0.816    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 7.87     |
| rollout/Q_mean          | 64.1     |
| rollout/actions_mean    | 0.126    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.8     |
| rollout/episodes        | 1.55e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 3.21e+03 |
| total/episodes          | 1.55e+04 |
| total/epochs            | 1        |
| total/steps             | 1549998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.233    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1551000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1552000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1553000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1554000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 1555000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1556000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1557000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 1558000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1559000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1560000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 7.23     |
| reference_action_mean   | -0.58    |
| reference_action_std    | 0.79     |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.48     |
| rollout/Q_mean          | 64.2     |
| rollout/actions_mean    | 0.127    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.7     |
| rollout/episodes        | 1.56e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.5     |
| total/duration          | 3.23e+03 |
| total/episodes          | 1.56e+04 |
| total/epochs            | 1        |
| total/steps             | 1559998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.232    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1561000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1562000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 1563000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 1564000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 1565000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 1566000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1567000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 1568000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 1569000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 1570000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
--------------------------------------
| reference_Q_mean        | 52.6     |
| reference_Q_std         | 7.07     |
| reference_action_mean   | -0.573   |
| reference_action_std    | 0.789    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.3      |
| rollout/Q_mean          | 64.2     |
| rollout/actions_mean    | 0.127    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.6     |
| rollout/episodes        | 1.58e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.25e+03 |
| total/episodes          | 1.58e+04 |
| total/epochs            | 1        |
| total/steps             | 1569998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 0.364    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1571000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 1572000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1573000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1574000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1575000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 1576000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1577000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
Num timesteps: 1578000
Best mean reward: 94.03 - Last mean reward per episode: 92.92
Num timesteps: 1579000
Best mean reward: 94.03 - Last mean reward per episode: 92.72
Num timesteps: 1580000
Best mean reward: 94.03 - Last mean reward per episode: 92.76
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 7.31     |
| reference_action_mean   | -0.636   |
| reference_action_std    | 0.762    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 7.47     |
| rollout/Q_mean          | 64.2     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.6     |
| rollout/episodes        | 1.59e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.8     |
| total/duration          | 3.27e+03 |
| total/episodes          | 1.59e+04 |
| total/epochs            | 1        |
| total/steps             | 1579998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.337    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1581000
Best mean reward: 94.03 - Last mean reward per episode: 92.81
Num timesteps: 1582000
Best mean reward: 94.03 - Last mean reward per episode: 92.85
Num timesteps: 1583000
Best mean reward: 94.03 - Last mean reward per episode: 92.66
Num timesteps: 1584000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
Num timesteps: 1585000
Best mean reward: 94.03 - Last mean reward per episode: 92.86
Num timesteps: 1586000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 1587000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 1588000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1589000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 1590000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 7.4      |
| reference_action_mean   | -0.603   |
| reference_action_std    | 0.79     |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 7.53     |
| rollout/Q_mean          | 64.3     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.4     |
| rollout/episodes        | 1.6e+04  |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 3.29e+03 |
| total/episodes          | 1.6e+04  |
| total/epochs            | 1        |
| total/steps             | 1589998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.349    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1591000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1592000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 1593000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 1594000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 1595000
Best mean reward: 94.03 - Last mean reward per episode: 92.06
Num timesteps: 1596000
Best mean reward: 94.03 - Last mean reward per episode: 92.11
Num timesteps: 1597000
Best mean reward: 94.03 - Last mean reward per episode: 92.16
Num timesteps: 1598000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
Num timesteps: 1599000
Best mean reward: 94.03 - Last mean reward per episode: 92.10
Num timesteps: 1600000
Best mean reward: 94.03 - Last mean reward per episode: 92.11
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.31     |
| reference_action_mean   | -0.609   |
| reference_action_std    | 0.787    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 7.53     |
| rollout/Q_mean          | 64.3     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.4     |
| rollout/episodes        | 1.61e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.1     |
| total/duration          | 3.31e+03 |
| total/episodes          | 1.61e+04 |
| total/epochs            | 1        |
| total/steps             | 1599998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.322    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1601000
Best mean reward: 94.03 - Last mean reward per episode: 90.64
Num timesteps: 1602000
Best mean reward: 94.03 - Last mean reward per episode: 90.46
Num timesteps: 1603000
Best mean reward: 94.03 - Last mean reward per episode: 90.45
Num timesteps: 1604000
Best mean reward: 94.03 - Last mean reward per episode: 90.33
Num timesteps: 1605000
Best mean reward: 94.03 - Last mean reward per episode: 92.03
Num timesteps: 1606000
Best mean reward: 94.03 - Last mean reward per episode: 92.01
Num timesteps: 1607000
Best mean reward: 94.03 - Last mean reward per episode: 92.02
Num timesteps: 1608000
Best mean reward: 94.03 - Last mean reward per episode: 92.02
Num timesteps: 1609000
Best mean reward: 94.03 - Last mean reward per episode: 91.90
Num timesteps: 1610000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 7.34     |
| reference_action_mean   | -0.476   |
| reference_action_std    | 0.859    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.22     |
| rollout/Q_mean          | 64.3     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.4     |
| rollout/episodes        | 1.62e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.34e+03 |
| total/episodes          | 1.62e+04 |
| total/epochs            | 1        |
| total/steps             | 1609998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.972    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1611000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 1612000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1613000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1614000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 1615000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 1616000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 1617000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1618000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1619000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1620000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
--------------------------------------
| reference_Q_mean        | 52.3     |
| reference_Q_std         | 7.21     |
| reference_action_mean   | -0.563   |
| reference_action_std    | 0.82     |
| reference_actor_Q_mean  | 53.4     |
| reference_actor_Q_std   | 6.92     |
| rollout/Q_mean          | 64.4     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.3     |
| rollout/episodes        | 1.63e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.36e+03 |
| total/episodes          | 1.63e+04 |
| total/epochs            | 1        |
| total/steps             | 1619998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.48     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1621000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 1622000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 1623000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 1624000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 1625000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1626000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1627000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 1628000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 1629000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1630000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
--------------------------------------
| reference_Q_mean        | 53       |
| reference_Q_std         | 6.89     |
| reference_action_mean   | -0.492   |
| reference_action_std    | 0.854    |
| reference_actor_Q_mean  | 53.8     |
| reference_actor_Q_std   | 6.84     |
| rollout/Q_mean          | 64.4     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.3     |
| rollout/episodes        | 1.64e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.38e+03 |
| total/episodes          | 1.64e+04 |
| total/epochs            | 1        |
| total/steps             | 1629998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.399    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1631000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1632000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1633000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 1634000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 1635000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1636000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1637000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 1638000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1639000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1640000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
--------------------------------------
| reference_Q_mean        | 53.6     |
| reference_Q_std         | 6.47     |
| reference_action_mean   | 0.128    |
| reference_action_std    | 0.971    |
| reference_actor_Q_mean  | 54.1     |
| reference_actor_Q_std   | 6.45     |
| rollout/Q_mean          | 64.4     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.2     |
| rollout/episodes        | 1.65e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 3.4e+03  |
| total/episodes          | 1.65e+04 |
| total/epochs            | 1        |
| total/steps             | 1639998  |
| total/steps_per_second  | 482      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.492    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1641000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1642000
Best mean reward: 94.03 - Last mean reward per episode: 92.98
Num timesteps: 1643000
Best mean reward: 94.03 - Last mean reward per episode: 92.73
Num timesteps: 1644000
Best mean reward: 94.03 - Last mean reward per episode: 92.52
Num timesteps: 1645000
Best mean reward: 94.03 - Last mean reward per episode: 92.26
Num timesteps: 1646000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
Num timesteps: 1647000
Best mean reward: 94.03 - Last mean reward per episode: 92.19
Num timesteps: 1648000
Best mean reward: 94.03 - Last mean reward per episode: 92.20
Num timesteps: 1649000
Best mean reward: 94.03 - Last mean reward per episode: 92.15
Num timesteps: 1650000
Best mean reward: 94.03 - Last mean reward per episode: 92.29
--------------------------------------
| reference_Q_mean        | 53.4     |
| reference_Q_std         | 6.61     |
| reference_action_mean   | -0.416   |
| reference_action_std    | 0.883    |
| reference_actor_Q_mean  | 54.1     |
| reference_actor_Q_std   | 6.38     |
| rollout/Q_mean          | 64.5     |
| rollout/actions_mean    | 0.133    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.2     |
| rollout/episodes        | 1.66e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.3     |
| total/duration          | 3.42e+03 |
| total/episodes          | 1.66e+04 |
| total/epochs            | 1        |
| total/steps             | 1649998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 0.458    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1651000
Best mean reward: 94.03 - Last mean reward per episode: 92.70
Num timesteps: 1652000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 1653000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1654000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 1655000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1656000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 1657000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 1658000
Best mean reward: 94.03 - Last mean reward per episode: 93.84
Num timesteps: 1659000
Best mean reward: 94.03 - Last mean reward per episode: 93.77
Num timesteps: 1660000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 6.79     |
| reference_action_mean   | -0.36    |
| reference_action_std    | 0.915    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.67     |
| rollout/Q_mean          | 64.5     |
| rollout/actions_mean    | 0.132    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99       |
| rollout/episodes        | 1.68e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.8     |
| total/duration          | 3.44e+03 |
| total/episodes          | 1.68e+04 |
| total/epochs            | 1        |
| total/steps             | 1659998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 0.337    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1661000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 1662000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 1663000
Best mean reward: 94.03 - Last mean reward per episode: 92.08
Num timesteps: 1664000
Best mean reward: 94.03 - Last mean reward per episode: 91.97
Num timesteps: 1665000
Best mean reward: 94.03 - Last mean reward per episode: 91.84
Num timesteps: 1666000
Best mean reward: 94.03 - Last mean reward per episode: 91.88
Num timesteps: 1667000
Best mean reward: 94.03 - Last mean reward per episode: 91.75
Num timesteps: 1668000
Best mean reward: 94.03 - Last mean reward per episode: 91.70
Num timesteps: 1669000
Best mean reward: 94.03 - Last mean reward per episode: 91.68
Num timesteps: 1670000
Best mean reward: 94.03 - Last mean reward per episode: 91.72
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 6.79     |
| reference_action_mean   | -0.305   |
| reference_action_std    | 0.928    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 6.71     |
| rollout/Q_mean          | 64.5     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99       |
| rollout/episodes        | 1.69e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 91.7     |
| total/duration          | 3.46e+03 |
| total/episodes          | 1.69e+04 |
| total/epochs            | 1        |
| total/steps             | 1669998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.343    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1671000
Best mean reward: 94.03 - Last mean reward per episode: 91.75
Num timesteps: 1672000
Best mean reward: 94.03 - Last mean reward per episode: 90.08
Num timesteps: 1673000
Best mean reward: 94.03 - Last mean reward per episode: 91.94
Num timesteps: 1674000
Best mean reward: 94.03 - Last mean reward per episode: 92.07
Num timesteps: 1675000
Best mean reward: 94.03 - Last mean reward per episode: 92.08
Num timesteps: 1676000
Best mean reward: 94.03 - Last mean reward per episode: 92.21
Num timesteps: 1677000
Best mean reward: 94.03 - Last mean reward per episode: 92.26
Num timesteps: 1678000
Best mean reward: 94.03 - Last mean reward per episode: 92.25
Num timesteps: 1679000
Best mean reward: 94.03 - Last mean reward per episode: 92.22
Num timesteps: 1680000
Best mean reward: 94.03 - Last mean reward per episode: 93.82
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 7.3      |
| reference_action_mean   | -0.26    |
| reference_action_std    | 0.954    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 7.21     |
| rollout/Q_mean          | 64.6     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.9     |
| rollout/episodes        | 1.7e+04  |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.8     |
| total/duration          | 3.48e+03 |
| total/episodes          | 1.7e+04  |
| total/epochs            | 1        |
| total/steps             | 1679998  |
| total/steps_per_second  | 483      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 0.461    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1681000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 1682000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 1683000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 1684000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1685000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1686000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 1687000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 1688000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1689000
Best mean reward: 94.03 - Last mean reward per episode: 93.06
Num timesteps: 1690000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 7.27     |
| reference_action_mean   | -0.391   |
| reference_action_std    | 0.905    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 7.18     |
| rollout/Q_mean          | 64.6     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.9     |
| rollout/episodes        | 1.71e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.1     |
| total/duration          | 3.5e+03  |
| total/episodes          | 1.71e+04 |
| total/epochs            | 1        |
| total/steps             | 1689998  |
| total/steps_per_second  | 482      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.437    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1691000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 1692000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 1693000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1694000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1695000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1696000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1697000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 1698000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1699000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1700000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 7.25     |
| reference_action_mean   | -0.246   |
| reference_action_std    | 0.959    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 6.98     |
| rollout/Q_mean          | 64.6     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.8     |
| rollout/episodes        | 1.72e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.4     |
| total/duration          | 3.52e+03 |
| total/episodes          | 1.72e+04 |
| total/epochs            | 1        |
| total/steps             | 1699998  |
| total/steps_per_second  | 482      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.276    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1701000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1702000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1703000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1704000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1705000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1706000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 1707000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 1708000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 1709000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 1710000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 7.04     |
| reference_action_mean   | -0.286   |
| reference_action_std    | 0.951    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 6.83     |
| rollout/Q_mean          | 64.7     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 1.73e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.6     |
| total/duration          | 3.55e+03 |
| total/episodes          | 1.73e+04 |
| total/epochs            | 1        |
| total/steps             | 1709998  |
| total/steps_per_second  | 482      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.272    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1711000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 1712000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1713000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 1714000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1715000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1716000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 1717000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1718000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 1719000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 1720000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 6.96     |
| reference_action_mean   | -0.422   |
| reference_action_std    | 0.883    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 6.85     |
| rollout/Q_mean          | 64.7     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.6     |
| rollout/episodes        | 1.74e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.5     |
| total/duration          | 3.57e+03 |
| total/episodes          | 1.74e+04 |
| total/epochs            | 1        |
| total/steps             | 1719998  |
| total/steps_per_second  | 482      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.189    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1721000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1722000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
Num timesteps: 1723000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
Num timesteps: 1724000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1725000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 1726000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1727000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 1728000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 1729000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1730000
Best mean reward: 94.03 - Last mean reward per episode: 91.68
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 7.02     |
| reference_action_mean   | -0.464   |
| reference_action_std    | 0.876    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 6.95     |
| rollout/Q_mean          | 64.7     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.6     |
| rollout/episodes        | 1.76e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 91.7     |
| total/duration          | 3.59e+03 |
| total/episodes          | 1.76e+04 |
| total/epochs            | 1        |
| total/steps             | 1729998  |
| total/steps_per_second  | 482      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.241    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1731000
Best mean reward: 94.03 - Last mean reward per episode: 91.66
Num timesteps: 1732000
Best mean reward: 94.03 - Last mean reward per episode: 91.70
Num timesteps: 1733000
Best mean reward: 94.03 - Last mean reward per episode: 91.73
Num timesteps: 1734000
Best mean reward: 94.03 - Last mean reward per episode: 91.35
Num timesteps: 1735000
Best mean reward: 94.03 - Last mean reward per episode: 91.36
Num timesteps: 1736000
Best mean reward: 94.03 - Last mean reward per episode: 91.49
Num timesteps: 1737000
Best mean reward: 94.03 - Last mean reward per episode: 91.54
Num timesteps: 1738000
Best mean reward: 94.03 - Last mean reward per episode: 90.02
Num timesteps: 1739000
Best mean reward: 94.03 - Last mean reward per episode: 90.17
Num timesteps: 1740000
Best mean reward: 94.03 - Last mean reward per episode: 89.89
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 7.36     |
| reference_action_mean   | -0.452   |
| reference_action_std    | 0.887    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 7        |
| rollout/Q_mean          | 64.8     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.6     |
| rollout/episodes        | 1.76e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 89.9     |
| total/duration          | 3.61e+03 |
| total/episodes          | 1.76e+04 |
| total/epochs            | 1        |
| total/steps             | 1739998  |
| total/steps_per_second  | 481      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.236    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1741000
Best mean reward: 94.03 - Last mean reward per episode: 91.74
Num timesteps: 1742000
Best mean reward: 94.03 - Last mean reward per episode: 91.70
Num timesteps: 1743000
Best mean reward: 94.03 - Last mean reward per episode: 92.02
Num timesteps: 1744000
Best mean reward: 94.03 - Last mean reward per episode: 91.94
Num timesteps: 1745000
Best mean reward: 94.03 - Last mean reward per episode: 90.02
Num timesteps: 1746000
Best mean reward: 94.03 - Last mean reward per episode: 89.79
Num timesteps: 1747000
Best mean reward: 94.03 - Last mean reward per episode: 89.66
Num timesteps: 1748000
Best mean reward: 94.03 - Last mean reward per episode: 89.59
Num timesteps: 1749000
Best mean reward: 94.03 - Last mean reward per episode: 91.06
Num timesteps: 1750000
Best mean reward: 94.03 - Last mean reward per episode: 91.21
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 7.64     |
| reference_action_mean   | -0.428   |
| reference_action_std    | 0.889    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 7.13     |
| rollout/Q_mean          | 64.8     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.6     |
| rollout/episodes        | 1.77e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 91.2     |
| total/duration          | 3.63e+03 |
| total/episodes          | 1.77e+04 |
| total/epochs            | 1        |
| total/steps             | 1749998  |
| total/steps_per_second  | 482      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 1.41     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1751000
Best mean reward: 94.03 - Last mean reward per episode: 91.20
Num timesteps: 1752000
Best mean reward: 94.03 - Last mean reward per episode: 91.06
Num timesteps: 1753000
Best mean reward: 94.03 - Last mean reward per episode: 91.11
Num timesteps: 1754000
Best mean reward: 94.03 - Last mean reward per episode: 90.93
Num timesteps: 1755000
Best mean reward: 94.03 - Last mean reward per episode: 92.88
Num timesteps: 1756000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 1757000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 1758000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 1759000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 1760000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 7.23     |
| reference_action_mean   | -0.374   |
| reference_action_std    | 0.912    |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 6.82     |
| rollout/Q_mean          | 64.8     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.6     |
| rollout/episodes        | 1.79e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.5     |
| total/duration          | 3.65e+03 |
| total/episodes          | 1.79e+04 |
| total/epochs            | 1        |
| total/steps             | 1759998  |
| total/steps_per_second  | 482      |
| train/loss_actor        | -70      |
| train/loss_critic       | 1.31     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1761000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1762000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1763000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 1764000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 1765000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1766000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 1767000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1768000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 1769000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1770000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
--------------------------------------
| reference_Q_mean        | 52.6     |
| reference_Q_std         | 6.85     |
| reference_action_mean   | -0.433   |
| reference_action_std    | 0.884    |
| reference_actor_Q_mean  | 53.8     |
| reference_actor_Q_std   | 6.64     |
| rollout/Q_mean          | 64.8     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.5     |
| rollout/episodes        | 1.8e+04  |
| rollout/return          | 92       |
| rollout/return_history  | 93.5     |
| total/duration          | 3.68e+03 |
| total/episodes          | 1.8e+04  |
| total/epochs            | 1        |
| total/steps             | 1769998  |
| total/steps_per_second  | 481      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 1.31     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1771000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1772000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 1773000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 1774000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 1775000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1776000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 1777000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1778000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 1779000
Best mean reward: 94.03 - Last mean reward per episode: 91.91
Num timesteps: 1780000
Best mean reward: 94.03 - Last mean reward per episode: 91.91
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 7.12     |
| reference_action_mean   | -0.529   |
| reference_action_std    | 0.827    |
| reference_actor_Q_mean  | 54       |
| reference_actor_Q_std   | 6.63     |
| rollout/Q_mean          | 64.9     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.5     |
| rollout/episodes        | 1.81e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 91.9     |
| total/duration          | 3.7e+03  |
| total/episodes          | 1.81e+04 |
| total/epochs            | 1        |
| total/steps             | 1779998  |
| total/steps_per_second  | 481      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.305    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1781000
Best mean reward: 94.03 - Last mean reward per episode: 92.05
Num timesteps: 1782000
Best mean reward: 94.03 - Last mean reward per episode: 91.99
Num timesteps: 1783000
Best mean reward: 94.03 - Last mean reward per episode: 91.97
Num timesteps: 1784000
Best mean reward: 94.03 - Last mean reward per episode: 91.69
Num timesteps: 1785000
Best mean reward: 94.03 - Last mean reward per episode: 91.61
Num timesteps: 1786000
Best mean reward: 94.03 - Last mean reward per episode: 91.43
Num timesteps: 1787000
Best mean reward: 94.03 - Last mean reward per episode: 91.43
Num timesteps: 1788000
Best mean reward: 94.03 - Last mean reward per episode: 91.58
Num timesteps: 1789000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 1790000
Best mean reward: 94.03 - Last mean reward per episode: 92.96
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 7.26     |
| reference_action_mean   | -0.472   |
| reference_action_std    | 0.873    |
| reference_actor_Q_mean  | 53.8     |
| reference_actor_Q_std   | 6.81     |
| rollout/Q_mean          | 64.9     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.5     |
| rollout/episodes        | 1.82e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93       |
| total/duration          | 3.72e+03 |
| total/episodes          | 1.82e+04 |
| total/epochs            | 1        |
| total/steps             | 1789998  |
| total/steps_per_second  | 481      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.278    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1791000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 1792000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 1793000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 1794000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1795000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1796000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1797000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 1798000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1799000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 1800000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 7.43     |
| reference_action_mean   | -0.464   |
| reference_action_std    | 0.874    |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 6.96     |
| rollout/Q_mean          | 64.9     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.4     |
| rollout/episodes        | 1.83e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.4     |
| total/duration          | 3.74e+03 |
| total/episodes          | 1.83e+04 |
| total/epochs            | 1        |
| total/steps             | 1799998  |
| total/steps_per_second  | 481      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.309    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1801000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 1802000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1803000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1804000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 1805000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1806000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1807000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1808000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 1809000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 1810000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 7.54     |
| reference_action_mean   | -0.423   |
| reference_action_std    | 0.904    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 6.9      |
| rollout/Q_mean          | 65       |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.3     |
| rollout/episodes        | 1.84e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.6     |
| total/duration          | 3.76e+03 |
| total/episodes          | 1.84e+04 |
| total/epochs            | 1        |
| total/steps             | 1809998  |
| total/steps_per_second  | 481      |
| train/loss_actor        | -71      |
| train/loss_critic       | 0.329    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1811000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1812000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1813000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1814000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 1815000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 1816000
Best mean reward: 94.03 - Last mean reward per episode: 92.68
Num timesteps: 1817000
Best mean reward: 94.03 - Last mean reward per episode: 92.63
Num timesteps: 1818000
Best mean reward: 94.03 - Last mean reward per episode: 92.65
Num timesteps: 1819000
Best mean reward: 94.03 - Last mean reward per episode: 92.68
Num timesteps: 1820000
Best mean reward: 94.03 - Last mean reward per episode: 92.68
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 6.97     |
| reference_action_mean   | -0.378   |
| reference_action_std    | 0.923    |
| reference_actor_Q_mean  | 54.2     |
| reference_actor_Q_std   | 6.35     |
| rollout/Q_mean          | 65       |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.3     |
| rollout/episodes        | 1.85e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 92.7     |
| total/duration          | 3.79e+03 |
| total/episodes          | 1.85e+04 |
| total/epochs            | 1        |
| total/steps             | 1819998  |
| total/steps_per_second  | 481      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.28     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1821000
Best mean reward: 94.03 - Last mean reward per episode: 92.68
Num timesteps: 1822000
Best mean reward: 94.03 - Last mean reward per episode: 92.86
Num timesteps: 1823000
Best mean reward: 94.03 - Last mean reward per episode: 92.61
Num timesteps: 1824000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
Num timesteps: 1825000
Best mean reward: 94.03 - Last mean reward per episode: 92.94
Num timesteps: 1826000
Best mean reward: 94.03 - Last mean reward per episode: 92.97
Num timesteps: 1827000
Best mean reward: 94.03 - Last mean reward per episode: 92.91
Num timesteps: 1828000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
Num timesteps: 1829000
Best mean reward: 94.03 - Last mean reward per episode: 92.51
Num timesteps: 1830000
Best mean reward: 94.03 - Last mean reward per episode: 92.50
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 6.78     |
| reference_action_mean   | -0.376   |
| reference_action_std    | 0.924    |
| reference_actor_Q_mean  | 54.4     |
| reference_actor_Q_std   | 6.14     |
| rollout/Q_mean          | 65       |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.3     |
| rollout/episodes        | 1.86e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 92.5     |
| total/duration          | 3.81e+03 |
| total/episodes          | 1.86e+04 |
| total/epochs            | 1        |
| total/steps             | 1829998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.344    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1831000
Best mean reward: 94.03 - Last mean reward per episode: 92.44
Num timesteps: 1832000
Best mean reward: 94.03 - Last mean reward per episode: 92.68
Num timesteps: 1833000
Best mean reward: 94.03 - Last mean reward per episode: 92.79
Num timesteps: 1834000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 1835000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 1836000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 1837000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 1838000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1839000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 1840000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 6.62     |
| reference_action_mean   | -0.436   |
| reference_action_std    | 0.898    |
| reference_actor_Q_mean  | 53.8     |
| reference_actor_Q_std   | 6.18     |
| rollout/Q_mean          | 65.1     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.2     |
| rollout/episodes        | 1.87e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.4     |
| total/duration          | 3.83e+03 |
| total/episodes          | 1.87e+04 |
| total/epochs            | 1        |
| total/steps             | 1839998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.331    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1841000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 1842000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 1843000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 1844000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 1845000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1846000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1847000
Best mean reward: 94.03 - Last mean reward per episode: 91.59
Num timesteps: 1848000
Best mean reward: 94.03 - Last mean reward per episode: 91.68
Num timesteps: 1849000
Best mean reward: 94.03 - Last mean reward per episode: 91.80
Num timesteps: 1850000
Best mean reward: 94.03 - Last mean reward per episode: 91.86
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 6.56     |
| reference_action_mean   | -0.549   |
| reference_action_std    | 0.832    |
| reference_actor_Q_mean  | 53.4     |
| reference_actor_Q_std   | 6.37     |
| rollout/Q_mean          | 65.1     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.2     |
| rollout/episodes        | 1.88e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 91.9     |
| total/duration          | 3.85e+03 |
| total/episodes          | 1.88e+04 |
| total/epochs            | 1        |
| total/steps             | 1849998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.282    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1851000
Best mean reward: 94.03 - Last mean reward per episode: 92.02
Num timesteps: 1852000
Best mean reward: 94.03 - Last mean reward per episode: 92.03
Num timesteps: 1853000
Best mean reward: 94.03 - Last mean reward per episode: 92.03
Num timesteps: 1854000
Best mean reward: 94.03 - Last mean reward per episode: 91.92
Num timesteps: 1855000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 1856000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 1857000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 1858000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 1859000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 1860000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
--------------------------------------
| reference_Q_mean        | 52.9     |
| reference_Q_std         | 6.65     |
| reference_action_mean   | -0.334   |
| reference_action_std    | 0.937    |
| reference_actor_Q_mean  | 53.4     |
| reference_actor_Q_std   | 6.58     |
| rollout/Q_mean          | 65.1     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 98.1     |
| rollout/episodes        | 1.9e+04  |
| rollout/return          | 92       |
| rollout/return_history  | 93.2     |
| total/duration          | 3.87e+03 |
| total/episodes          | 1.9e+04  |
| total/epochs            | 1        |
| total/steps             | 1859998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 0.25     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1861000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 1862000
Best mean reward: 94.03 - Last mean reward per episode: 93.20
Num timesteps: 1863000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 1864000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 1865000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 1866000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1867000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 1868000
Best mean reward: 94.03 - Last mean reward per episode: 92.96
Num timesteps: 1869000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 1870000
Best mean reward: 94.03 - Last mean reward per episode: 92.87
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 6.82     |
| reference_action_mean   | -0.26    |
| reference_action_std    | 0.959    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.76     |
| rollout/Q_mean          | 65.1     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 98.1     |
| rollout/episodes        | 1.91e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 92.9     |
| total/duration          | 3.89e+03 |
| total/episodes          | 1.91e+04 |
| total/epochs            | 1        |
| total/steps             | 1869998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 0.287    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1871000
Best mean reward: 94.03 - Last mean reward per episode: 92.73
Num timesteps: 1872000
Best mean reward: 94.03 - Last mean reward per episode: 92.79
Num timesteps: 1873000
Best mean reward: 94.03 - Last mean reward per episode: 92.73
Num timesteps: 1874000
Best mean reward: 94.03 - Last mean reward per episode: 92.70
Num timesteps: 1875000
Best mean reward: 94.03 - Last mean reward per episode: 92.65
Num timesteps: 1876000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 1877000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1878000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1879000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 1880000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 6.9      |
| reference_action_mean   | -0.424   |
| reference_action_std    | 0.902    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 6.88     |
| rollout/Q_mean          | 65.1     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 98       |
| rollout/episodes        | 1.92e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 3.91e+03 |
| total/episodes          | 1.92e+04 |
| total/epochs            | 1        |
| total/steps             | 1879998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.27     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1881000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1882000
Best mean reward: 94.03 - Last mean reward per episode: 92.06
Num timesteps: 1883000
Best mean reward: 94.03 - Last mean reward per episode: 92.07
Num timesteps: 1884000
Best mean reward: 94.03 - Last mean reward per episode: 91.70
Num timesteps: 1885000
Best mean reward: 94.03 - Last mean reward per episode: 91.82
Num timesteps: 1886000
Best mean reward: 94.03 - Last mean reward per episode: 91.82
Num timesteps: 1887000
Best mean reward: 94.03 - Last mean reward per episode: 91.73
Num timesteps: 1888000
Best mean reward: 94.03 - Last mean reward per episode: 91.54
Num timesteps: 1889000
Best mean reward: 94.03 - Last mean reward per episode: 91.52
Num timesteps: 1890000
Best mean reward: 94.03 - Last mean reward per episode: 91.10
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 6.85     |
| reference_action_mean   | -0.421   |
| reference_action_std    | 0.904    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 6.86     |
| rollout/Q_mean          | 65.2     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 98.1     |
| rollout/episodes        | 1.93e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 91.1     |
| total/duration          | 3.93e+03 |
| total/episodes          | 1.93e+04 |
| total/epochs            | 1        |
| total/steps             | 1889998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 0.427    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1891000
Best mean reward: 94.03 - Last mean reward per episode: 90.78
Num timesteps: 1892000
Best mean reward: 94.03 - Last mean reward per episode: 90.77
Num timesteps: 1893000
Best mean reward: 94.03 - Last mean reward per episode: 92.24
Num timesteps: 1894000
Best mean reward: 94.03 - Last mean reward per episode: 92.54
Num timesteps: 1895000
Best mean reward: 94.03 - Last mean reward per episode: 92.56
Num timesteps: 1896000
Best mean reward: 94.03 - Last mean reward per episode: 92.54
Num timesteps: 1897000
Best mean reward: 94.03 - Last mean reward per episode: 92.64
Num timesteps: 1898000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 1899000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 1900000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 7.06     |
| reference_action_mean   | -0.732   |
| reference_action_std    | 0.678    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 7.07     |
| rollout/Q_mean          | 65.2     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 98       |
| rollout/episodes        | 1.94e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.96e+03 |
| total/episodes          | 1.94e+04 |
| total/epochs            | 1        |
| total/steps             | 1899998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 0.391    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1901000
Best mean reward: 94.03 - Last mean reward per episode: 93.16
Num timesteps: 1902000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 1903000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 1904000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 1905000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 1906000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 1907000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 1908000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 1909000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1910000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 7.27     |
| reference_action_mean   | -0.621   |
| reference_action_std    | 0.77     |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 7.26     |
| rollout/Q_mean          | 65.2     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 98       |
| rollout/episodes        | 1.95e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.98e+03 |
| total/episodes          | 1.95e+04 |
| total/epochs            | 1        |
| total/steps             | 1909998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.327    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1911000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 1912000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 1913000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1914000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 1915000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
Num timesteps: 1916000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1917000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1918000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1919000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1920000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 7.48     |
| reference_action_mean   | -0.49    |
| reference_action_std    | 0.867    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 7.26     |
| rollout/Q_mean          | 65.2     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.9     |
| rollout/episodes        | 1.96e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 4e+03    |
| total/episodes          | 1.96e+04 |
| total/epochs            | 1        |
| total/steps             | 1919998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.361    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1921000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1922000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1923000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 1924000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 1925000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1926000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 1927000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
Num timesteps: 1928000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1929000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 1930000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 8.09     |
| reference_action_mean   | -0.386   |
| reference_action_std    | 0.911    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 7.76     |
| rollout/Q_mean          | 65.3     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.9     |
| rollout/episodes        | 1.97e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.2     |
| total/duration          | 4.03e+03 |
| total/episodes          | 1.97e+04 |
| total/epochs            | 1        |
| total/steps             | 1929998  |
| total/steps_per_second  | 479      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.331    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1931000
Best mean reward: 94.03 - Last mean reward per episode: 93.16
Num timesteps: 1932000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 1933000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1934000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 1935000
Best mean reward: 94.03 - Last mean reward per episode: 93.08
Num timesteps: 1936000
Best mean reward: 94.03 - Last mean reward per episode: 93.07
Num timesteps: 1937000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
Num timesteps: 1938000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 1939000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 1940000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 8.26     |
| reference_action_mean   | -0.459   |
| reference_action_std    | 0.873    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 8.04     |
| rollout/Q_mean          | 65.3     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.8     |
| rollout/episodes        | 1.98e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.3     |
| total/duration          | 4.05e+03 |
| total/episodes          | 1.98e+04 |
| total/epochs            | 1        |
| total/steps             | 1939998  |
| total/steps_per_second  | 479      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.424    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1941000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 1942000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1943000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1944000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 1945000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1946000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 1947000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1948000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 1949000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 1950000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 7.94     |
| reference_action_mean   | -0.381   |
| reference_action_std    | 0.916    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 7.67     |
| rollout/Q_mean          | 65.3     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.7     |
| rollout/episodes        | 2e+04    |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 4.07e+03 |
| total/episodes          | 2e+04    |
| total/epochs            | 1        |
| total/steps             | 1949998  |
| total/steps_per_second  | 479      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.253    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1951000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 1952000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 1953000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1954000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 1955000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1956000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 1957000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 1958000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 1959000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1960000
Best mean reward: 94.03 - Last mean reward per episode: 93.58
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 7.59     |
| reference_action_mean   | -0.47    |
| reference_action_std    | 0.873    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 7.26     |
| rollout/Q_mean          | 65.4     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.7     |
| rollout/episodes        | 2.01e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.6     |
| total/duration          | 4.09e+03 |
| total/episodes          | 2.01e+04 |
| total/epochs            | 1        |
| total/steps             | 1959998  |
| total/steps_per_second  | 479      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.353    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1961000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1962000
Best mean reward: 94.03 - Last mean reward per episode: 93.07
Num timesteps: 1963000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 1964000
Best mean reward: 94.03 - Last mean reward per episode: 92.89
Num timesteps: 1965000
Best mean reward: 94.03 - Last mean reward per episode: 92.90
Num timesteps: 1966000
Best mean reward: 94.03 - Last mean reward per episode: 92.99
Num timesteps: 1967000
Best mean reward: 94.03 - Last mean reward per episode: 92.79
Num timesteps: 1968000
Best mean reward: 94.03 - Last mean reward per episode: 92.72
Num timesteps: 1969000
Best mean reward: 94.03 - Last mean reward per episode: 91.19
Num timesteps: 1970000
Best mean reward: 94.03 - Last mean reward per episode: 90.94
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.6      |
| reference_action_mean   | -0.453   |
| reference_action_std    | 0.89     |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 7.12     |
| rollout/Q_mean          | 65.4     |
| rollout/actions_mean    | 0.127    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.8     |
| rollout/episodes        | 2.01e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 90.9     |
| total/duration          | 4.11e+03 |
| total/episodes          | 2.01e+04 |
| total/epochs            | 1        |
| total/steps             | 1969998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.338    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1971000
Best mean reward: 94.03 - Last mean reward per episode: 90.98
Num timesteps: 1972000
Best mean reward: 94.03 - Last mean reward per episode: 90.96
Num timesteps: 1973000
Best mean reward: 94.03 - Last mean reward per episode: 91.42
Num timesteps: 1974000
Best mean reward: 94.03 - Last mean reward per episode: 91.67
Num timesteps: 1975000
Best mean reward: 94.03 - Last mean reward per episode: 91.57
Num timesteps: 1976000
Best mean reward: 94.03 - Last mean reward per episode: 91.58
Num timesteps: 1977000
Best mean reward: 94.03 - Last mean reward per episode: 91.80
Num timesteps: 1978000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 1979000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 1980000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 7.49     |
| reference_action_mean   | -0.402   |
| reference_action_std    | 0.904    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 7.07     |
| rollout/Q_mean          | 65.4     |
| rollout/actions_mean    | 0.127    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.6     |
| rollout/episodes        | 2.03e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.6     |
| total/duration          | 4.13e+03 |
| total/episodes          | 2.03e+04 |
| total/epochs            | 1        |
| total/steps             | 1979998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.262    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1981000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 1982000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 1983000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 1984000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 1985000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 1986000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 1987000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
Num timesteps: 1988000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 1989000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 1990000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.33     |
| reference_action_mean   | -0.435   |
| reference_action_std    | 0.891    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.97     |
| rollout/Q_mean          | 65.4     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.6     |
| rollout/episodes        | 2.04e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 4.15e+03 |
| total/episodes          | 2.04e+04 |
| total/epochs            | 1        |
| total/steps             | 1989998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.312    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1991000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 1992000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 1993000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 1994000
Best mean reward: 94.03 - Last mean reward per episode: 93.20
Num timesteps: 1995000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 1996000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 1997000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 1998000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 1999000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 2000000
Best mean reward: 94.03 - Last mean reward per episode: 93.61
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.3      |
| reference_action_mean   | -0.431   |
| reference_action_std    | 0.892    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.99     |
| rollout/Q_mean          | 65.5     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.5     |
| rollout/episodes        | 2.05e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.6     |
| total/duration          | 4.17e+03 |
| total/episodes          | 2.05e+04 |
| total/epochs            | 1        |
| total/steps             | 1999998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.795    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2001000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 2002000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 2003000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
Num timesteps: 2004000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 2005000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 2006000
Best mean reward: 94.03 - Last mean reward per episode: 93.01
Num timesteps: 2007000
Best mean reward: 94.03 - Last mean reward per episode: 93.04
Num timesteps: 2008000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 2009000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 2010000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.4      |
| reference_action_mean   | -0.422   |
| reference_action_std    | 0.9      |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 7.04     |
| rollout/Q_mean          | 65.5     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.5     |
| rollout/episodes        | 2.06e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.3     |
| total/duration          | 4.19e+03 |
| total/episodes          | 2.06e+04 |
| total/epochs            | 1        |
| total/steps             | 2009998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.318    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2011000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 2012000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 2013000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 2014000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 2015000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 2016000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 2017000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 2018000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 2019000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 2020000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.33     |
| reference_action_mean   | -0.44    |
| reference_action_std    | 0.886    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 6.95     |
| rollout/Q_mean          | 65.5     |
| rollout/actions_mean    | 0.128    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.4     |
| rollout/episodes        | 2.07e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 4.21e+03 |
| total/episodes          | 2.07e+04 |
| total/epochs            | 1        |
| total/steps             | 2019998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.31     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2021000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 2022000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 2023000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 2024000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 2025000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 2026000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 2027000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 2028000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 2029000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 2030000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 7.31     |
| reference_action_mean   | -0.405   |
| reference_action_std    | 0.899    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.96     |
| rollout/Q_mean          | 65.5     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.4     |
| rollout/episodes        | 2.08e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.7     |
| total/duration          | 4.23e+03 |
| total/episodes          | 2.08e+04 |
| total/epochs            | 1        |
| total/steps             | 2029998  |
| total/steps_per_second  | 479      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.324    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2031000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 2032000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 2033000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 2034000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 2035000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 2036000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 2037000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 2038000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 2039000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 2040000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 7.06     |
| reference_action_mean   | -0.364   |
| reference_action_std    | 0.927    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 6.63     |
| rollout/Q_mean          | 65.6     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.3     |
| rollout/episodes        | 2.1e+04  |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.2     |
| total/duration          | 4.25e+03 |
| total/episodes          | 2.1e+04  |
| total/epochs            | 1        |
| total/steps             | 2039998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.311    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2041000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 2042000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 2043000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 2044000
Best mean reward: 94.03 - Last mean reward per episode: 92.94
Num timesteps: 2045000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 2046000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 2047000
Best mean reward: 94.03 - Last mean reward per episode: 93.07
Num timesteps: 2048000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 2049000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 2050000
Best mean reward: 94.03 - Last mean reward per episode: 93.27
--------------------------------------
| reference_Q_mean        | 52.9     |
| reference_Q_std         | 6.65     |
| reference_action_mean   | -0.779   |
| reference_action_std    | 0.624    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 6.72     |
| rollout/Q_mean          | 65.6     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.3     |
| rollout/episodes        | 2.11e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.3     |
| total/duration          | 4.27e+03 |
| total/episodes          | 2.11e+04 |
| total/epochs            | 1        |
| total/steps             | 2049998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.247    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2051000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 2052000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 2053000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 2054000
Best mean reward: 94.03 - Last mean reward per episode: 93.83
Num timesteps: 2055000
Best mean reward: 94.03 - Last mean reward per episode: 93.92
Num timesteps: 2056000
Best mean reward: 94.03 - Last mean reward per episode: 93.88
Num timesteps: 2057000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 2058000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 2059000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 2060000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 6.93     |
| reference_action_mean   | -0.643   |
| reference_action_std    | 0.754    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 6.85     |
| rollout/Q_mean          | 65.6     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.12e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 4.29e+03 |
| total/episodes          | 2.12e+04 |
| total/epochs            | 1        |
| total/steps             | 2059998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.271    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2061000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 2062000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 2063000
Best mean reward: 94.03 - Last mean reward per episode: 92.97
Num timesteps: 2064000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 2065000
Best mean reward: 94.03 - Last mean reward per episode: 92.88
Num timesteps: 2066000
Best mean reward: 94.03 - Last mean reward per episode: 92.83
Num timesteps: 2067000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 2068000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 2069000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 2070000
Best mean reward: 94.03 - Last mean reward per episode: 91.41
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 7.01     |
| reference_action_mean   | -0.147   |
| reference_action_std    | 0.975    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.66     |
| rollout/Q_mean          | 65.6     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.3     |
| rollout/episodes        | 2.13e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.4     |
| total/duration          | 4.32e+03 |
| total/episodes          | 2.13e+04 |
| total/epochs            | 1        |
| total/steps             | 2069998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.294    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2071000
Best mean reward: 94.03 - Last mean reward per episode: 91.25
Num timesteps: 2072000
Best mean reward: 94.03 - Last mean reward per episode: 91.11
Num timesteps: 2073000
Best mean reward: 94.03 - Last mean reward per episode: 91.26
Num timesteps: 2074000
Best mean reward: 94.03 - Last mean reward per episode: 91.26
Num timesteps: 2075000
Best mean reward: 94.03 - Last mean reward per episode: 91.34
Num timesteps: 2076000
Best mean reward: 94.03 - Last mean reward per episode: 91.32
Num timesteps: 2077000
Best mean reward: 94.03 - Last mean reward per episode: 91.29
Num timesteps: 2078000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 2079000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 2080000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 7.16     |
| reference_action_mean   | -0.514   |
| reference_action_std    | 0.849    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 6.75     |
| rollout/Q_mean          | 65.6     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.14e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 4.34e+03 |
| total/episodes          | 2.14e+04 |
| total/epochs            | 1        |
| total/steps             | 2079998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.308    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2081000
Best mean reward: 94.03 - Last mean reward per episode: 91.82
Num timesteps: 2082000
Best mean reward: 94.03 - Last mean reward per episode: 91.88
Num timesteps: 2083000
Best mean reward: 94.03 - Last mean reward per episode: 91.86
Num timesteps: 2084000
Best mean reward: 94.03 - Last mean reward per episode: 91.84
Num timesteps: 2085000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 2086000
Best mean reward: 94.03 - Last mean reward per episode: 91.88
Num timesteps: 2087000
Best mean reward: 94.03 - Last mean reward per episode: 91.73
Num timesteps: 2088000
Best mean reward: 94.03 - Last mean reward per episode: 91.70
Num timesteps: 2089000
Best mean reward: 94.03 - Last mean reward per episode: 91.65
Num timesteps: 2090000
Best mean reward: 94.03 - Last mean reward per episode: 91.54
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 7.3      |
| reference_action_mean   | -0.575   |
| reference_action_std    | 0.778    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 6.77     |
| rollout/Q_mean          | 65.7     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.15e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.5     |
| total/duration          | 4.36e+03 |
| total/episodes          | 2.15e+04 |
| total/epochs            | 1        |
| total/steps             | 2089998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.366    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2091000
Best mean reward: 94.03 - Last mean reward per episode: 93.28
Num timesteps: 2092000
Best mean reward: 94.03 - Last mean reward per episode: 92.91
Num timesteps: 2093000
Best mean reward: 94.03 - Last mean reward per episode: 93.05
Num timesteps: 2094000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 2095000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 2096000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 2097000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 2098000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 2099000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 2100000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 7.38     |
| reference_action_mean   | -0.559   |
| reference_action_std    | 0.799    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.94     |
| rollout/Q_mean          | 65.7     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.16e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.3     |
| total/duration          | 4.38e+03 |
| total/episodes          | 2.16e+04 |
| total/epochs            | 1        |
| total/steps             | 2099998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.366    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2101000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 2102000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 2103000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 2104000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 2105000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 2106000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 2107000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 2108000
Best mean reward: 94.03 - Last mean reward per episode: 93.03
Num timesteps: 2109000
Best mean reward: 94.03 - Last mean reward per episode: 92.73
Num timesteps: 2110000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 7.61     |
| reference_action_mean   | -0.543   |
| reference_action_std    | 0.822    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 6.96     |
| rollout/Q_mean          | 65.7     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.3     |
| rollout/episodes        | 2.17e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93       |
| total/duration          | 4.4e+03  |
| total/episodes          | 2.17e+04 |
| total/epochs            | 1        |
| total/steps             | 2109998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.21     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2111000
Best mean reward: 94.03 - Last mean reward per episode: 93.01
Num timesteps: 2112000
Best mean reward: 94.03 - Last mean reward per episode: 93.22
Num timesteps: 2113000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 2114000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 2115000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 2116000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 2117000
Best mean reward: 94.03 - Last mean reward per episode: 93.93
Num timesteps: 2118000
Best mean reward: 94.03 - Last mean reward per episode: 93.90
Num timesteps: 2119000
Best mean reward: 94.03 - Last mean reward per episode: 93.82
Num timesteps: 2120000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 8.04     |
| reference_action_mean   | -0.495   |
| reference_action_std    | 0.846    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.21     |
| rollout/Q_mean          | 65.7     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.18e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.7     |
| total/duration          | 4.42e+03 |
| total/episodes          | 2.18e+04 |
| total/epochs            | 1        |
| total/steps             | 2119998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.407    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2121000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 2122000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
Num timesteps: 2123000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 2124000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 2125000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 2126000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 2127000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
Num timesteps: 2128000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 2129000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 2130000
Best mean reward: 94.03 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 50.6     |
| reference_Q_std         | 7.95     |
| reference_action_mean   | -0.198   |
| reference_action_std    | 0.962    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 6.87     |
| rollout/Q_mean          | 65.7     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.19e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.1     |
| total/duration          | 4.44e+03 |
| total/episodes          | 2.19e+04 |
| total/epochs            | 1        |
| total/steps             | 2129998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.508    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2131000
Best mean reward: 94.03 - Last mean reward per episode: 92.98
Num timesteps: 2132000
Best mean reward: 94.03 - Last mean reward per episode: 92.86
Num timesteps: 2133000
Best mean reward: 94.03 - Last mean reward per episode: 92.87
Num timesteps: 2134000
Best mean reward: 94.03 - Last mean reward per episode: 92.90
Num timesteps: 2135000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 2136000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 2137000
Best mean reward: 94.03 - Last mean reward per episode: 93.24
Num timesteps: 2138000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 2139000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 2140000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 7.37     |
| reference_action_mean   | -0.51    |
| reference_action_std    | 0.843    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 6.56     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.2e+04  |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.6     |
| total/duration          | 4.46e+03 |
| total/episodes          | 2.2e+04  |
| total/epochs            | 1        |
| total/steps             | 2139998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.399    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2141000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 2142000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 2143000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 2144000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 2145000
Best mean reward: 94.03 - Last mean reward per episode: 93.81
Num timesteps: 2146000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
Num timesteps: 2147000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 2148000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 2149000
Best mean reward: 94.03 - Last mean reward per episode: 93.77
Num timesteps: 2150000
Best mean reward: 94.03 - Last mean reward per episode: 93.81
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 7.24     |
| reference_action_mean   | -0.484   |
| reference_action_std    | 0.858    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 6.53     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.22e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.8     |
| total/duration          | 4.48e+03 |
| total/episodes          | 2.22e+04 |
| total/epochs            | 1        |
| total/steps             | 2149998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.291    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2151000
Best mean reward: 94.03 - Last mean reward per episode: 93.82
Num timesteps: 2152000
Best mean reward: 94.03 - Last mean reward per episode: 93.82
Num timesteps: 2153000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 2154000
Best mean reward: 94.03 - Last mean reward per episode: 93.92
Num timesteps: 2155000
Best mean reward: 94.03 - Last mean reward per episode: 93.99
Num timesteps: 2156000
Best mean reward: 94.03 - Last mean reward per episode: 93.96
Num timesteps: 2157000
Best mean reward: 94.03 - Last mean reward per episode: 93.80
Num timesteps: 2158000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 2159000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 2160000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 6.9      |
| reference_action_mean   | -0.422   |
| reference_action_std    | 0.89     |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 6.33     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.22e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.6     |
| total/duration          | 4.5e+03  |
| total/episodes          | 2.22e+04 |
| total/epochs            | 1        |
| total/steps             | 2159998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.239    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2161000
Best mean reward: 94.03 - Last mean reward per episode: 93.66
Num timesteps: 2162000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 2163000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 2164000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 2165000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 2166000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 2167000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 2168000
Best mean reward: 94.03 - Last mean reward per episode: 93.65
Num timesteps: 2169000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 2170000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 6.84     |
| reference_action_mean   | -0.168   |
| reference_action_std    | 0.937    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 6.42     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.24e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.4     |
| total/duration          | 4.52e+03 |
| total/episodes          | 2.24e+04 |
| total/epochs            | 1        |
| total/steps             | 2169998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.274    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2171000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 2172000
Best mean reward: 94.03 - Last mean reward per episode: 93.21
Num timesteps: 2173000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 2174000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 2175000
Best mean reward: 94.03 - Last mean reward per episode: 92.54
Num timesteps: 2176000
Best mean reward: 94.03 - Last mean reward per episode: 92.49
Num timesteps: 2177000
Best mean reward: 94.03 - Last mean reward per episode: 92.51
Num timesteps: 2178000
Best mean reward: 94.03 - Last mean reward per episode: 92.57
Num timesteps: 2179000
Best mean reward: 94.03 - Last mean reward per episode: 92.55
Num timesteps: 2180000
Best mean reward: 94.03 - Last mean reward per episode: 92.85
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 6.54     |
| reference_action_mean   | -0.326   |
| reference_action_std    | 0.918    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 6.27     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.24e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 92.8     |
| total/duration          | 4.54e+03 |
| total/episodes          | 2.24e+04 |
| total/epochs            | 1        |
| total/steps             | 2179998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 0.256    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2181000
Best mean reward: 94.03 - Last mean reward per episode: 93.15
Num timesteps: 2182000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 2183000
Best mean reward: 94.03 - Last mean reward per episode: 93.29
Num timesteps: 2184000
Best mean reward: 94.03 - Last mean reward per episode: 93.23
Num timesteps: 2185000
Best mean reward: 94.03 - Last mean reward per episode: 93.82
Num timesteps: 2186000
Best mean reward: 94.03 - Last mean reward per episode: 93.80
Num timesteps: 2187000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 2188000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 2189000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 2190000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 6.36     |
| reference_action_mean   | -0.393   |
| reference_action_std    | 0.872    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.14     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.25e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.3     |
| total/duration          | 4.56e+03 |
| total/episodes          | 2.25e+04 |
| total/epochs            | 1        |
| total/steps             | 2189998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 0.327    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2191000
Best mean reward: 94.03 - Last mean reward per episode: 93.25
Num timesteps: 2192000
Best mean reward: 94.03 - Last mean reward per episode: 92.85
Num timesteps: 2193000
Best mean reward: 94.03 - Last mean reward per episode: 92.46
Num timesteps: 2194000
Best mean reward: 94.03 - Last mean reward per episode: 92.39
Num timesteps: 2195000
Best mean reward: 94.03 - Last mean reward per episode: 92.44
Num timesteps: 2196000
Best mean reward: 94.03 - Last mean reward per episode: 92.56
Num timesteps: 2197000
Best mean reward: 94.03 - Last mean reward per episode: 92.68
Num timesteps: 2198000
Best mean reward: 94.03 - Last mean reward per episode: 92.74
Num timesteps: 2199000
Best mean reward: 94.03 - Last mean reward per episode: 92.26
Num timesteps: 2200000
Best mean reward: 94.03 - Last mean reward per episode: 92.34
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 6.37     |
| reference_action_mean   | -0.3     |
| reference_action_std    | 0.934    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 6.22     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.26e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 92.3     |
| total/duration          | 4.58e+03 |
| total/episodes          | 2.26e+04 |
| total/epochs            | 1        |
| total/steps             | 2199998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 0.321    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2201000
Best mean reward: 94.03 - Last mean reward per episode: 90.70
Num timesteps: 2202000
Best mean reward: 94.03 - Last mean reward per episode: 90.81
Num timesteps: 2203000
Best mean reward: 94.03 - Last mean reward per episode: 91.55
Num timesteps: 2204000
Best mean reward: 94.03 - Last mean reward per episode: 91.60
Num timesteps: 2205000
Best mean reward: 94.03 - Last mean reward per episode: 91.49
Num timesteps: 2206000
Best mean reward: 94.03 - Last mean reward per episode: 91.49
Num timesteps: 2207000
Best mean reward: 94.03 - Last mean reward per episode: 91.55
Num timesteps: 2208000
Best mean reward: 94.03 - Last mean reward per episode: 91.47
Num timesteps: 2209000
Best mean reward: 94.03 - Last mean reward per episode: 92.07
Num timesteps: 2210000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 6.59     |
| reference_action_mean   | -0.419   |
| reference_action_std    | 0.889    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 6.61     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.27e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.7     |
| total/duration          | 4.6e+03  |
| total/episodes          | 2.27e+04 |
| total/epochs            | 1        |
| total/steps             | 2209998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -67.4    |
| train/loss_critic       | 0.307    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2211000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 2212000
Best mean reward: 94.03 - Last mean reward per episode: 93.75
Num timesteps: 2213000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 2214000
Best mean reward: 94.03 - Last mean reward per episode: 93.69
Num timesteps: 2215000
Best mean reward: 94.03 - Last mean reward per episode: 93.60
Num timesteps: 2216000
Best mean reward: 94.03 - Last mean reward per episode: 93.37
Num timesteps: 2217000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 2218000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 2219000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 2220000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 7.18     |
| reference_action_mean   | -0.372   |
| reference_action_std    | 0.919    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 7.36     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.28e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.4     |
| total/duration          | 4.62e+03 |
| total/episodes          | 2.28e+04 |
| total/epochs            | 1        |
| total/steps             | 2219998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 0.326    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2221000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 2222000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 2223000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 2224000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 2225000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 2226000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 2227000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 2228000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 2229000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 2230000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 7.62     |
| reference_action_mean   | -0.507   |
| reference_action_std    | 0.843    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 7.64     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.3e+04  |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.7     |
| total/duration          | 4.64e+03 |
| total/episodes          | 2.3e+04  |
| total/epochs            | 1        |
| total/steps             | 2229998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -68.1    |
| train/loss_critic       | 0.317    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2231000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 2232000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 2233000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 2234000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
Num timesteps: 2235000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 2236000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 2237000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 2238000
Best mean reward: 94.03 - Last mean reward per episode: 93.30
Num timesteps: 2239000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 2240000
Best mean reward: 94.03 - Last mean reward per episode: 93.55
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 7.83     |
| reference_action_mean   | -0.448   |
| reference_action_std    | 0.881    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 7.75     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.31e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.6     |
| total/duration          | 4.66e+03 |
| total/episodes          | 2.31e+04 |
| total/epochs            | 1        |
| total/steps             | 2239998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.362    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2241000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 2242000
Best mean reward: 94.03 - Last mean reward per episode: 93.36
Num timesteps: 2243000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
Num timesteps: 2244000
Best mean reward: 94.03 - Last mean reward per episode: 93.34
Num timesteps: 2245000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 2246000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 2247000
Best mean reward: 94.03 - Last mean reward per episode: 93.54
Num timesteps: 2248000
Best mean reward: 94.03 - Last mean reward per episode: 93.52
Num timesteps: 2249000
Best mean reward: 94.03 - Last mean reward per episode: 93.57
Num timesteps: 2250000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 8.07     |
| reference_action_mean   | -0.548   |
| reference_action_std    | 0.825    |
| reference_actor_Q_mean  | 51.2     |
| reference_actor_Q_std   | 7.95     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97       |
| rollout/episodes        | 2.32e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 4.69e+03 |
| total/episodes          | 2.32e+04 |
| total/epochs            | 1        |
| total/steps             | 2249998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 0.686    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2251000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 2252000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 2253000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 2254000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 2255000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 2256000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 2257000
Best mean reward: 94.03 - Last mean reward per episode: 93.85
Num timesteps: 2258000
Best mean reward: 94.03 - Last mean reward per episode: 93.90
Num timesteps: 2259000
Best mean reward: 94.03 - Last mean reward per episode: 93.87
Num timesteps: 2260000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 8.29     |
| reference_action_mean   | -0.444   |
| reference_action_std    | 0.886    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 8.06     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 96.9     |
| rollout/episodes        | 2.33e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.8     |
| total/duration          | 4.71e+03 |
| total/episodes          | 2.33e+04 |
| total/epochs            | 1        |
| total/steps             | 2259998  |
| total/steps_per_second  | 480      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.572    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2261000
Best mean reward: 94.03 - Last mean reward per episode: 93.62
Num timesteps: 2262000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 2263000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 2264000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 2265000
Best mean reward: 94.03 - Last mean reward per episode: 93.11
Num timesteps: 2266000
Best mean reward: 94.03 - Last mean reward per episode: 93.13
Num timesteps: 2267000
Best mean reward: 94.03 - Last mean reward per episode: 93.04
Num timesteps: 2268000
Best mean reward: 94.03 - Last mean reward per episode: 93.04
Num timesteps: 2269000
Best mean reward: 94.03 - Last mean reward per episode: 91.43
Num timesteps: 2270000
Best mean reward: 94.03 - Last mean reward per episode: 91.45
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 7.71     |
| reference_action_mean   | -0.574   |
| reference_action_std    | 0.797    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 7.73     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97       |
| rollout/episodes        | 2.34e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.4     |
| total/duration          | 4.74e+03 |
| total/episodes          | 2.34e+04 |
| total/epochs            | 1        |
| total/steps             | 2269998  |
| total/steps_per_second  | 479      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.491    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2271000
Best mean reward: 94.03 - Last mean reward per episode: 91.40
Num timesteps: 2272000
Best mean reward: 94.03 - Last mean reward per episode: 91.87
Num timesteps: 2273000
Best mean reward: 94.03 - Last mean reward per episode: 91.99
Num timesteps: 2274000
Best mean reward: 94.03 - Last mean reward per episode: 91.72
Num timesteps: 2275000
Best mean reward: 94.03 - Last mean reward per episode: 91.71
Num timesteps: 2276000
Best mean reward: 94.03 - Last mean reward per episode: 91.73
Num timesteps: 2277000
Best mean reward: 94.03 - Last mean reward per episode: 91.76
Num timesteps: 2278000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 2279000
Best mean reward: 94.03 - Last mean reward per episode: 93.16
Num timesteps: 2280000
Best mean reward: 94.03 - Last mean reward per episode: 91.46
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 7.23     |
| reference_action_mean   | -0.321   |
| reference_action_std    | 0.94     |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.16     |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97       |
| rollout/episodes        | 2.35e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.5     |
| total/duration          | 4.76e+03 |
| total/episodes          | 2.35e+04 |
| total/epochs            | 1        |
| total/steps             | 2279998  |
| total/steps_per_second  | 479      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.416    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2281000
Best mean reward: 94.03 - Last mean reward per episode: 91.40
Num timesteps: 2282000
Best mean reward: 94.03 - Last mean reward per episode: 89.99
Num timesteps: 2283000
Best mean reward: 94.03 - Last mean reward per episode: 89.98
Num timesteps: 2284000
Best mean reward: 94.03 - Last mean reward per episode: 88.44
Num timesteps: 2285000
Best mean reward: 94.03 - Last mean reward per episode: 88.34
Num timesteps: 2286000
Best mean reward: 94.03 - Last mean reward per episode: 86.72
Num timesteps: 2287000
Best mean reward: 94.03 - Last mean reward per episode: 86.92
Num timesteps: 2288000
Best mean reward: 94.03 - Last mean reward per episode: 85.35
Num timesteps: 2289000
Best mean reward: 94.03 - Last mean reward per episode: 85.29
Num timesteps: 2290000
Best mean reward: 94.03 - Last mean reward per episode: 85.28
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 7.03     |
| reference_action_mean   | -0.146   |
| reference_action_std    | 0.983    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.99     |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.36e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 85.3     |
| total/duration          | 4.78e+03 |
| total/episodes          | 2.36e+04 |
| total/epochs            | 1        |
| total/steps             | 2289998  |
| total/steps_per_second  | 479      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 1.4      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2291000
Best mean reward: 94.03 - Last mean reward per episode: 85.13
Num timesteps: 2292000
Best mean reward: 94.03 - Last mean reward per episode: 85.25
Num timesteps: 2293000
Best mean reward: 94.03 - Last mean reward per episode: 88.49
Num timesteps: 2294000
Best mean reward: 94.03 - Last mean reward per episode: 88.52
Num timesteps: 2295000
Best mean reward: 94.03 - Last mean reward per episode: 90.12
Num timesteps: 2296000
Best mean reward: 94.03 - Last mean reward per episode: 91.74
Num timesteps: 2297000
Best mean reward: 94.03 - Last mean reward per episode: 91.74
Num timesteps: 2298000
Best mean reward: 94.03 - Last mean reward per episode: 92.06
Num timesteps: 2299000
Best mean reward: 94.03 - Last mean reward per episode: 92.08
Num timesteps: 2300000
Best mean reward: 94.03 - Last mean reward per episode: 92.20
--------------------------------------
| reference_Q_mean        | 52.6     |
| reference_Q_std         | 6.87     |
| reference_action_mean   | -0.00703 |
| reference_action_std    | 0.99     |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 6.56     |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.37e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 92.2     |
| total/duration          | 4.81e+03 |
| total/episodes          | 2.37e+04 |
| total/epochs            | 1        |
| total/steps             | 2299998  |
| total/steps_per_second  | 479      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 0.36     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2301000
Best mean reward: 94.03 - Last mean reward per episode: 92.11
Num timesteps: 2302000
Best mean reward: 94.03 - Last mean reward per episode: 92.13
Num timesteps: 2303000
Best mean reward: 94.03 - Last mean reward per episode: 92.17
Num timesteps: 2304000
Best mean reward: 94.03 - Last mean reward per episode: 92.20
Num timesteps: 2305000
Best mean reward: 94.03 - Last mean reward per episode: 92.20
Num timesteps: 2306000
Best mean reward: 94.03 - Last mean reward per episode: 93.59
Num timesteps: 2307000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 2308000
Best mean reward: 94.03 - Last mean reward per episode: 91.83
Num timesteps: 2309000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 2310000
Best mean reward: 94.03 - Last mean reward per episode: 91.81
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 6.99     |
| reference_action_mean   | -0.272   |
| reference_action_std    | 0.952    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 6.71     |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.38e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.8     |
| total/duration          | 4.83e+03 |
| total/episodes          | 2.38e+04 |
| total/epochs            | 1        |
| total/steps             | 2309998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 1.09     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2311000
Best mean reward: 94.03 - Last mean reward per episode: 91.84
Num timesteps: 2312000
Best mean reward: 94.03 - Last mean reward per episode: 91.82
Num timesteps: 2313000
Best mean reward: 94.03 - Last mean reward per episode: 91.89
Num timesteps: 2314000
Best mean reward: 94.03 - Last mean reward per episode: 91.70
Num timesteps: 2315000
Best mean reward: 94.03 - Last mean reward per episode: 91.77
Num timesteps: 2316000
Best mean reward: 94.03 - Last mean reward per episode: 91.73
Num timesteps: 2317000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 2318000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 2319000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 2320000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
--------------------------------------
| reference_Q_mean        | 52.3     |
| reference_Q_std         | 6.94     |
| reference_action_mean   | -0.14    |
| reference_action_std    | 0.982    |
| reference_actor_Q_mean  | 53.4     |
| reference_actor_Q_std   | 6.7      |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97       |
| rollout/episodes        | 2.39e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 4.85e+03 |
| total/episodes          | 2.39e+04 |
| total/epochs            | 1        |
| total/steps             | 2319998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.436    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2321000
Best mean reward: 94.03 - Last mean reward per episode: 93.56
Num timesteps: 2322000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 2323000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 2324000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 2325000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 2326000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 2327000
Best mean reward: 94.03 - Last mean reward per episode: 93.49
Num timesteps: 2328000
Best mean reward: 94.03 - Last mean reward per episode: 93.33
Num timesteps: 2329000
Best mean reward: 94.03 - Last mean reward per episode: 91.71
Num timesteps: 2330000
Best mean reward: 94.03 - Last mean reward per episode: 91.64
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 6.85     |
| reference_action_mean   | -0.115   |
| reference_action_std    | 0.98     |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.7      |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97       |
| rollout/episodes        | 2.4e+04  |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.6     |
| total/duration          | 4.87e+03 |
| total/episodes          | 2.4e+04  |
| total/epochs            | 1        |
| total/steps             | 2329998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.434    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2331000
Best mean reward: 94.03 - Last mean reward per episode: 91.74
Num timesteps: 2332000
Best mean reward: 94.03 - Last mean reward per episode: 91.84
Num timesteps: 2333000
Best mean reward: 94.03 - Last mean reward per episode: 91.78
Num timesteps: 2334000
Best mean reward: 94.03 - Last mean reward per episode: 91.65
Num timesteps: 2335000
Best mean reward: 94.03 - Last mean reward per episode: 91.35
Num timesteps: 2336000
Best mean reward: 94.03 - Last mean reward per episode: 91.39
Num timesteps: 2337000
Best mean reward: 94.03 - Last mean reward per episode: 91.37
Num timesteps: 2338000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 2339000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 2340000
Best mean reward: 94.03 - Last mean reward per episode: 93.12
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.21     |
| reference_action_mean   | -0.395   |
| reference_action_std    | 0.904    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 6.97     |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97       |
| rollout/episodes        | 2.41e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.1     |
| total/duration          | 4.89e+03 |
| total/episodes          | 2.41e+04 |
| total/epochs            | 1        |
| total/steps             | 2339998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 1.98     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2341000
Best mean reward: 94.03 - Last mean reward per episode: 93.14
Num timesteps: 2342000
Best mean reward: 94.03 - Last mean reward per episode: 93.10
Num timesteps: 2343000
Best mean reward: 94.03 - Last mean reward per episode: 93.19
Num timesteps: 2344000
Best mean reward: 94.03 - Last mean reward per episode: 91.74
Num timesteps: 2345000
Best mean reward: 94.03 - Last mean reward per episode: 91.95
Num timesteps: 2346000
Best mean reward: 94.03 - Last mean reward per episode: 91.75
Num timesteps: 2347000
Best mean reward: 94.03 - Last mean reward per episode: 91.66
Num timesteps: 2348000
Best mean reward: 94.03 - Last mean reward per episode: 91.71
Num timesteps: 2349000
Best mean reward: 94.03 - Last mean reward per episode: 91.51
Num timesteps: 2350000
Best mean reward: 94.03 - Last mean reward per episode: 91.66
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 7.22     |
| reference_action_mean   | -0.447   |
| reference_action_std    | 0.884    |
| reference_actor_Q_mean  | 53.3     |
| reference_actor_Q_std   | 6.86     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97       |
| rollout/episodes        | 2.42e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.7     |
| total/duration          | 4.91e+03 |
| total/episodes          | 2.42e+04 |
| total/epochs            | 1        |
| total/steps             | 2349998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.625    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2351000
Best mean reward: 94.03 - Last mean reward per episode: 91.64
Num timesteps: 2352000
Best mean reward: 94.03 - Last mean reward per episode: 91.62
Num timesteps: 2353000
Best mean reward: 94.03 - Last mean reward per episode: 93.00
Num timesteps: 2354000
Best mean reward: 94.03 - Last mean reward per episode: 93.17
Num timesteps: 2355000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 2356000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 2357000
Best mean reward: 94.03 - Last mean reward per episode: 91.96
Num timesteps: 2358000
Best mean reward: 94.03 - Last mean reward per episode: 92.15
Num timesteps: 2359000
Best mean reward: 94.03 - Last mean reward per episode: 92.00
Num timesteps: 2360000
Best mean reward: 94.03 - Last mean reward per episode: 91.91
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 7.66     |
| reference_action_mean   | -0.532   |
| reference_action_std    | 0.845    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.18     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97       |
| rollout/episodes        | 2.43e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.9     |
| total/duration          | 4.93e+03 |
| total/episodes          | 2.43e+04 |
| total/epochs            | 1        |
| total/steps             | 2359998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.686    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2361000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 2362000
Best mean reward: 94.03 - Last mean reward per episode: 91.75
Num timesteps: 2363000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 2364000
Best mean reward: 94.03 - Last mean reward per episode: 90.18
Num timesteps: 2365000
Best mean reward: 94.03 - Last mean reward per episode: 89.93
Num timesteps: 2366000
Best mean reward: 94.03 - Last mean reward per episode: 89.79
Num timesteps: 2367000
Best mean reward: 94.03 - Last mean reward per episode: 89.75
Num timesteps: 2368000
Best mean reward: 94.03 - Last mean reward per episode: 90.92
Num timesteps: 2369000
Best mean reward: 94.03 - Last mean reward per episode: 90.89
Num timesteps: 2370000
Best mean reward: 94.03 - Last mean reward per episode: 89.41
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 8.22     |
| reference_action_mean   | -0.604   |
| reference_action_std    | 0.785    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.77     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.44e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 89.4     |
| total/duration          | 4.96e+03 |
| total/episodes          | 2.44e+04 |
| total/epochs            | 1        |
| total/steps             | 2369998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.602    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2371000
Best mean reward: 94.03 - Last mean reward per episode: 89.33
Num timesteps: 2372000
Best mean reward: 94.03 - Last mean reward per episode: 89.56
Num timesteps: 2373000
Best mean reward: 94.03 - Last mean reward per episode: 89.58
Num timesteps: 2374000
Best mean reward: 94.03 - Last mean reward per episode: 89.74
Num timesteps: 2375000
Best mean reward: 94.03 - Last mean reward per episode: 91.65
Num timesteps: 2376000
Best mean reward: 94.03 - Last mean reward per episode: 91.87
Num timesteps: 2377000
Best mean reward: 94.03 - Last mean reward per episode: 92.07
Num timesteps: 2378000
Best mean reward: 94.03 - Last mean reward per episode: 93.64
Num timesteps: 2379000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 2380000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 8.22     |
| reference_action_mean   | -0.655   |
| reference_action_std    | 0.734    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.68     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.45e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.7     |
| total/duration          | 4.98e+03 |
| total/episodes          | 2.45e+04 |
| total/epochs            | 1        |
| total/steps             | 2379998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.706    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2381000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
Num timesteps: 2382000
Best mean reward: 94.03 - Last mean reward per episode: 93.76
Num timesteps: 2383000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 2384000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 2385000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 2386000
Best mean reward: 94.03 - Last mean reward per episode: 93.50
Num timesteps: 2387000
Best mean reward: 94.03 - Last mean reward per episode: 91.87
Num timesteps: 2388000
Best mean reward: 94.03 - Last mean reward per episode: 91.93
Num timesteps: 2389000
Best mean reward: 94.03 - Last mean reward per episode: 91.82
Num timesteps: 2390000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 8.1      |
| reference_action_mean   | -0.43    |
| reference_action_std    | 0.881    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.64     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.1     |
| rollout/episodes        | 2.46e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.8     |
| total/duration          | 5e+03    |
| total/episodes          | 2.46e+04 |
| total/epochs            | 1        |
| total/steps             | 2389998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.491    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2391000
Best mean reward: 94.03 - Last mean reward per episode: 91.69
Num timesteps: 2392000
Best mean reward: 94.03 - Last mean reward per episode: 91.92
Num timesteps: 2393000
Best mean reward: 94.03 - Last mean reward per episode: 91.84
Num timesteps: 2394000
Best mean reward: 94.03 - Last mean reward per episode: 91.76
Num timesteps: 2395000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 2396000
Best mean reward: 94.03 - Last mean reward per episode: 93.31
Num timesteps: 2397000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 2398000
Best mean reward: 94.03 - Last mean reward per episode: 93.39
Num timesteps: 2399000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
Num timesteps: 2400000
Best mean reward: 94.03 - Last mean reward per episode: 93.38
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 8.04     |
| reference_action_mean   | -0.454   |
| reference_action_std    | 0.872    |
| reference_actor_Q_mean  | 53.4     |
| reference_actor_Q_std   | 7.52     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97       |
| rollout/episodes        | 2.47e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.4     |
| total/duration          | 5.02e+03 |
| total/episodes          | 2.47e+04 |
| total/epochs            | 1        |
| total/steps             | 2399998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 0.651    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2401000
Best mean reward: 94.03 - Last mean reward per episode: 93.45
Num timesteps: 2402000
Best mean reward: 94.03 - Last mean reward per episode: 93.51
Num timesteps: 2403000
Best mean reward: 94.03 - Last mean reward per episode: 93.53
Num timesteps: 2404000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 2405000
Best mean reward: 94.03 - Last mean reward per episode: 91.90
Num timesteps: 2406000
Best mean reward: 94.03 - Last mean reward per episode: 91.90
Num timesteps: 2407000
Best mean reward: 94.03 - Last mean reward per episode: 92.03
Num timesteps: 2408000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
Num timesteps: 2409000
Best mean reward: 94.03 - Last mean reward per episode: 92.07
Num timesteps: 2410000
Best mean reward: 94.03 - Last mean reward per episode: 92.04
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 7.88     |
| reference_action_mean   | -0.662   |
| reference_action_std    | 0.74     |
| reference_actor_Q_mean  | 53.9     |
| reference_actor_Q_std   | 6.72     |
| rollout/Q_mean          | 66.2     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 96.9     |
| rollout/episodes        | 2.49e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 92       |
| total/duration          | 5.04e+03 |
| total/episodes          | 2.49e+04 |
| total/epochs            | 1        |
| total/steps             | 2409998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.788    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2411000
Best mean reward: 94.03 - Last mean reward per episode: 92.08
Num timesteps: 2412000
Best mean reward: 94.03 - Last mean reward per episode: 92.05
Num timesteps: 2413000
Best mean reward: 94.03 - Last mean reward per episode: 93.67
Num timesteps: 2414000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 2415000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 2416000
Best mean reward: 94.03 - Last mean reward per episode: 93.71
Num timesteps: 2417000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 2418000
Best mean reward: 94.03 - Last mean reward per episode: 93.70
Num timesteps: 2419000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 2420000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 7.61     |
| reference_action_mean   | -0.739   |
| reference_action_std    | 0.662    |
| reference_actor_Q_mean  | 53.9     |
| reference_actor_Q_std   | 6.83     |
| rollout/Q_mean          | 66.2     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 96.9     |
| rollout/episodes        | 2.5e+04  |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.2     |
| total/duration          | 5.06e+03 |
| total/episodes          | 2.5e+04  |
| total/epochs            | 1        |
| total/steps             | 2419998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 0.567    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2421000
Best mean reward: 94.03 - Last mean reward per episode: 93.04
Num timesteps: 2422000
Best mean reward: 94.03 - Last mean reward per episode: 93.04
Num timesteps: 2423000
Best mean reward: 94.03 - Last mean reward per episode: 92.94
Num timesteps: 2424000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 2425000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 2426000
Best mean reward: 94.03 - Last mean reward per episode: 93.05
Num timesteps: 2427000
Best mean reward: 94.03 - Last mean reward per episode: 93.04
Num timesteps: 2428000
Best mean reward: 94.03 - Last mean reward per episode: 93.07
Num timesteps: 2429000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 2430000
Best mean reward: 94.03 - Last mean reward per episode: 93.79
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.64     |
| reference_action_mean   | -0.618   |
| reference_action_std    | 0.769    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 7.2      |
| rollout/Q_mean          | 66.2     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 96.8     |
| rollout/episodes        | 2.51e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.8     |
| total/duration          | 5.09e+03 |
| total/episodes          | 2.51e+04 |
| total/epochs            | 1        |
| total/steps             | 2429998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.564    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2431000
Best mean reward: 94.03 - Last mean reward per episode: 93.78
Num timesteps: 2432000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 2433000
Best mean reward: 94.03 - Last mean reward per episode: 93.74
Num timesteps: 2434000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 2435000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 2436000
Best mean reward: 94.03 - Last mean reward per episode: 93.68
Num timesteps: 2437000
Best mean reward: 94.03 - Last mean reward per episode: 93.73
Num timesteps: 2438000
Best mean reward: 94.03 - Last mean reward per episode: 93.72
Num timesteps: 2439000
Best mean reward: 94.03 - Last mean reward per episode: 93.63
Num timesteps: 2440000
Best mean reward: 94.03 - Last mean reward per episode: 92.14
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 8.07     |
| reference_action_mean   | -0.628   |
| reference_action_std    | 0.768    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 7.2      |
| rollout/Q_mean          | 66.2     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 96.8     |
| rollout/episodes        | 2.52e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 92.1     |
| total/duration          | 5.11e+03 |
| total/episodes          | 2.52e+04 |
| total/epochs            | 1        |
| total/steps             | 2439998  |
| total/steps_per_second  | 478      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.401    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2441000
Best mean reward: 94.03 - Last mean reward per episode: 92.20
Num timesteps: 2442000
Best mean reward: 94.03 - Last mean reward per episode: 91.95
Num timesteps: 2443000
Best mean reward: 94.03 - Last mean reward per episode: 91.96
Num timesteps: 2444000
Best mean reward: 94.03 - Last mean reward per episode: 90.40
Num timesteps: 2445000
Best mean reward: 94.03 - Last mean reward per episode: 90.34
Num timesteps: 2446000
Best mean reward: 94.03 - Last mean reward per episode: 90.30
Num timesteps: 2447000
Best mean reward: 94.03 - Last mean reward per episode: 90.25
Num timesteps: 2448000
Best mean reward: 94.03 - Last mean reward per episode: 90.20
Num timesteps: 2449000
Best mean reward: 94.03 - Last mean reward per episode: 90.13
Num timesteps: 2450000
Best mean reward: 94.03 - Last mean reward per episode: 90.13
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 8.24     |
| reference_action_mean   | -0.564   |
| reference_action_std    | 0.817    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 7.28     |
| rollout/Q_mean          | 66.2     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 96.8     |
| rollout/episodes        | 2.53e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 90.1     |
| total/duration          | 5.13e+03 |
| total/episodes          | 2.53e+04 |
| total/epochs            | 1        |
| total/steps             | 2449998  |
| total/steps_per_second  | 477      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.398    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2451000
Best mean reward: 94.03 - Last mean reward per episode: 91.62
Num timesteps: 2452000
Best mean reward: 94.03 - Last mean reward per episode: 91.35
Num timesteps: 2453000
Best mean reward: 94.03 - Last mean reward per episode: 91.46
Num timesteps: 2454000
Best mean reward: 94.03 - Last mean reward per episode: 92.96
Num timesteps: 2455000
Best mean reward: 94.03 - Last mean reward per episode: 93.04
Num timesteps: 2456000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 2457000
Best mean reward: 94.03 - Last mean reward per episode: 92.92
Num timesteps: 2458000
Best mean reward: 94.03 - Last mean reward per episode: 92.97
Num timesteps: 2459000
Best mean reward: 94.03 - Last mean reward per episode: 92.95
Num timesteps: 2460000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 8.6      |
| reference_action_mean   | -0.562   |
| reference_action_std    | 0.817    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 7.45     |
| rollout/Q_mean          | 66.3     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 96.8     |
| rollout/episodes        | 2.54e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.4     |
| total/duration          | 5.15e+03 |
| total/episodes          | 2.54e+04 |
| total/epochs            | 1        |
| total/steps             | 2459998  |
| total/steps_per_second  | 477      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 1.26     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2461000
Best mean reward: 94.03 - Last mean reward per episode: 93.47
Num timesteps: 2462000
Best mean reward: 94.03 - Last mean reward per episode: 93.46
Num timesteps: 2463000
Best mean reward: 94.03 - Last mean reward per episode: 93.48
Num timesteps: 2464000
Best mean reward: 94.03 - Last mean reward per episode: 93.01
Num timesteps: 2465000
Best mean reward: 94.03 - Last mean reward per episode: 92.67
Num timesteps: 2466000
Best mean reward: 94.03 - Last mean reward per episode: 92.59
Num timesteps: 2467000
Best mean reward: 94.03 - Last mean reward per episode: 92.34
Num timesteps: 2468000
Best mean reward: 94.03 - Last mean reward per episode: 92.25
Num timesteps: 2469000
Best mean reward: 94.03 - Last mean reward per episode: 92.16
Num timesteps: 2470000
Best mean reward: 94.03 - Last mean reward per episode: 92.09
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 9.08     |
| reference_action_mean   | -0.435   |
| reference_action_std    | 0.892    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.81     |
| rollout/Q_mean          | 66.3     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 96.8     |
| rollout/episodes        | 2.55e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 92.1     |
| total/duration          | 5.18e+03 |
| total/episodes          | 2.55e+04 |
| total/epochs            | 1        |
| total/steps             | 2469998  |
| total/steps_per_second  | 477      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.683    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2471000
Best mean reward: 94.03 - Last mean reward per episode: 92.01
Num timesteps: 2472000
Best mean reward: 94.03 - Last mean reward per episode: 91.86
Num timesteps: 2473000
Best mean reward: 94.03 - Last mean reward per episode: 91.91
Num timesteps: 2474000
Best mean reward: 94.03 - Last mean reward per episode: 91.79
Num timesteps: 2475000
Best mean reward: 94.03 - Last mean reward per episode: 92.71
Num timesteps: 2476000
Best mean reward: 94.03 - Last mean reward per episode: 92.94
Num timesteps: 2477000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 2478000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 2479000
Best mean reward: 94.03 - Last mean reward per episode: 93.42
Num timesteps: 2480000
Best mean reward: 94.03 - Last mean reward per episode: 93.05
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 8.47     |
| reference_action_mean   | -0.563   |
| reference_action_std    | 0.815    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.64     |
| rollout/Q_mean          | 66.3     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 96.8     |
| rollout/episodes        | 2.56e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.1     |
| total/duration          | 5.2e+03  |
| total/episodes          | 2.56e+04 |
| total/epochs            | 1        |
| total/steps             | 2479998  |
| total/steps_per_second  | 477      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.535    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2481000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 2482000
Best mean reward: 94.03 - Last mean reward per episode: 93.18
Num timesteps: 2483000
Best mean reward: 94.03 - Last mean reward per episode: 93.41
Num timesteps: 2484000
Best mean reward: 94.03 - Last mean reward per episode: 93.44
Num timesteps: 2485000
Best mean reward: 94.03 - Last mean reward per episode: 93.35
Num timesteps: 2486000
Best mean reward: 94.03 - Last mean reward per episode: 93.26
Num timesteps: 2487000
Best mean reward: 94.03 - Last mean reward per episode: 92.93
Num timesteps: 2488000
Best mean reward: 94.03 - Last mean reward per episode: 92.99
Num timesteps: 2489000
Best mean reward: 94.03 - Last mean reward per episode: 93.40
Num timesteps: 2490000
Best mean reward: 94.03 - Last mean reward per episode: 93.32
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 8.16     |
| reference_action_mean   | -0.555   |
| reference_action_std    | 0.821    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 7.56     |
| rollout/Q_mean          | 66.3     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 96.7     |
| rollout/episodes        | 2.57e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.3     |
| total/duration          | 5.22e+03 |
| total/episodes          | 2.57e+04 |
| total/epochs            | 1        |
| total/steps             | 2489998  |
| total/steps_per_second  | 477      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.352    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2491000
Best mean reward: 94.03 - Last mean reward per episode: 93.43
Num timesteps: 2492000
Best mean reward: 94.03 - Last mean reward per episode: 91.72
Num timesteps: 2493000
Best mean reward: 94.03 - Last mean reward per episode: 91.68
Num timesteps: 2494000
Best mean reward: 94.03 - Last mean reward per episode: 91.63
Num timesteps: 2495000
Best mean reward: 94.03 - Last mean reward per episode: 91.55
Num timesteps: 2496000
Best mean reward: 94.03 - Last mean reward per episode: 91.86
Num timesteps: 2497000
Best mean reward: 94.03 - Last mean reward per episode: 91.72
Num timesteps: 2498000
Best mean reward: 94.03 - Last mean reward per episode: 91.62
Num timesteps: 2499000
Best mean reward: 94.03 - Last mean reward per episode: 91.44
Num timesteps: 2500000
Best mean reward: 94.03 - Last mean reward per episode: 91.43
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.91     |
| reference_action_mean   | -0.568   |
| reference_action_std    | 0.817    |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 7.33     |
| rollout/Q_mean          | 66.3     |
| rollout/actions_mean    | 0.129    |
| rollout/actions_std     | 0.845    |
| rollout/episode_steps   | 96.8     |
| rollout/episodes        | 2.58e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.4     |
| total/duration          | 5.25e+03 |
| total/episodes          | 2.58e+04 |
| total/epochs            | 1        |
| total/steps             | 2499998  |
| total/steps_per_second  | 477      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.304    |
| train/param_noise_di... | 0        |
--------------------------------------


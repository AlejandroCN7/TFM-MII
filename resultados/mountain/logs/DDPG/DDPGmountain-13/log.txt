--------------------------------------------------------------------------
[[12692,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: instancia

Another transport will be used instead, although this may result in
lower performance.

NOTE: You can disable this warning by setting the MCA parameter
btl_base_warn_component_unused to 0.
--------------------------------------------------------------------------
Num timesteps: 1000
Best mean reward: -inf - Last mean reward per episode: -9.13
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 2000
Best mean reward: -9.13 - Last mean reward per episode: 34.08
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 3000
Best mean reward: 34.08 - Last mean reward per episode: 10.83
Num timesteps: 4000
Best mean reward: 34.08 - Last mean reward per episode: -8.63
Num timesteps: 5000
Best mean reward: 34.08 - Last mean reward per episode: -20.46
Num timesteps: 6000
Best mean reward: 34.08 - Last mean reward per episode: -22.26
Num timesteps: 7000
Best mean reward: 34.08 - Last mean reward per episode: -24.41
Num timesteps: 8000
Best mean reward: 34.08 - Last mean reward per episode: -28.29
Num timesteps: 9000
Best mean reward: 34.08 - Last mean reward per episode: -29.37
Num timesteps: 10000
Best mean reward: 34.08 - Last mean reward per episode: -28.41
--------------------------------------
| reference_Q_mean        | 0.651    |
| reference_Q_std         | 3.49     |
| reference_action_mean   | -0.293   |
| reference_action_std    | 0.143    |
| reference_actor_Q_mean  | 0.74     |
| reference_actor_Q_std   | 3.48     |
| rollout/Q_mean          | 0.199    |
| rollout/actions_mean    | 0.367    |
| rollout/actions_std     | 0.499    |
| rollout/episode_steps   | 998      |
| rollout/episodes        | 10       |
| rollout/return          | -28.4    |
| rollout/return_history  | -28.4    |
| total/duration          | 22.3     |
| total/episodes          | 10       |
| total/epochs            | 1        |
| total/steps             | 9998     |
| total/steps_per_second  | 448      |
| train/loss_actor        | -0.426   |
| train/loss_critic       | 1.91     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 11000
Best mean reward: 34.08 - Last mean reward per episode: -30.60
Num timesteps: 12000
Best mean reward: 34.08 - Last mean reward per episode: -32.34
Num timesteps: 13000
Best mean reward: 34.08 - Last mean reward per episode: -32.05
Num timesteps: 14000
Best mean reward: 34.08 - Last mean reward per episode: -32.43
Num timesteps: 15000
Best mean reward: 34.08 - Last mean reward per episode: -31.52
Num timesteps: 16000
Best mean reward: 34.08 - Last mean reward per episode: -33.30
Num timesteps: 17000
Best mean reward: 34.08 - Last mean reward per episode: -32.80
Num timesteps: 18000
Best mean reward: 34.08 - Last mean reward per episode: -32.47
Num timesteps: 19000
Best mean reward: 34.08 - Last mean reward per episode: -27.38
Num timesteps: 20000
Best mean reward: 34.08 - Last mean reward per episode: -27.34
--------------------------------------
| reference_Q_mean        | 0.682    |
| reference_Q_std         | 6.22     |
| reference_action_mean   | -0.0486  |
| reference_action_std    | 0.104    |
| reference_actor_Q_mean  | 0.812    |
| reference_actor_Q_std   | 6.2      |
| rollout/Q_mean          | 0.388    |
| rollout/actions_mean    | 0.114    |
| rollout/actions_std     | 0.6      |
| rollout/episode_steps   | 999      |
| rollout/episodes        | 20       |
| rollout/return          | -27.3    |
| rollout/return_history  | -27.3    |
| total/duration          | 45       |
| total/episodes          | 20       |
| total/epochs            | 1        |
| total/steps             | 19998    |
| total/steps_per_second  | 445      |
| train/loss_actor        | -0.583   |
| train/loss_critic       | 0.433    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 21000
Best mean reward: 34.08 - Last mean reward per episode: -21.91
Num timesteps: 22000
Best mean reward: 34.08 - Last mean reward per episode: -21.78
Num timesteps: 23000
Best mean reward: 34.08 - Last mean reward per episode: -17.24
Num timesteps: 24000
Best mean reward: 34.08 - Last mean reward per episode: -17.13
Num timesteps: 25000
Best mean reward: 34.08 - Last mean reward per episode: -17.31
Num timesteps: 26000
Best mean reward: 34.08 - Last mean reward per episode: -19.59
Num timesteps: 27000
Best mean reward: 34.08 - Last mean reward per episode: -21.07
Num timesteps: 28000
Best mean reward: 34.08 - Last mean reward per episode: -21.22
Num timesteps: 29000
Best mean reward: 34.08 - Last mean reward per episode: -22.16
Num timesteps: 30000
Best mean reward: 34.08 - Last mean reward per episode: -19.89
--------------------------------------
| reference_Q_mean        | 1.21     |
| reference_Q_std         | 8.75     |
| reference_action_mean   | 0.0122   |
| reference_action_std    | 0.0884   |
| reference_actor_Q_mean  | 1.26     |
| reference_actor_Q_std   | 8.74     |
| rollout/Q_mean          | 0.572    |
| rollout/actions_mean    | 0.0334   |
| rollout/actions_std     | 0.608    |
| rollout/episode_steps   | 980      |
| rollout/episodes        | 30       |
| rollout/return          | -19.9    |
| rollout/return_history  | -19.9    |
| total/duration          | 66.9     |
| total/episodes          | 30       |
| total/epochs            | 1        |
| total/steps             | 29998    |
| total/steps_per_second  | 448      |
| train/loss_actor        | -0.794   |
| train/loss_critic       | 0.0152   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 31000
Best mean reward: 34.08 - Last mean reward per episode: -20.21
Num timesteps: 32000
Best mean reward: 34.08 - Last mean reward per episode: -20.73
Num timesteps: 33000
Best mean reward: 34.08 - Last mean reward per episode: -17.61
Num timesteps: 34000
Best mean reward: 34.08 - Last mean reward per episode: -18.99
Num timesteps: 35000
Best mean reward: 34.08 - Last mean reward per episode: -19.24
Num timesteps: 36000
Best mean reward: 34.08 - Last mean reward per episode: -19.25
Num timesteps: 37000
Best mean reward: 34.08 - Last mean reward per episode: -17.16
Num timesteps: 38000
Best mean reward: 34.08 - Last mean reward per episode: -17.56
Num timesteps: 39000
Best mean reward: 34.08 - Last mean reward per episode: -18.61
Num timesteps: 40000
Best mean reward: 34.08 - Last mean reward per episode: -19.04
--------------------------------------
| reference_Q_mean        | 1.23     |
| reference_Q_std         | 9.11     |
| reference_action_mean   | 0.045    |
| reference_action_std    | 0.0889   |
| reference_actor_Q_mean  | 1.29     |
| reference_actor_Q_std   | 9.1      |
| rollout/Q_mean          | 0.725    |
| rollout/actions_mean    | 0.0524   |
| rollout/actions_std     | 0.606    |
| rollout/episode_steps   | 975      |
| rollout/episodes        | 41       |
| rollout/return          | -19      |
| rollout/return_history  | -19      |
| total/duration          | 90.9     |
| total/episodes          | 41       |
| total/epochs            | 1        |
| total/steps             | 39998    |
| total/steps_per_second  | 440      |
| train/loss_actor        | -0.934   |
| train/loss_critic       | 0.0133   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 41000
Best mean reward: 34.08 - Last mean reward per episode: -19.15
Num timesteps: 42000
Best mean reward: 34.08 - Last mean reward per episode: -19.43
Num timesteps: 43000
Best mean reward: 34.08 - Last mean reward per episode: -17.77
Num timesteps: 44000
Best mean reward: 34.08 - Last mean reward per episode: -18.09
Num timesteps: 45000
Best mean reward: 34.08 - Last mean reward per episode: -15.77
Num timesteps: 46000
Best mean reward: 34.08 - Last mean reward per episode: -16.67
Num timesteps: 47000
Best mean reward: 34.08 - Last mean reward per episode: -16.96
Num timesteps: 48000
Best mean reward: 34.08 - Last mean reward per episode: -17.51
Num timesteps: 49000
Best mean reward: 34.08 - Last mean reward per episode: -17.91
Num timesteps: 50000
Best mean reward: 34.08 - Last mean reward per episode: -18.67
--------------------------------------
| reference_Q_mean        | 1.23     |
| reference_Q_std         | 9.23     |
| reference_action_mean   | 0.0382   |
| reference_action_std    | 0.101    |
| reference_actor_Q_mean  | 1.28     |
| reference_actor_Q_std   | 9.23     |
| rollout/Q_mean          | 0.793    |
| rollout/actions_mean    | 0.0656   |
| rollout/actions_std     | 0.605    |
| rollout/episode_steps   | 976      |
| rollout/episodes        | 51       |
| rollout/return          | -18.7    |
| rollout/return_history  | -18.7    |
| total/duration          | 116      |
| total/episodes          | 51       |
| total/epochs            | 1        |
| total/steps             | 49998    |
| total/steps_per_second  | 432      |
| train/loss_actor        | -0.97    |
| train/loss_critic       | 0.0324   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 51000
Best mean reward: 34.08 - Last mean reward per episode: -16.66
Num timesteps: 52000
Best mean reward: 34.08 - Last mean reward per episode: -15.81
Num timesteps: 53000
Best mean reward: 34.08 - Last mean reward per episode: -14.19
Num timesteps: 54000
Best mean reward: 34.08 - Last mean reward per episode: -13.31
Num timesteps: 55000
Best mean reward: 34.08 - Last mean reward per episode: -10.08
Num timesteps: 56000
Best mean reward: 34.08 - Last mean reward per episode: -9.11
Num timesteps: 57000
Best mean reward: 34.08 - Last mean reward per episode: -6.07
Num timesteps: 58000
Best mean reward: 34.08 - Last mean reward per episode: -4.70
Num timesteps: 59000
Best mean reward: 34.08 - Last mean reward per episode: -2.68
Num timesteps: 60000
Best mean reward: 34.08 - Last mean reward per episode: 0.07
--------------------------------------
| reference_Q_mean        | 1.13     |
| reference_Q_std         | 9.09     |
| reference_action_mean   | 0.00259  |
| reference_action_std    | 0.115    |
| reference_actor_Q_mean  | 1.16     |
| reference_actor_Q_std   | 9.09     |
| rollout/Q_mean          | 1.19     |
| rollout/actions_mean    | 0.113    |
| rollout/actions_std     | 0.608    |
| rollout/episode_steps   | 892      |
| rollout/episodes        | 67       |
| rollout/return          | 0.073    |
| rollout/return_history  | 0.073    |
| total/duration          | 139      |
| total/episodes          | 67       |
| total/epochs            | 1        |
| total/steps             | 59998    |
| total/steps_per_second  | 431      |
| train/loss_actor        | -1.44    |
| train/loss_critic       | 0.897    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 61000
Best mean reward: 34.08 - Last mean reward per episode: 2.73
Num timesteps: 62000
Best mean reward: 34.08 - Last mean reward per episode: 3.84
Num timesteps: 63000
Best mean reward: 34.08 - Last mean reward per episode: 7.36
Num timesteps: 64000
Best mean reward: 34.08 - Last mean reward per episode: 9.59
Num timesteps: 65000
Best mean reward: 34.08 - Last mean reward per episode: 11.13
Num timesteps: 66000
Best mean reward: 34.08 - Last mean reward per episode: 12.15
Num timesteps: 67000
Best mean reward: 34.08 - Last mean reward per episode: 16.06
Num timesteps: 68000
Best mean reward: 34.08 - Last mean reward per episode: 18.55
Num timesteps: 69000
Best mean reward: 34.08 - Last mean reward per episode: 19.82
Num timesteps: 70000
Best mean reward: 34.08 - Last mean reward per episode: 21.10
--------------------------------------
| reference_Q_mean        | 1.08     |
| reference_Q_std         | 8.75     |
| reference_action_mean   | -0.036   |
| reference_action_std    | 0.149    |
| reference_actor_Q_mean  | 1.16     |
| reference_actor_Q_std   | 8.75     |
| rollout/Q_mean          | 1.72     |
| rollout/actions_mean    | 0.0907   |
| rollout/actions_std     | 0.606    |
| rollout/episode_steps   | 785      |
| rollout/episodes        | 89       |
| rollout/return          | 21.1     |
| rollout/return_history  | 21.1     |
| total/duration          | 162      |
| total/episodes          | 89       |
| total/epochs            | 1        |
| total/steps             | 69998    |
| total/steps_per_second  | 431      |
| train/loss_actor        | -2.81    |
| train/loss_critic       | 0.141    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 71000
Best mean reward: 34.08 - Last mean reward per episode: 22.35
Num timesteps: 72000
Best mean reward: 34.08 - Last mean reward per episode: 24.53
Num timesteps: 73000
Best mean reward: 34.08 - Last mean reward per episode: 26.50
Num timesteps: 74000
Best mean reward: 34.08 - Last mean reward per episode: 25.29
Num timesteps: 75000
Best mean reward: 34.08 - Last mean reward per episode: 26.55
Num timesteps: 76000
Best mean reward: 34.08 - Last mean reward per episode: 28.76
Num timesteps: 77000
Best mean reward: 34.08 - Last mean reward per episode: 32.49
Num timesteps: 78000
Best mean reward: 34.08 - Last mean reward per episode: 36.37
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 79000
Best mean reward: 36.37 - Last mean reward per episode: 40.42
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 80000
Best mean reward: 40.42 - Last mean reward per episode: 41.56
Saving new best model to ./modelos/DDPG/mountain-13.pkl
--------------------------------------
| reference_Q_mean        | 1.59     |
| reference_Q_std         | 8.49     |
| reference_action_mean   | 0.0153   |
| reference_action_std    | 0.164    |
| reference_actor_Q_mean  | 1.66     |
| reference_actor_Q_std   | 8.48     |
| rollout/Q_mean          | 2.45     |
| rollout/actions_mean    | 0.0709   |
| rollout/actions_std     | 0.621    |
| rollout/episode_steps   | 702      |
| rollout/episodes        | 113      |
| rollout/return          | 33.1     |
| rollout/return_history  | 41.6     |
| total/duration          | 187      |
| total/episodes          | 113      |
| total/epochs            | 1        |
| total/steps             | 79998    |
| total/steps_per_second  | 429      |
| train/loss_actor        | -4.46    |
| train/loss_critic       | 0.794    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 81000
Best mean reward: 41.56 - Last mean reward per episode: 44.75
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 82000
Best mean reward: 44.75 - Last mean reward per episode: 47.41
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 83000
Best mean reward: 47.41 - Last mean reward per episode: 49.64
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 84000
Best mean reward: 49.64 - Last mean reward per episode: 50.54
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 85000
Best mean reward: 50.54 - Last mean reward per episode: 53.24
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 86000
Best mean reward: 53.24 - Last mean reward per episode: 55.35
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 87000
Best mean reward: 55.35 - Last mean reward per episode: 58.22
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 88000
Best mean reward: 58.22 - Last mean reward per episode: 63.65
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 89000
Best mean reward: 63.65 - Last mean reward per episode: 68.59
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 90000
Best mean reward: 68.59 - Last mean reward per episode: 76.88
Saving new best model to ./modelos/DDPG/mountain-13.pkl
--------------------------------------
| reference_Q_mean        | 1.08     |
| reference_Q_std         | 8.76     |
| reference_action_mean   | -0.548   |
| reference_action_std    | 0.368    |
| reference_actor_Q_mean  | 1.06     |
| reference_actor_Q_std   | 8.76     |
| rollout/Q_mean          | 3.66     |
| rollout/actions_mean    | 0.0571   |
| rollout/actions_std     | 0.631    |
| rollout/episode_steps   | 600      |
| rollout/episodes        | 150      |
| rollout/return          | 45.3     |
| rollout/return_history  | 76.9     |
| total/duration          | 212      |
| total/episodes          | 150      |
| total/epochs            | 1        |
| total/steps             | 89998    |
| total/steps_per_second  | 424      |
| train/loss_actor        | -8.28    |
| train/loss_critic       | 0.206    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 91000
Best mean reward: 76.88 - Last mean reward per episode: 81.26
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 92000
Best mean reward: 81.26 - Last mean reward per episode: 81.58
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 93000
Best mean reward: 81.58 - Last mean reward per episode: 83.09
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 94000
Best mean reward: 83.09 - Last mean reward per episode: 81.18
Num timesteps: 95000
Best mean reward: 83.09 - Last mean reward per episode: 79.55
Num timesteps: 96000
Best mean reward: 83.09 - Last mean reward per episode: 78.20
Num timesteps: 97000
Best mean reward: 83.09 - Last mean reward per episode: 77.98
Num timesteps: 98000
Best mean reward: 83.09 - Last mean reward per episode: 77.78
Num timesteps: 99000
Best mean reward: 83.09 - Last mean reward per episode: 76.05
Num timesteps: 100000
Best mean reward: 83.09 - Last mean reward per episode: 74.15
--------------------------------------
| reference_Q_mean        | 0.799    |
| reference_Q_std         | 8.98     |
| reference_action_mean   | 1        |
| reference_action_std    | 4.74e-08 |
| reference_actor_Q_mean  | 0.83     |
| reference_actor_Q_std   | 9        |
| rollout/Q_mean          | 4.18     |
| rollout/actions_mean    | 0.103    |
| rollout/actions_std     | 0.646    |
| rollout/episode_steps   | 568      |
| rollout/episodes        | 175      |
| rollout/return          | 46.5     |
| rollout/return_history  | 74.2     |
| total/duration          | 237      |
| total/episodes          | 175      |
| total/epochs            | 1        |
| total/steps             | 99998    |
| total/steps_per_second  | 423      |
| train/loss_actor        | -12.2    |
| train/loss_critic       | 0.346    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 101000
Best mean reward: 83.09 - Last mean reward per episode: 73.44
Num timesteps: 102000
Best mean reward: 83.09 - Last mean reward per episode: 71.83
Num timesteps: 103000
Best mean reward: 83.09 - Last mean reward per episode: 71.41
Num timesteps: 104000
Best mean reward: 83.09 - Last mean reward per episode: 69.78
Num timesteps: 105000
Best mean reward: 83.09 - Last mean reward per episode: 68.34
Num timesteps: 106000
Best mean reward: 83.09 - Last mean reward per episode: 66.43
Num timesteps: 107000
Best mean reward: 83.09 - Last mean reward per episode: 64.82
Num timesteps: 108000
Best mean reward: 83.09 - Last mean reward per episode: 63.23
Num timesteps: 109000
Best mean reward: 83.09 - Last mean reward per episode: 61.86
Num timesteps: 110000
Best mean reward: 83.09 - Last mean reward per episode: 60.77
--------------------------------------
| reference_Q_mean        | 1.03     |
| reference_Q_std         | 9.36     |
| reference_action_mean   | 1        |
| reference_action_std    | 0        |
| reference_actor_Q_mean  | 1.31     |
| reference_actor_Q_std   | 9.55     |
| rollout/Q_mean          | 4.04     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.652    |
| rollout/episode_steps   | 586      |
| rollout/episodes        | 187      |
| rollout/return          | 41.7     |
| rollout/return_history  | 60.8     |
| total/duration          | 262      |
| total/episodes          | 187      |
| total/epochs            | 1        |
| total/steps             | 109998   |
| total/steps_per_second  | 421      |
| train/loss_actor        | -17.6    |
| train/loss_critic       | 2.31     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 111000
Best mean reward: 83.09 - Last mean reward per episode: 59.20
Num timesteps: 112000
Best mean reward: 83.09 - Last mean reward per episode: 57.95
Num timesteps: 113000
Best mean reward: 83.09 - Last mean reward per episode: 56.10
Num timesteps: 114000
Best mean reward: 83.09 - Last mean reward per episode: 54.64
Num timesteps: 115000
Best mean reward: 83.09 - Last mean reward per episode: 54.32
Num timesteps: 116000
Best mean reward: 83.09 - Last mean reward per episode: 53.90
Num timesteps: 117000
Best mean reward: 83.09 - Last mean reward per episode: 52.74
Num timesteps: 118000
Best mean reward: 83.09 - Last mean reward per episode: 51.03
Num timesteps: 119000
Best mean reward: 83.09 - Last mean reward per episode: 49.23
Num timesteps: 120000
Best mean reward: 83.09 - Last mean reward per episode: 47.79
--------------------------------------
| reference_Q_mean        | 2.26     |
| reference_Q_std         | 12.2     |
| reference_action_mean   | 1        |
| reference_action_std    | 0        |
| reference_actor_Q_mean  | 2.6      |
| reference_actor_Q_std   | 12.8     |
| rollout/Q_mean          | 4.19     |
| rollout/actions_mean    | 0.184    |
| rollout/actions_std     | 0.658    |
| rollout/episode_steps   | 606      |
| rollout/episodes        | 197      |
| rollout/return          | 37.3     |
| rollout/return_history  | 47.8     |
| total/duration          | 288      |
| total/episodes          | 197      |
| total/epochs            | 1        |
| total/steps             | 119998   |
| total/steps_per_second  | 417      |
| train/loss_actor        | -18.3    |
| train/loss_critic       | 4.9      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 121000
Best mean reward: 83.09 - Last mean reward per episode: 48.86
Num timesteps: 122000
Best mean reward: 83.09 - Last mean reward per episode: 47.06
Num timesteps: 123000
Best mean reward: 83.09 - Last mean reward per episode: 45.90
Num timesteps: 124000
Best mean reward: 83.09 - Last mean reward per episode: 45.41
Num timesteps: 125000
Best mean reward: 83.09 - Last mean reward per episode: 43.89
Num timesteps: 126000
Best mean reward: 83.09 - Last mean reward per episode: 43.78
Num timesteps: 127000
Best mean reward: 83.09 - Last mean reward per episode: 42.08
Num timesteps: 128000
Best mean reward: 83.09 - Last mean reward per episode: 40.54
Num timesteps: 129000
Best mean reward: 83.09 - Last mean reward per episode: 38.90
Num timesteps: 130000
Best mean reward: 83.09 - Last mean reward per episode: 39.94
--------------------------------------
| reference_Q_mean        | 2.96     |
| reference_Q_std         | 16       |
| reference_action_mean   | -0.616   |
| reference_action_std    | 0.667    |
| reference_actor_Q_mean  | 3.48     |
| reference_actor_Q_std   | 15.9     |
| rollout/Q_mean          | 4.8      |
| rollout/actions_mean    | 0.22     |
| rollout/actions_std     | 0.662    |
| rollout/episode_steps   | 585      |
| rollout/episodes        | 222      |
| rollout/return          | 38.2     |
| rollout/return_history  | 39.9     |
| total/duration          | 313      |
| total/episodes          | 222      |
| total/epochs            | 1        |
| total/steps             | 129998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -15.6    |
| train/loss_critic       | 0.874    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 131000
Best mean reward: 83.09 - Last mean reward per episode: 42.01
Num timesteps: 132000
Best mean reward: 83.09 - Last mean reward per episode: 42.30
Num timesteps: 133000
Best mean reward: 83.09 - Last mean reward per episode: 40.99
Num timesteps: 134000
Best mean reward: 83.09 - Last mean reward per episode: 41.29
Num timesteps: 135000
Best mean reward: 83.09 - Last mean reward per episode: 41.17
Num timesteps: 136000
Best mean reward: 83.09 - Last mean reward per episode: 43.61
Num timesteps: 137000
Best mean reward: 83.09 - Last mean reward per episode: 47.14
Num timesteps: 138000
Best mean reward: 83.09 - Last mean reward per episode: 55.69
Num timesteps: 139000
Best mean reward: 83.09 - Last mean reward per episode: 72.12
Num timesteps: 140000
Best mean reward: 83.09 - Last mean reward per episode: 83.76
Saving new best model to ./modelos/DDPG/mountain-13.pkl
--------------------------------------
| reference_Q_mean        | 5.58     |
| reference_Q_std         | 16       |
| reference_action_mean   | -0.795   |
| reference_action_std    | 0.573    |
| reference_actor_Q_mean  | 6.31     |
| reference_actor_Q_std   | 15.7     |
| rollout/Q_mean          | 8.33     |
| rollout/actions_mean    | 0.216    |
| rollout/actions_std     | 0.671    |
| rollout/episode_steps   | 463      |
| rollout/episodes        | 302      |
| rollout/return          | 52.1     |
| rollout/return_history  | 83.8     |
| total/duration          | 336      |
| total/episodes          | 302      |
| total/epochs            | 1        |
| total/steps             | 139998   |
| total/steps_per_second  | 416      |
| train/loss_actor        | -23.3    |
| train/loss_critic       | 1.45     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 141000
Best mean reward: 83.76 - Last mean reward per episode: 91.46
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 142000
Best mean reward: 91.46 - Last mean reward per episode: 91.46
Num timesteps: 143000
Best mean reward: 91.46 - Last mean reward per episode: 89.83
Num timesteps: 144000
Best mean reward: 91.46 - Last mean reward per episode: 91.12
Num timesteps: 145000
Best mean reward: 91.46 - Last mean reward per episode: 91.25
Num timesteps: 146000
Best mean reward: 91.46 - Last mean reward per episode: 91.43
Num timesteps: 147000
Best mean reward: 91.46 - Last mean reward per episode: 91.44
Num timesteps: 148000
Best mean reward: 91.46 - Last mean reward per episode: 91.56
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 149000
Best mean reward: 91.56 - Last mean reward per episode: 91.50
Num timesteps: 150000
Best mean reward: 91.56 - Last mean reward per episode: 89.92
--------------------------------------
| reference_Q_mean        | 9.99     |
| reference_Q_std         | 17.5     |
| reference_action_mean   | -0.882   |
| reference_action_std    | 0.429    |
| reference_actor_Q_mean  | 11       |
| reference_actor_Q_std   | 17.5     |
| rollout/Q_mean          | 12.5     |
| rollout/actions_mean    | 0.185    |
| rollout/actions_std     | 0.692    |
| rollout/episode_steps   | 386      |
| rollout/episodes        | 388      |
| rollout/return          | 60.4     |
| rollout/return_history  | 89.9     |
| total/duration          | 361      |
| total/episodes          | 388      |
| total/epochs            | 1        |
| total/steps             | 149998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -37      |
| train/loss_critic       | 1.02     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 151000
Best mean reward: 91.56 - Last mean reward per episode: 89.43
Num timesteps: 152000
Best mean reward: 91.56 - Last mean reward per episode: 89.38
Num timesteps: 153000
Best mean reward: 91.56 - Last mean reward per episode: 91.13
Num timesteps: 154000
Best mean reward: 91.56 - Last mean reward per episode: 91.14
Num timesteps: 155000
Best mean reward: 91.56 - Last mean reward per episode: 91.30
Num timesteps: 156000
Best mean reward: 91.56 - Last mean reward per episode: 91.27
Num timesteps: 157000
Best mean reward: 91.56 - Last mean reward per episode: 91.46
Num timesteps: 158000
Best mean reward: 91.56 - Last mean reward per episode: 91.40
Num timesteps: 159000
Best mean reward: 91.56 - Last mean reward per episode: 93.11
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 160000
Best mean reward: 93.11 - Last mean reward per episode: 93.31
Saving new best model to ./modelos/DDPG/mountain-13.pkl
--------------------------------------
| reference_Q_mean        | 16.2     |
| reference_Q_std         | 19.9     |
| reference_action_mean   | -0.832   |
| reference_action_std    | 0.531    |
| reference_actor_Q_mean  | 17.6     |
| reference_actor_Q_std   | 20.1     |
| rollout/Q_mean          | 16.4     |
| rollout/actions_mean    | 0.171    |
| rollout/actions_std     | 0.706    |
| rollout/episode_steps   | 323      |
| rollout/episodes        | 495      |
| rollout/return          | 67.4     |
| rollout/return_history  | 93.3     |
| total/duration          | 386      |
| total/episodes          | 495      |
| total/epochs            | 1        |
| total/steps             | 159998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -53.6    |
| train/loss_critic       | 2.03     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 161000
Best mean reward: 93.31 - Last mean reward per episode: 93.39
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 162000
Best mean reward: 93.39 - Last mean reward per episode: 93.46
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 163000
Best mean reward: 93.46 - Last mean reward per episode: 93.43
Num timesteps: 164000
Best mean reward: 93.46 - Last mean reward per episode: 93.55
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 165000
Best mean reward: 93.55 - Last mean reward per episode: 93.55
Num timesteps: 166000
Best mean reward: 93.55 - Last mean reward per episode: 93.51
Num timesteps: 167000
Best mean reward: 93.55 - Last mean reward per episode: 93.63
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 168000
Best mean reward: 93.63 - Last mean reward per episode: 93.85
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 169000
Best mean reward: 93.85 - Last mean reward per episode: 93.82
Num timesteps: 170000
Best mean reward: 93.85 - Last mean reward per episode: 93.83
--------------------------------------
| reference_Q_mean        | 25.7     |
| reference_Q_std         | 21.3     |
| reference_action_mean   | -0.905   |
| reference_action_std    | 0.394    |
| reference_actor_Q_mean  | 28.3     |
| reference_actor_Q_std   | 21.8     |
| rollout/Q_mean          | 19.9     |
| rollout/actions_mean    | 0.163    |
| rollout/actions_std     | 0.718    |
| rollout/episode_steps   | 274      |
| rollout/episodes        | 621      |
| rollout/return          | 72.8     |
| rollout/return_history  | 93.8     |
| total/duration          | 409      |
| total/episodes          | 621      |
| total/epochs            | 1        |
| total/steps             | 169998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -67.6    |
| train/loss_critic       | 1.16     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 171000
Best mean reward: 93.85 - Last mean reward per episode: 93.90
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 172000
Best mean reward: 93.90 - Last mean reward per episode: 93.90
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 173000
Best mean reward: 93.90 - Last mean reward per episode: 93.88
Num timesteps: 174000
Best mean reward: 93.90 - Last mean reward per episode: 93.85
Num timesteps: 175000
Best mean reward: 93.90 - Last mean reward per episode: 93.86
Num timesteps: 176000
Best mean reward: 93.90 - Last mean reward per episode: 93.75
Num timesteps: 177000
Best mean reward: 93.90 - Last mean reward per episode: 93.76
Num timesteps: 178000
Best mean reward: 93.90 - Last mean reward per episode: 93.70
Num timesteps: 179000
Best mean reward: 93.90 - Last mean reward per episode: 93.67
Num timesteps: 180000
Best mean reward: 93.90 - Last mean reward per episode: 93.64
--------------------------------------
| reference_Q_mean        | 42       |
| reference_Q_std         | 21.2     |
| reference_action_mean   | -0.891   |
| reference_action_std    | 0.437    |
| reference_actor_Q_mean  | 46.9     |
| reference_actor_Q_std   | 21.6     |
| rollout/Q_mean          | 23.2     |
| rollout/actions_mean    | 0.16     |
| rollout/actions_std     | 0.729    |
| rollout/episode_steps   | 240      |
| rollout/episodes        | 749      |
| rollout/return          | 76.4     |
| rollout/return_history  | 93.6     |
| total/duration          | 434      |
| total/episodes          | 749      |
| total/epochs            | 1        |
| total/steps             | 179998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -76.8    |
| train/loss_critic       | 0.578    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 181000
Best mean reward: 93.90 - Last mean reward per episode: 93.63
Num timesteps: 182000
Best mean reward: 93.90 - Last mean reward per episode: 93.60
Num timesteps: 183000
Best mean reward: 93.90 - Last mean reward per episode: 93.69
Num timesteps: 184000
Best mean reward: 93.90 - Last mean reward per episode: 93.83
Num timesteps: 185000
Best mean reward: 93.90 - Last mean reward per episode: 93.72
Num timesteps: 186000
Best mean reward: 93.90 - Last mean reward per episode: 93.78
Num timesteps: 187000
Best mean reward: 93.90 - Last mean reward per episode: 93.89
Num timesteps: 188000
Best mean reward: 93.90 - Last mean reward per episode: 92.29
Num timesteps: 189000
Best mean reward: 93.90 - Last mean reward per episode: 92.33
Num timesteps: 190000
Best mean reward: 93.90 - Last mean reward per episode: 92.43
--------------------------------------
| reference_Q_mean        | 55.3     |
| reference_Q_std         | 16.8     |
| reference_action_mean   | -0.898   |
| reference_action_std    | 0.425    |
| reference_actor_Q_mean  | 62.8     |
| reference_actor_Q_std   | 17       |
| rollout/Q_mean          | 26.1     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.736    |
| rollout/episode_steps   | 221      |
| rollout/episodes        | 860      |
| rollout/return          | 78.4     |
| rollout/return_history  | 92.4     |
| total/duration          | 459      |
| total/episodes          | 860      |
| total/epochs            | 1        |
| total/steps             | 189998   |
| total/steps_per_second  | 414      |
| train/loss_actor        | -78      |
| train/loss_critic       | 1.34     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 191000
Best mean reward: 93.90 - Last mean reward per episode: 92.47
Num timesteps: 192000
Best mean reward: 93.90 - Last mean reward per episode: 92.34
Num timesteps: 193000
Best mean reward: 93.90 - Last mean reward per episode: 92.35
Num timesteps: 194000
Best mean reward: 93.90 - Last mean reward per episode: 90.47
Num timesteps: 195000
Best mean reward: 93.90 - Last mean reward per episode: 89.63
Num timesteps: 196000
Best mean reward: 93.90 - Last mean reward per episode: 89.63
Num timesteps: 197000
Best mean reward: 93.90 - Last mean reward per episode: 88.89
Num timesteps: 198000
Best mean reward: 93.90 - Last mean reward per episode: 88.77
Num timesteps: 199000
Best mean reward: 93.90 - Last mean reward per episode: 88.55
Num timesteps: 200000
Best mean reward: 93.90 - Last mean reward per episode: 90.05
--------------------------------------
| reference_Q_mean        | 62.8     |
| reference_Q_std         | 13.2     |
| reference_action_mean   | -0.969   |
| reference_action_std    | 0.238    |
| reference_actor_Q_mean  | 67.7     |
| reference_actor_Q_std   | 13.4     |
| rollout/Q_mean          | 28.4     |
| rollout/actions_mean    | 0.13     |
| rollout/actions_std     | 0.749    |
| rollout/episode_steps   | 212      |
| rollout/episodes        | 943      |
| rollout/return          | 79.4     |
| rollout/return_history  | 90       |
| total/duration          | 482      |
| total/episodes          | 943      |
| total/epochs            | 1        |
| total/steps             | 199998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -75.9    |
| train/loss_critic       | 1.12     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 201000
Best mean reward: 93.90 - Last mean reward per episode: 89.95
Num timesteps: 202000
Best mean reward: 93.90 - Last mean reward per episode: 89.81
Num timesteps: 203000
Best mean reward: 93.90 - Last mean reward per episode: 89.86
Num timesteps: 204000
Best mean reward: 93.90 - Last mean reward per episode: 89.67
Num timesteps: 205000
Best mean reward: 93.90 - Last mean reward per episode: 92.28
Num timesteps: 206000
Best mean reward: 93.90 - Last mean reward per episode: 93.10
Num timesteps: 207000
Best mean reward: 93.90 - Last mean reward per episode: 93.34
Num timesteps: 208000
Best mean reward: 93.90 - Last mean reward per episode: 93.21
Num timesteps: 209000
Best mean reward: 93.90 - Last mean reward per episode: 93.31
Num timesteps: 210000
Best mean reward: 93.90 - Last mean reward per episode: 93.33
--------------------------------------
| reference_Q_mean        | 64.7     |
| reference_Q_std         | 11.6     |
| reference_action_mean   | -0.921   |
| reference_action_std    | 0.367    |
| reference_actor_Q_mean  | 66.3     |
| reference_actor_Q_std   | 11.9     |
| rollout/Q_mean          | 30.7     |
| rollout/actions_mean    | 0.121    |
| rollout/actions_std     | 0.755    |
| rollout/episode_steps   | 199      |
| rollout/episodes        | 1.06e+03 |
| rollout/return          | 80.9     |
| rollout/return_history  | 93.3     |
| total/duration          | 506      |
| total/episodes          | 1.06e+03 |
| total/epochs            | 1        |
| total/steps             | 209998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -74.3    |
| train/loss_critic       | 1.17     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 211000
Best mean reward: 93.90 - Last mean reward per episode: 93.43
Num timesteps: 212000
Best mean reward: 93.90 - Last mean reward per episode: 93.17
Num timesteps: 213000
Best mean reward: 93.90 - Last mean reward per episode: 92.76
Num timesteps: 214000
Best mean reward: 93.90 - Last mean reward per episode: 92.92
Num timesteps: 215000
Best mean reward: 93.90 - Last mean reward per episode: 93.09
Num timesteps: 216000
Best mean reward: 93.90 - Last mean reward per episode: 93.05
Num timesteps: 217000
Best mean reward: 93.90 - Last mean reward per episode: 92.87
Num timesteps: 218000
Best mean reward: 93.90 - Last mean reward per episode: 93.09
Num timesteps: 219000
Best mean reward: 93.90 - Last mean reward per episode: 93.12
Num timesteps: 220000
Best mean reward: 93.90 - Last mean reward per episode: 93.36
--------------------------------------
| reference_Q_mean        | 63.3     |
| reference_Q_std         | 9.77     |
| reference_action_mean   | -0.898   |
| reference_action_std    | 0.423    |
| reference_actor_Q_mean  | 62.9     |
| reference_actor_Q_std   | 9.76     |
| rollout/Q_mean          | 32.7     |
| rollout/actions_mean    | 0.113    |
| rollout/actions_std     | 0.761    |
| rollout/episode_steps   | 189      |
| rollout/episodes        | 1.16e+03 |
| rollout/return          | 82       |
| rollout/return_history  | 93.4     |
| total/duration          | 530      |
| total/episodes          | 1.16e+03 |
| total/epochs            | 1        |
| total/steps             | 219998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -73.3    |
| train/loss_critic       | 0.349    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 221000
Best mean reward: 93.90 - Last mean reward per episode: 93.81
Num timesteps: 222000
Best mean reward: 93.90 - Last mean reward per episode: 93.74
Num timesteps: 223000
Best mean reward: 93.90 - Last mean reward per episode: 93.81
Num timesteps: 224000
Best mean reward: 93.90 - Last mean reward per episode: 93.95
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 225000
Best mean reward: 93.95 - Last mean reward per episode: 93.95
Num timesteps: 226000
Best mean reward: 93.95 - Last mean reward per episode: 93.96
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 227000
Best mean reward: 93.96 - Last mean reward per episode: 93.98
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 228000
Best mean reward: 93.98 - Last mean reward per episode: 93.58
Num timesteps: 229000
Best mean reward: 93.98 - Last mean reward per episode: 93.56
Num timesteps: 230000
Best mean reward: 93.98 - Last mean reward per episode: 93.65
--------------------------------------
| reference_Q_mean        | 61.2     |
| reference_Q_std         | 8.33     |
| reference_action_mean   | -0.351   |
| reference_action_std    | 0.693    |
| reference_actor_Q_mean  | 61.6     |
| reference_actor_Q_std   | 8.32     |
| rollout/Q_mean          | 34.4     |
| rollout/actions_mean    | 0.11     |
| rollout/actions_std     | 0.765    |
| rollout/episode_steps   | 180      |
| rollout/episodes        | 1.28e+03 |
| rollout/return          | 83       |
| rollout/return_history  | 93.6     |
| total/duration          | 554      |
| total/episodes          | 1.28e+03 |
| total/epochs            | 1        |
| total/steps             | 229998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -72.5    |
| train/loss_critic       | 0.232    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 231000
Best mean reward: 93.98 - Last mean reward per episode: 93.67
Num timesteps: 232000
Best mean reward: 93.98 - Last mean reward per episode: 93.70
Num timesteps: 233000
Best mean reward: 93.98 - Last mean reward per episode: 93.74
Num timesteps: 234000
Best mean reward: 93.98 - Last mean reward per episode: 93.77
Num timesteps: 235000
Best mean reward: 93.98 - Last mean reward per episode: 92.17
Num timesteps: 236000
Best mean reward: 93.98 - Last mean reward per episode: 92.26
Num timesteps: 237000
Best mean reward: 93.98 - Last mean reward per episode: 92.71
Num timesteps: 238000
Best mean reward: 93.98 - Last mean reward per episode: 92.75
Num timesteps: 239000
Best mean reward: 93.98 - Last mean reward per episode: 91.26
Num timesteps: 240000
Best mean reward: 93.98 - Last mean reward per episode: 91.23
--------------------------------------
| reference_Q_mean        | 60.2     |
| reference_Q_std         | 6.76     |
| reference_action_mean   | -0.556   |
| reference_action_std    | 0.628    |
| reference_actor_Q_mean  | 61.3     |
| reference_actor_Q_std   | 6.63     |
| rollout/Q_mean          | 36       |
| rollout/actions_mean    | 0.106    |
| rollout/actions_std     | 0.768    |
| rollout/episode_steps   | 174      |
| rollout/episodes        | 1.37e+03 |
| rollout/return          | 83.6     |
| rollout/return_history  | 91.2     |
| total/duration          | 577      |
| total/episodes          | 1.37e+03 |
| total/epochs            | 1        |
| total/steps             | 239998   |
| total/steps_per_second  | 416      |
| train/loss_actor        | -71.6    |
| train/loss_critic       | 0.227    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 241000
Best mean reward: 93.98 - Last mean reward per episode: 91.07
Num timesteps: 242000
Best mean reward: 93.98 - Last mean reward per episode: 91.04
Num timesteps: 243000
Best mean reward: 93.98 - Last mean reward per episode: 91.05
Num timesteps: 244000
Best mean reward: 93.98 - Last mean reward per episode: 89.33
Num timesteps: 245000
Best mean reward: 93.98 - Last mean reward per episode: 89.29
Num timesteps: 246000
Best mean reward: 93.98 - Last mean reward per episode: 90.88
Num timesteps: 247000
Best mean reward: 93.98 - Last mean reward per episode: 90.81
Num timesteps: 248000
Best mean reward: 93.98 - Last mean reward per episode: 92.30
Num timesteps: 249000
Best mean reward: 93.98 - Last mean reward per episode: 92.37
Num timesteps: 250000
Best mean reward: 93.98 - Last mean reward per episode: 92.52
--------------------------------------
| reference_Q_mean        | 59.3     |
| reference_Q_std         | 5.48     |
| reference_action_mean   | -0.593   |
| reference_action_std    | 0.647    |
| reference_actor_Q_mean  | 60.8     |
| reference_actor_Q_std   | 5.23     |
| rollout/Q_mean          | 37.4     |
| rollout/actions_mean    | 0.105    |
| rollout/actions_std     | 0.771    |
| rollout/episode_steps   | 168      |
| rollout/episodes        | 1.48e+03 |
| rollout/return          | 84.3     |
| rollout/return_history  | 92.5     |
| total/duration          | 602      |
| total/episodes          | 1.48e+03 |
| total/epochs            | 1        |
| total/steps             | 249998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -71.5    |
| train/loss_critic       | 0.163    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 251000
Best mean reward: 93.98 - Last mean reward per episode: 92.55
Num timesteps: 252000
Best mean reward: 93.98 - Last mean reward per episode: 94.27
Saving new best model to ./modelos/DDPG/mountain-13.pkl
Num timesteps: 253000
Best mean reward: 94.27 - Last mean reward per episode: 94.18
Num timesteps: 254000
Best mean reward: 94.27 - Last mean reward per episode: 94.11
Num timesteps: 255000
Best mean reward: 94.27 - Last mean reward per episode: 92.69
Num timesteps: 256000
Best mean reward: 94.27 - Last mean reward per episode: 92.68
Num timesteps: 257000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
Num timesteps: 258000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
Num timesteps: 259000
Best mean reward: 94.27 - Last mean reward per episode: 92.65
Num timesteps: 260000
Best mean reward: 94.27 - Last mean reward per episode: 92.69
--------------------------------------
| reference_Q_mean        | 58.4     |
| reference_Q_std         | 5.3      |
| reference_action_mean   | -0.456   |
| reference_action_std    | 0.732    |
| reference_actor_Q_mean  | 59.6     |
| reference_actor_Q_std   | 4.97     |
| rollout/Q_mean          | 38.7     |
| rollout/actions_mean    | 0.103    |
| rollout/actions_std     | 0.772    |
| rollout/episode_steps   | 164      |
| rollout/episodes        | 1.59e+03 |
| rollout/return          | 84.8     |
| rollout/return_history  | 92.7     |
| total/duration          | 626      |
| total/episodes          | 1.59e+03 |
| total/epochs            | 1        |
| total/steps             | 259998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -71      |
| train/loss_critic       | 0.183    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 261000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
Num timesteps: 262000
Best mean reward: 94.27 - Last mean reward per episode: 92.64
Num timesteps: 263000
Best mean reward: 94.27 - Last mean reward per episode: 94.15
Num timesteps: 264000
Best mean reward: 94.27 - Last mean reward per episode: 94.10
Num timesteps: 265000
Best mean reward: 94.27 - Last mean reward per episode: 94.04
Num timesteps: 266000
Best mean reward: 94.27 - Last mean reward per episode: 94.02
Num timesteps: 267000
Best mean reward: 94.27 - Last mean reward per episode: 93.88
Num timesteps: 268000
Best mean reward: 94.27 - Last mean reward per episode: 93.85
Num timesteps: 269000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 270000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
--------------------------------------
| reference_Q_mean        | 56.3     |
| reference_Q_std         | 5.53     |
| reference_action_mean   | -0.263   |
| reference_action_std    | 0.808    |
| reference_actor_Q_mean  | 57.3     |
| reference_actor_Q_std   | 5.16     |
| rollout/Q_mean          | 39.9     |
| rollout/actions_mean    | 0.108    |
| rollout/actions_std     | 0.772    |
| rollout/episode_steps   | 160      |
| rollout/episodes        | 1.69e+03 |
| rollout/return          | 85.4     |
| rollout/return_history  | 93.7     |
| total/duration          | 650      |
| total/episodes          | 1.69e+03 |
| total/epochs            | 1        |
| total/steps             | 269998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 2.56     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 271000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 272000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 273000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 274000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 275000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 276000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 277000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 278000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 279000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 280000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
--------------------------------------
| reference_Q_mean        | 54.5     |
| reference_Q_std         | 5.52     |
| reference_action_mean   | -0.122   |
| reference_action_std    | 0.804    |
| reference_actor_Q_mean  | 55.2     |
| reference_actor_Q_std   | 5.26     |
| rollout/Q_mean          | 41       |
| rollout/actions_mean    | 0.118    |
| rollout/actions_std     | 0.772    |
| rollout/episode_steps   | 157      |
| rollout/episodes        | 1.79e+03 |
| rollout/return          | 85.8     |
| rollout/return_history  | 93.3     |
| total/duration          | 673      |
| total/episodes          | 1.79e+03 |
| total/epochs            | 1        |
| total/steps             | 279998   |
| total/steps_per_second  | 416      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.634    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 281000
Best mean reward: 94.27 - Last mean reward per episode: 93.14
Num timesteps: 282000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 283000
Best mean reward: 94.27 - Last mean reward per episode: 93.05
Num timesteps: 284000
Best mean reward: 94.27 - Last mean reward per episode: 92.98
Num timesteps: 285000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 286000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 287000
Best mean reward: 94.27 - Last mean reward per episode: 92.81
Num timesteps: 288000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
Num timesteps: 289000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 290000
Best mean reward: 94.27 - Last mean reward per episode: 92.73
--------------------------------------
| reference_Q_mean        | 52.9     |
| reference_Q_std         | 5.48     |
| reference_action_mean   | -0.21    |
| reference_action_std    | 0.711    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 5.34     |
| rollout/Q_mean          | 41.9     |
| rollout/actions_mean    | 0.126    |
| rollout/actions_std     | 0.77     |
| rollout/episode_steps   | 155      |
| rollout/episodes        | 1.87e+03 |
| rollout/return          | 86.1     |
| rollout/return_history  | 92.7     |
| total/duration          | 697      |
| total/episodes          | 1.87e+03 |
| total/epochs            | 1        |
| total/steps             | 289998   |
| total/steps_per_second  | 416      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 0.127    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 291000
Best mean reward: 94.27 - Last mean reward per episode: 92.61
Num timesteps: 292000
Best mean reward: 94.27 - Last mean reward per episode: 92.62
Num timesteps: 293000
Best mean reward: 94.27 - Last mean reward per episode: 92.73
Num timesteps: 294000
Best mean reward: 94.27 - Last mean reward per episode: 92.68
Num timesteps: 295000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 296000
Best mean reward: 94.27 - Last mean reward per episode: 92.65
Num timesteps: 297000
Best mean reward: 94.27 - Last mean reward per episode: 91.17
Num timesteps: 298000
Best mean reward: 94.27 - Last mean reward per episode: 91.21
Num timesteps: 299000
Best mean reward: 94.27 - Last mean reward per episode: 91.10
Num timesteps: 300000
Best mean reward: 94.27 - Last mean reward per episode: 91.12
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 5.48     |
| reference_action_mean   | -0.184   |
| reference_action_std    | 0.863    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 5.47     |
| rollout/Q_mean          | 42.7     |
| rollout/actions_mean    | 0.134    |
| rollout/actions_std     | 0.77     |
| rollout/episode_steps   | 154      |
| rollout/episodes        | 1.95e+03 |
| rollout/return          | 86.3     |
| rollout/return_history  | 91.1     |
| total/duration          | 723      |
| total/episodes          | 1.95e+03 |
| total/epochs            | 1        |
| total/steps             | 299998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 0.761    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 301000
Best mean reward: 94.27 - Last mean reward per episode: 91.22
Num timesteps: 302000
Best mean reward: 94.27 - Last mean reward per episode: 91.09
Num timesteps: 303000
Best mean reward: 94.27 - Last mean reward per episode: 91.15
Num timesteps: 304000
Best mean reward: 94.27 - Last mean reward per episode: 91.22
Num timesteps: 305000
Best mean reward: 94.27 - Last mean reward per episode: 91.36
Num timesteps: 306000
Best mean reward: 94.27 - Last mean reward per episode: 91.47
Num timesteps: 307000
Best mean reward: 94.27 - Last mean reward per episode: 92.96
Num timesteps: 308000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 309000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 310000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 5.69     |
| reference_action_mean   | -0.498   |
| reference_action_std    | 0.798    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 5.64     |
| rollout/Q_mean          | 43.5     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.771    |
| rollout/episode_steps   | 151      |
| rollout/episodes        | 2.05e+03 |
| rollout/return          | 86.7     |
| rollout/return_history  | 93.3     |
| total/duration          | 747      |
| total/episodes          | 2.05e+03 |
| total/epochs            | 1        |
| total/steps             | 309998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -66.9    |
| train/loss_critic       | 0.151    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 311000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 312000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 313000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 314000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 315000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 316000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 317000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 318000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 319000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
Num timesteps: 320000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 5.7      |
| reference_action_mean   | -0.514   |
| reference_action_std    | 0.817    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 5.67     |
| rollout/Q_mean          | 44.4     |
| rollout/actions_mean    | 0.139    |
| rollout/actions_std     | 0.772    |
| rollout/episode_steps   | 148      |
| rollout/episodes        | 2.16e+03 |
| rollout/return          | 87       |
| rollout/return_history  | 93.4     |
| total/duration          | 771      |
| total/episodes          | 2.16e+03 |
| total/epochs            | 1        |
| total/steps             | 319998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -66.7    |
| train/loss_critic       | 0.253    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 321000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 322000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 323000
Best mean reward: 94.27 - Last mean reward per episode: 91.82
Num timesteps: 324000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 325000
Best mean reward: 94.27 - Last mean reward per episode: 91.81
Num timesteps: 326000
Best mean reward: 94.27 - Last mean reward per episode: 91.92
Num timesteps: 327000
Best mean reward: 94.27 - Last mean reward per episode: 92.14
Num timesteps: 328000
Best mean reward: 94.27 - Last mean reward per episode: 92.17
Num timesteps: 329000
Best mean reward: 94.27 - Last mean reward per episode: 92.09
Num timesteps: 330000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 5.71     |
| reference_action_mean   | -0.455   |
| reference_action_std    | 0.857    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 5.71     |
| rollout/Q_mean          | 45.1     |
| rollout/actions_mean    | 0.135    |
| rollout/actions_std     | 0.774    |
| rollout/episode_steps   | 146      |
| rollout/episodes        | 2.25e+03 |
| rollout/return          | 87.2     |
| rollout/return_history  | 92       |
| total/duration          | 795      |
| total/episodes          | 2.25e+03 |
| total/epochs            | 1        |
| total/steps             | 329998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -67.2    |
| train/loss_critic       | 0.793    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 331000
Best mean reward: 94.27 - Last mean reward per episode: 92.02
Num timesteps: 332000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 333000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 334000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 335000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 336000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 337000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 338000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 339000
Best mean reward: 94.27 - Last mean reward per episode: 91.93
Num timesteps: 340000
Best mean reward: 94.27 - Last mean reward per episode: 91.72
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 5.83     |
| reference_action_mean   | -0.426   |
| reference_action_std    | 0.851    |
| reference_actor_Q_mean  | 50       |
| reference_actor_Q_std   | 5.83     |
| rollout/Q_mean          | 45.8     |
| rollout/actions_mean    | 0.134    |
| rollout/actions_std     | 0.777    |
| rollout/episode_steps   | 144      |
| rollout/episodes        | 2.35e+03 |
| rollout/return          | 87.4     |
| rollout/return_history  | 91.7     |
| total/duration          | 819      |
| total/episodes          | 2.35e+03 |
| total/epochs            | 1        |
| total/steps             | 339998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 0.288    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 341000
Best mean reward: 94.27 - Last mean reward per episode: 91.60
Num timesteps: 342000
Best mean reward: 94.27 - Last mean reward per episode: 91.57
Num timesteps: 343000
Best mean reward: 94.27 - Last mean reward per episode: 91.36
Num timesteps: 344000
Best mean reward: 94.27 - Last mean reward per episode: 91.29
Num timesteps: 345000
Best mean reward: 94.27 - Last mean reward per episode: 91.06
Num timesteps: 346000
Best mean reward: 94.27 - Last mean reward per episode: 90.99
Num timesteps: 347000
Best mean reward: 94.27 - Last mean reward per episode: 90.93
Num timesteps: 348000
Best mean reward: 94.27 - Last mean reward per episode: 90.82
Num timesteps: 349000
Best mean reward: 94.27 - Last mean reward per episode: 92.71
Num timesteps: 350000
Best mean reward: 94.27 - Last mean reward per episode: 92.73
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 6.16     |
| reference_action_mean   | -0.466   |
| reference_action_std    | 0.856    |
| reference_actor_Q_mean  | 50       |
| reference_actor_Q_std   | 6.21     |
| rollout/Q_mean          | 46.5     |
| rollout/actions_mean    | 0.135    |
| rollout/actions_std     | 0.778    |
| rollout/episode_steps   | 143      |
| rollout/episodes        | 2.45e+03 |
| rollout/return          | 87.6     |
| rollout/return_history  | 92.7     |
| total/duration          | 844      |
| total/episodes          | 2.45e+03 |
| total/epochs            | 1        |
| total/steps             | 349998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -69      |
| train/loss_critic       | 2        |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 351000
Best mean reward: 94.27 - Last mean reward per episode: 92.88
Num timesteps: 352000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 353000
Best mean reward: 94.27 - Last mean reward per episode: 92.49
Num timesteps: 354000
Best mean reward: 94.27 - Last mean reward per episode: 92.55
Num timesteps: 355000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 356000
Best mean reward: 94.27 - Last mean reward per episode: 92.87
Num timesteps: 357000
Best mean reward: 94.27 - Last mean reward per episode: 92.89
Num timesteps: 358000
Best mean reward: 94.27 - Last mean reward per episode: 92.89
Num timesteps: 359000
Best mean reward: 94.27 - Last mean reward per episode: 91.14
Num timesteps: 360000
Best mean reward: 94.27 - Last mean reward per episode: 91.29
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 6.16     |
| reference_action_mean   | -0.774   |
| reference_action_std    | 0.607    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 6.21     |
| rollout/Q_mean          | 47.1     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.781    |
| rollout/episode_steps   | 141      |
| rollout/episodes        | 2.55e+03 |
| rollout/return          | 87.7     |
| rollout/return_history  | 91.3     |
| total/duration          | 869      |
| total/episodes          | 2.55e+03 |
| total/epochs            | 1        |
| total/steps             | 359998   |
| total/steps_per_second  | 414      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.25     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 361000
Best mean reward: 94.27 - Last mean reward per episode: 91.35
Num timesteps: 362000
Best mean reward: 94.27 - Last mean reward per episode: 91.94
Num timesteps: 363000
Best mean reward: 94.27 - Last mean reward per episode: 91.94
Num timesteps: 364000
Best mean reward: 94.27 - Last mean reward per episode: 91.85
Num timesteps: 365000
Best mean reward: 94.27 - Last mean reward per episode: 91.81
Num timesteps: 366000
Best mean reward: 94.27 - Last mean reward per episode: 91.76
Num timesteps: 367000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 368000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 369000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 370000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 6.07     |
| reference_action_mean   | -0.687   |
| reference_action_std    | 0.694    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 6.19     |
| rollout/Q_mean          | 47.8     |
| rollout/actions_mean    | 0.134    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 139      |
| rollout/episodes        | 2.67e+03 |
| rollout/return          | 88       |
| rollout/return_history  | 93.3     |
| total/duration          | 892      |
| total/episodes          | 2.67e+03 |
| total/epochs            | 1        |
| total/steps             | 369998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.209    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 371000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 372000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
Num timesteps: 373000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
Num timesteps: 374000
Best mean reward: 94.27 - Last mean reward per episode: 92.76
Num timesteps: 375000
Best mean reward: 94.27 - Last mean reward per episode: 92.89
Num timesteps: 376000
Best mean reward: 94.27 - Last mean reward per episode: 92.93
Num timesteps: 377000
Best mean reward: 94.27 - Last mean reward per episode: 92.94
Num timesteps: 378000
Best mean reward: 94.27 - Last mean reward per episode: 93.03
Num timesteps: 379000
Best mean reward: 94.27 - Last mean reward per episode: 93.03
Num timesteps: 380000
Best mean reward: 94.27 - Last mean reward per episode: 93.17
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 5.76     |
| reference_action_mean   | -0.699   |
| reference_action_std    | 0.679    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 6.04     |
| rollout/Q_mean          | 48.4     |
| rollout/actions_mean    | 0.134    |
| rollout/actions_std     | 0.785    |
| rollout/episode_steps   | 137      |
| rollout/episodes        | 2.77e+03 |
| rollout/return          | 88.2     |
| rollout/return_history  | 93.2     |
| total/duration          | 916      |
| total/episodes          | 2.77e+03 |
| total/epochs            | 1        |
| total/steps             | 379998   |
| total/steps_per_second  | 415      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.194    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 381000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 382000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 383000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 384000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 385000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 386000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 387000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 388000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 389000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 390000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 5.59     |
| reference_action_mean   | -0.473   |
| reference_action_std    | 0.831    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 5.7      |
| rollout/Q_mean          | 48.9     |
| rollout/actions_mean    | 0.136    |
| rollout/actions_std     | 0.786    |
| rollout/episode_steps   | 135      |
| rollout/episodes        | 2.89e+03 |
| rollout/return          | 88.4     |
| rollout/return_history  | 93.6     |
| total/duration          | 942      |
| total/episodes          | 2.89e+03 |
| total/epochs            | 1        |
| total/steps             | 389998   |
| total/steps_per_second  | 414      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.174    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 391000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 392000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 393000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 394000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 395000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 396000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 397000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 398000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 399000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 400000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 5.56     |
| reference_action_mean   | -0.321   |
| reference_action_std    | 0.911    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 5.56     |
| rollout/Q_mean          | 49.5     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.788    |
| rollout/episode_steps   | 133      |
| rollout/episodes        | 3e+03    |
| rollout/return          | 88.6     |
| rollout/return_history  | 93.5     |
| total/duration          | 967      |
| total/episodes          | 3e+03    |
| total/epochs            | 1        |
| total/steps             | 399998   |
| total/steps_per_second  | 414      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.241    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 401000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 402000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 403000
Best mean reward: 94.27 - Last mean reward per episode: 91.57
Num timesteps: 404000
Best mean reward: 94.27 - Last mean reward per episode: 91.38
Num timesteps: 405000
Best mean reward: 94.27 - Last mean reward per episode: 91.45
Num timesteps: 406000
Best mean reward: 94.27 - Last mean reward per episode: 91.41
Num timesteps: 407000
Best mean reward: 94.27 - Last mean reward per episode: 91.25
Num timesteps: 408000
Best mean reward: 94.27 - Last mean reward per episode: 91.40
Num timesteps: 409000
Best mean reward: 94.27 - Last mean reward per episode: 91.50
Num timesteps: 410000
Best mean reward: 94.27 - Last mean reward per episode: 91.50
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 5.36     |
| reference_action_mean   | -0.307   |
| reference_action_std    | 0.913    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 5.28     |
| rollout/Q_mean          | 50       |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.789    |
| rollout/episode_steps   | 132      |
| rollout/episodes        | 3.1e+03  |
| rollout/return          | 88.7     |
| rollout/return_history  | 91.5     |
| total/duration          | 991      |
| total/episodes          | 3.1e+03  |
| total/epochs            | 1        |
| total/steps             | 409998   |
| total/steps_per_second  | 414      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.217    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 411000
Best mean reward: 94.27 - Last mean reward per episode: 91.56
Num timesteps: 412000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 413000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 414000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 415000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 416000
Best mean reward: 94.27 - Last mean reward per episode: 91.88
Num timesteps: 417000
Best mean reward: 94.27 - Last mean reward per episode: 91.92
Num timesteps: 418000
Best mean reward: 94.27 - Last mean reward per episode: 91.86
Num timesteps: 419000
Best mean reward: 94.27 - Last mean reward per episode: 91.88
Num timesteps: 420000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 5.43     |
| reference_action_mean   | -0.369   |
| reference_action_std    | 0.887    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 5.28     |
| rollout/Q_mean          | 50.4     |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.791    |
| rollout/episode_steps   | 131      |
| rollout/episodes        | 3.20e+03 |
| rollout/return          | 88.8     |
| rollout/return_history  | 91.8     |
| total/duration          | 1.01e+03 |
| total/episodes          | 3.20e+03 |
| total/epochs            | 1        |
| total/steps             | 419998   |
| total/steps_per_second  | 414      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.228    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 421000
Best mean reward: 94.27 - Last mean reward per episode: 91.82
Num timesteps: 422000
Best mean reward: 94.27 - Last mean reward per episode: 91.81
Num timesteps: 423000
Best mean reward: 94.27 - Last mean reward per episode: 91.65
Num timesteps: 424000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 425000
Best mean reward: 94.27 - Last mean reward per episode: 93.29
Num timesteps: 426000
Best mean reward: 94.27 - Last mean reward per episode: 93.14
Num timesteps: 427000
Best mean reward: 94.27 - Last mean reward per episode: 93.15
Num timesteps: 428000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 429000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 430000
Best mean reward: 94.27 - Last mean reward per episode: 92.66
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 5.46     |
| reference_action_mean   | -0.524   |
| reference_action_std    | 0.808    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 5.27     |
| rollout/Q_mean          | 50.9     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.792    |
| rollout/episode_steps   | 130      |
| rollout/episodes        | 3.30e+03 |
| rollout/return          | 88.9     |
| rollout/return_history  | 92.7     |
| total/duration          | 1.04e+03 |
| total/episodes          | 3.30e+03 |
| total/epochs            | 1        |
| total/steps             | 429998   |
| total/steps_per_second  | 413      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.218    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 431000
Best mean reward: 94.27 - Last mean reward per episode: 92.68
Num timesteps: 432000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 433000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
Num timesteps: 434000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
Num timesteps: 435000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
Num timesteps: 436000
Best mean reward: 94.27 - Last mean reward per episode: 92.96
Num timesteps: 437000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
Num timesteps: 438000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 439000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 440000
Best mean reward: 94.27 - Last mean reward per episode: 93.16
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 5.45     |
| reference_action_mean   | -0.148   |
| reference_action_std    | 0.944    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 5.31     |
| rollout/Q_mean          | 51.3     |
| rollout/actions_mean    | 0.141    |
| rollout/actions_std     | 0.793    |
| rollout/episode_steps   | 129      |
| rollout/episodes        | 3.42e+03 |
| rollout/return          | 89       |
| rollout/return_history  | 93.2     |
| total/duration          | 1.06e+03 |
| total/episodes          | 3.42e+03 |
| total/epochs            | 1        |
| total/steps             | 439998   |
| total/steps_per_second  | 413      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.308    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 441000
Best mean reward: 94.27 - Last mean reward per episode: 93.16
Num timesteps: 442000
Best mean reward: 94.27 - Last mean reward per episode: 91.52
Num timesteps: 443000
Best mean reward: 94.27 - Last mean reward per episode: 91.45
Num timesteps: 444000
Best mean reward: 94.27 - Last mean reward per episode: 91.29
Num timesteps: 445000
Best mean reward: 94.27 - Last mean reward per episode: 91.13
Num timesteps: 446000
Best mean reward: 94.27 - Last mean reward per episode: 91.05
Num timesteps: 447000
Best mean reward: 94.27 - Last mean reward per episode: 90.93
Num timesteps: 448000
Best mean reward: 94.27 - Last mean reward per episode: 90.95
Num timesteps: 449000
Best mean reward: 94.27 - Last mean reward per episode: 90.96
Num timesteps: 450000
Best mean reward: 94.27 - Last mean reward per episode: 90.91
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 5.65     |
| reference_action_mean   | -0.395   |
| reference_action_std    | 0.89     |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 5.46     |
| rollout/Q_mean          | 51.6     |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.794    |
| rollout/episode_steps   | 128      |
| rollout/episodes        | 3.51e+03 |
| rollout/return          | 89.1     |
| rollout/return_history  | 90.9     |
| total/duration          | 1.09e+03 |
| total/episodes          | 3.51e+03 |
| total/epochs            | 1        |
| total/steps             | 449998   |
| total/steps_per_second  | 412      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.237    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 451000
Best mean reward: 94.27 - Last mean reward per episode: 90.88
Num timesteps: 452000
Best mean reward: 94.27 - Last mean reward per episode: 92.44
Num timesteps: 453000
Best mean reward: 94.27 - Last mean reward per episode: 92.55
Num timesteps: 454000
Best mean reward: 94.27 - Last mean reward per episode: 92.44
Num timesteps: 455000
Best mean reward: 94.27 - Last mean reward per episode: 92.51
Num timesteps: 456000
Best mean reward: 94.27 - Last mean reward per episode: 92.52
Num timesteps: 457000
Best mean reward: 94.27 - Last mean reward per episode: 92.64
Num timesteps: 458000
Best mean reward: 94.27 - Last mean reward per episode: 92.57
Num timesteps: 459000
Best mean reward: 94.27 - Last mean reward per episode: 92.55
Num timesteps: 460000
Best mean reward: 94.27 - Last mean reward per episode: 92.65
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 6.03     |
| reference_action_mean   | -0.673   |
| reference_action_std    | 0.717    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 6.13     |
| rollout/Q_mean          | 51.9     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.795    |
| rollout/episode_steps   | 127      |
| rollout/episodes        | 3.61e+03 |
| rollout/return          | 89.2     |
| rollout/return_history  | 92.6     |
| total/duration          | 1.12e+03 |
| total/episodes          | 3.61e+03 |
| total/epochs            | 1        |
| total/steps             | 459998   |
| total/steps_per_second  | 413      |
| train/loss_actor        | -67.7    |
| train/loss_critic       | 0.618    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 461000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 462000
Best mean reward: 94.27 - Last mean reward per episode: 92.76
Num timesteps: 463000
Best mean reward: 94.27 - Last mean reward per episode: 92.62
Num timesteps: 464000
Best mean reward: 94.27 - Last mean reward per episode: 92.71
Num timesteps: 465000
Best mean reward: 94.27 - Last mean reward per episode: 91.18
Num timesteps: 466000
Best mean reward: 94.27 - Last mean reward per episode: 89.76
Num timesteps: 467000
Best mean reward: 94.27 - Last mean reward per episode: 89.92
Num timesteps: 468000
Best mean reward: 94.27 - Last mean reward per episode: 89.95
Num timesteps: 469000
Best mean reward: 94.27 - Last mean reward per episode: 90.00
Num timesteps: 470000
Best mean reward: 94.27 - Last mean reward per episode: 89.98
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 5.79     |
| reference_action_mean   | -0.857   |
| reference_action_std    | 0.509    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 5.7      |
| rollout/Q_mean          | 52.3     |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.797    |
| rollout/episode_steps   | 127      |
| rollout/episodes        | 3.7e+03  |
| rollout/return          | 89.2     |
| rollout/return_history  | 90       |
| total/duration          | 1.14e+03 |
| total/episodes          | 3.7e+03  |
| total/epochs            | 1        |
| total/steps             | 469998   |
| total/steps_per_second  | 413      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 0.255    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 471000
Best mean reward: 94.27 - Last mean reward per episode: 90.03
Num timesteps: 472000
Best mean reward: 94.27 - Last mean reward per episode: 88.38
Num timesteps: 473000
Best mean reward: 94.27 - Last mean reward per episode: 88.44
Num timesteps: 474000
Best mean reward: 94.27 - Last mean reward per episode: 88.43
Num timesteps: 475000
Best mean reward: 94.27 - Last mean reward per episode: 87.11
Num timesteps: 476000
Best mean reward: 94.27 - Last mean reward per episode: 90.11
Num timesteps: 477000
Best mean reward: 94.27 - Last mean reward per episode: 90.03
Num timesteps: 478000
Best mean reward: 94.27 - Last mean reward per episode: 90.12
Num timesteps: 479000
Best mean reward: 94.27 - Last mean reward per episode: 90.21
Num timesteps: 480000
Best mean reward: 94.27 - Last mean reward per episode: 90.13
--------------------------------------
| reference_Q_mean        | 48.7     |
| reference_Q_std         | 5.85     |
| reference_action_mean   | -0.876   |
| reference_action_std    | 0.469    |
| reference_actor_Q_mean  | 49.9     |
| reference_actor_Q_std   | 5.67     |
| rollout/Q_mean          | 52.7     |
| rollout/actions_mean    | 0.143    |
| rollout/actions_std     | 0.799    |
| rollout/episode_steps   | 126      |
| rollout/episodes        | 3.8e+03  |
| rollout/return          | 89.2     |
| rollout/return_history  | 90.1     |
| total/duration          | 1.16e+03 |
| total/episodes          | 3.8e+03  |
| total/epochs            | 1        |
| total/steps             | 479998   |
| total/steps_per_second  | 412      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 1.87     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 481000
Best mean reward: 94.27 - Last mean reward per episode: 91.80
Num timesteps: 482000
Best mean reward: 94.27 - Last mean reward per episode: 91.77
Num timesteps: 483000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 484000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 485000
Best mean reward: 94.27 - Last mean reward per episode: 91.80
Num timesteps: 486000
Best mean reward: 94.27 - Last mean reward per episode: 91.91
Num timesteps: 487000
Best mean reward: 94.27 - Last mean reward per episode: 91.88
Num timesteps: 488000
Best mean reward: 94.27 - Last mean reward per episode: 91.89
Num timesteps: 489000
Best mean reward: 94.27 - Last mean reward per episode: 92.05
Num timesteps: 490000
Best mean reward: 94.27 - Last mean reward per episode: 92.09
--------------------------------------
| reference_Q_mean        | 48.9     |
| reference_Q_std         | 5.98     |
| reference_action_mean   | -0.882   |
| reference_action_std    | 0.457    |
| reference_actor_Q_mean  | 49.7     |
| reference_actor_Q_std   | 5.81     |
| rollout/Q_mean          | 53       |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.8      |
| rollout/episode_steps   | 125      |
| rollout/episodes        | 3.92e+03 |
| rollout/return          | 89.3     |
| rollout/return_history  | 92.1     |
| total/duration          | 1.19e+03 |
| total/episodes          | 3.92e+03 |
| total/epochs            | 1        |
| total/steps             | 489998   |
| total/steps_per_second  | 412      |
| train/loss_actor        | -67.5    |
| train/loss_critic       | 0.725    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 491000
Best mean reward: 94.27 - Last mean reward per episode: 92.13
Num timesteps: 492000
Best mean reward: 94.27 - Last mean reward per episode: 92.18
Num timesteps: 493000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 494000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 495000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 496000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 497000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 498000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 499000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 500000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
--------------------------------------
| reference_Q_mean        | 48       |
| reference_Q_std         | 6.34     |
| reference_action_mean   | -0.919   |
| reference_action_std    | 0.387    |
| reference_actor_Q_mean  | 49.2     |
| reference_actor_Q_std   | 5.92     |
| rollout/Q_mean          | 53.4     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.802    |
| rollout/episode_steps   | 124      |
| rollout/episodes        | 4.05e+03 |
| rollout/return          | 89.5     |
| rollout/return_history  | 93.8     |
| total/duration          | 1.21e+03 |
| total/episodes          | 4.05e+03 |
| total/epochs            | 1        |
| total/steps             | 499998   |
| total/steps_per_second  | 412      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.187    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 501000
Best mean reward: 94.27 - Last mean reward per episode: 93.84
Num timesteps: 502000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 503000
Best mean reward: 94.27 - Last mean reward per episode: 93.90
Num timesteps: 504000
Best mean reward: 94.27 - Last mean reward per episode: 93.77
Num timesteps: 505000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 506000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 507000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 508000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 509000
Best mean reward: 94.27 - Last mean reward per episode: 93.05
Num timesteps: 510000
Best mean reward: 94.27 - Last mean reward per episode: 92.83
--------------------------------------
| reference_Q_mean        | 48       |
| reference_Q_std         | 6.75     |
| reference_action_mean   | -0.823   |
| reference_action_std    | 0.545    |
| reference_actor_Q_mean  | 48.9     |
| reference_actor_Q_std   | 6.18     |
| rollout/Q_mean          | 53.7     |
| rollout/actions_mean    | 0.146    |
| rollout/actions_std     | 0.802    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 4.15e+03 |
| rollout/return          | 89.5     |
| rollout/return_history  | 92.8     |
| total/duration          | 1.24e+03 |
| total/episodes          | 4.15e+03 |
| total/epochs            | 1        |
| total/steps             | 509998   |
| total/steps_per_second  | 412      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.184    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 511000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 512000
Best mean reward: 94.27 - Last mean reward per episode: 92.51
Num timesteps: 513000
Best mean reward: 94.27 - Last mean reward per episode: 92.25
Num timesteps: 514000
Best mean reward: 94.27 - Last mean reward per episode: 92.14
Num timesteps: 515000
Best mean reward: 94.27 - Last mean reward per episode: 91.78
Num timesteps: 516000
Best mean reward: 94.27 - Last mean reward per episode: 91.77
Num timesteps: 517000
Best mean reward: 94.27 - Last mean reward per episode: 91.41
Num timesteps: 518000
Best mean reward: 94.27 - Last mean reward per episode: 91.31
Num timesteps: 519000
Best mean reward: 94.27 - Last mean reward per episode: 91.45
Num timesteps: 520000
Best mean reward: 94.27 - Last mean reward per episode: 91.60
--------------------------------------
| reference_Q_mean        | 47.2     |
| reference_Q_std         | 7.15     |
| reference_action_mean   | -0.923   |
| reference_action_std    | 0.369    |
| reference_actor_Q_mean  | 48.8     |
| reference_actor_Q_std   | 6.31     |
| rollout/Q_mean          | 54       |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.803    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 4.23e+03 |
| rollout/return          | 89.6     |
| rollout/return_history  | 91.6     |
| total/duration          | 1.26e+03 |
| total/episodes          | 4.23e+03 |
| total/epochs            | 1        |
| total/steps             | 519998   |
| total/steps_per_second  | 412      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.271    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 521000
Best mean reward: 94.27 - Last mean reward per episode: 91.94
Num timesteps: 522000
Best mean reward: 94.27 - Last mean reward per episode: 92.20
Num timesteps: 523000
Best mean reward: 94.27 - Last mean reward per episode: 92.64
Num timesteps: 524000
Best mean reward: 94.27 - Last mean reward per episode: 92.43
Num timesteps: 525000
Best mean reward: 94.27 - Last mean reward per episode: 92.46
Num timesteps: 526000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 527000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 528000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 529000
Best mean reward: 94.27 - Last mean reward per episode: 93.03
Num timesteps: 530000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
--------------------------------------
| reference_Q_mean        | 47.3     |
| reference_Q_std         | 7.43     |
| reference_action_mean   | -0.899   |
| reference_action_std    | 0.422    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 5.99     |
| rollout/Q_mean          | 54.3     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.803    |
| rollout/episode_steps   | 122      |
| rollout/episodes        | 4.34e+03 |
| rollout/return          | 89.7     |
| rollout/return_history  | 93       |
| total/duration          | 1.29e+03 |
| total/episodes          | 4.34e+03 |
| total/epochs            | 1        |
| total/steps             | 529998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.253    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 531000
Best mean reward: 94.27 - Last mean reward per episode: 92.98
Num timesteps: 532000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 533000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 534000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 535000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 536000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 537000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 538000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 539000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 540000
Best mean reward: 94.27 - Last mean reward per episode: 91.96
--------------------------------------
| reference_Q_mean        | 47.3     |
| reference_Q_std         | 7.35     |
| reference_action_mean   | -0.877   |
| reference_action_std    | 0.455    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 5.77     |
| rollout/Q_mean          | 54.6     |
| rollout/actions_mean    | 0.143    |
| rollout/actions_std     | 0.805    |
| rollout/episode_steps   | 122      |
| rollout/episodes        | 4.44e+03 |
| rollout/return          | 89.7     |
| rollout/return_history  | 92       |
| total/duration          | 1.31e+03 |
| total/episodes          | 4.44e+03 |
| total/epochs            | 1        |
| total/steps             | 539998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.273    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 541000
Best mean reward: 94.27 - Last mean reward per episode: 91.92
Num timesteps: 542000
Best mean reward: 94.27 - Last mean reward per episode: 91.91
Num timesteps: 543000
Best mean reward: 94.27 - Last mean reward per episode: 91.92
Num timesteps: 544000
Best mean reward: 94.27 - Last mean reward per episode: 91.99
Num timesteps: 545000
Best mean reward: 94.27 - Last mean reward per episode: 91.93
Num timesteps: 546000
Best mean reward: 94.27 - Last mean reward per episode: 92.01
Num timesteps: 547000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
Num timesteps: 548000
Best mean reward: 94.27 - Last mean reward per episode: 92.02
Num timesteps: 549000
Best mean reward: 94.27 - Last mean reward per episode: 90.43
Num timesteps: 550000
Best mean reward: 94.27 - Last mean reward per episode: 92.15
--------------------------------------
| reference_Q_mean        | 46.6     |
| reference_Q_std         | 8.04     |
| reference_action_mean   | -0.896   |
| reference_action_std    | 0.431    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 5.94     |
| rollout/Q_mean          | 54.9     |
| rollout/actions_mean    | 0.141    |
| rollout/actions_std     | 0.806    |
| rollout/episode_steps   | 121      |
| rollout/episodes        | 4.55e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 92.2     |
| total/duration          | 1.34e+03 |
| total/episodes          | 4.55e+03 |
| total/epochs            | 1        |
| total/steps             | 549998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.273    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 551000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
Num timesteps: 552000
Best mean reward: 94.27 - Last mean reward per episode: 90.51
Num timesteps: 553000
Best mean reward: 94.27 - Last mean reward per episode: 90.58
Num timesteps: 554000
Best mean reward: 94.27 - Last mean reward per episode: 90.67
Num timesteps: 555000
Best mean reward: 94.27 - Last mean reward per episode: 90.55
Num timesteps: 556000
Best mean reward: 94.27 - Last mean reward per episode: 90.33
Num timesteps: 557000
Best mean reward: 94.27 - Last mean reward per episode: 90.31
Num timesteps: 558000
Best mean reward: 94.27 - Last mean reward per episode: 90.26
Num timesteps: 559000
Best mean reward: 94.27 - Last mean reward per episode: 91.74
Num timesteps: 560000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
--------------------------------------
| reference_Q_mean        | 46.9     |
| reference_Q_std         | 8.1      |
| reference_action_mean   | -0.905   |
| reference_action_std    | 0.418    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 6.06     |
| rollout/Q_mean          | 55.2     |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.807    |
| rollout/episode_steps   | 120      |
| rollout/episodes        | 4.65e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.37e+03 |
| total/episodes          | 4.65e+03 |
| total/epochs            | 1        |
| total/steps             | 559998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.5      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 561000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 562000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 563000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 564000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 565000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 566000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 567000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 568000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 569000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 570000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
--------------------------------------
| reference_Q_mean        | 46.4     |
| reference_Q_std         | 8.78     |
| reference_action_mean   | -0.772   |
| reference_action_std    | 0.628    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 6.55     |
| rollout/Q_mean          | 55.4     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.808    |
| rollout/episode_steps   | 120      |
| rollout/episodes        | 4.77e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.39e+03 |
| total/episodes          | 4.77e+03 |
| total/epochs            | 1        |
| total/steps             | 569998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.278    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 571000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 572000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
Num timesteps: 573000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 574000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 575000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 576000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 577000
Best mean reward: 94.27 - Last mean reward per episode: 93.23
Num timesteps: 578000
Best mean reward: 94.27 - Last mean reward per episode: 93.29
Num timesteps: 579000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 580000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
--------------------------------------
| reference_Q_mean        | 47.3     |
| reference_Q_std         | 7.67     |
| reference_action_mean   | -0.899   |
| reference_action_std    | 0.426    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 5.8      |
| rollout/Q_mean          | 55.7     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 119      |
| rollout/episodes        | 4.88e+03 |
| rollout/return          | 90       |
| rollout/return_history  | 93.5     |
| total/duration          | 1.42e+03 |
| total/episodes          | 4.88e+03 |
| total/epochs            | 1        |
| total/steps             | 579998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 1.21     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 581000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 582000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 583000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 584000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 585000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 586000
Best mean reward: 94.27 - Last mean reward per episode: 91.73
Num timesteps: 587000
Best mean reward: 94.27 - Last mean reward per episode: 91.64
Num timesteps: 588000
Best mean reward: 94.27 - Last mean reward per episode: 91.72
Num timesteps: 589000
Best mean reward: 94.27 - Last mean reward per episode: 91.96
Num timesteps: 590000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 6.58     |
| reference_action_mean   | -0.916   |
| reference_action_std    | 0.389    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 5.68     |
| rollout/Q_mean          | 56       |
| rollout/actions_mean    | 0.136    |
| rollout/actions_std     | 0.811    |
| rollout/episode_steps   | 118      |
| rollout/episodes        | 5e+03    |
| rollout/return          | 90       |
| rollout/return_history  | 92       |
| total/duration          | 1.44e+03 |
| total/episodes          | 5e+03    |
| total/epochs            | 1        |
| total/steps             | 589998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.257    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 591000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
Num timesteps: 592000
Best mean reward: 94.27 - Last mean reward per episode: 90.25
Num timesteps: 593000
Best mean reward: 94.27 - Last mean reward per episode: 90.29
Num timesteps: 594000
Best mean reward: 94.27 - Last mean reward per episode: 90.31
Num timesteps: 595000
Best mean reward: 94.27 - Last mean reward per episode: 92.15
Num timesteps: 596000
Best mean reward: 94.27 - Last mean reward per episode: 92.24
Num timesteps: 597000
Best mean reward: 94.27 - Last mean reward per episode: 92.25
Num timesteps: 598000
Best mean reward: 94.27 - Last mean reward per episode: 90.50
Num timesteps: 599000
Best mean reward: 94.27 - Last mean reward per episode: 90.56
Num timesteps: 600000
Best mean reward: 94.27 - Last mean reward per episode: 92.18
--------------------------------------
| reference_Q_mean        | 49.1     |
| reference_Q_std         | 6.74     |
| reference_action_mean   | -0.92    |
| reference_action_std    | 0.387    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 5.76     |
| rollout/Q_mean          | 56.2     |
| rollout/actions_mean    | 0.134    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 118      |
| rollout/episodes        | 5.1e+03  |
| rollout/return          | 90.1     |
| rollout/return_history  | 92.2     |
| total/duration          | 1.46e+03 |
| total/episodes          | 5.1e+03  |
| total/epochs            | 1        |
| total/steps             | 599998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 1.16     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 601000
Best mean reward: 94.27 - Last mean reward per episode: 92.10
Num timesteps: 602000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
Num timesteps: 603000
Best mean reward: 94.27 - Last mean reward per episode: 92.08
Num timesteps: 604000
Best mean reward: 94.27 - Last mean reward per episode: 91.98
Num timesteps: 605000
Best mean reward: 94.27 - Last mean reward per episode: 91.98
Num timesteps: 606000
Best mean reward: 94.27 - Last mean reward per episode: 93.71
Num timesteps: 607000
Best mean reward: 94.27 - Last mean reward per episode: 93.71
Num timesteps: 608000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 609000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 610000
Best mean reward: 94.27 - Last mean reward per episode: 93.77
--------------------------------------
| reference_Q_mean        | 49.2     |
| reference_Q_std         | 7.53     |
| reference_action_mean   | -0.836   |
| reference_action_std    | 0.527    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 5.99     |
| rollout/Q_mean          | 56.4     |
| rollout/actions_mean    | 0.135    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 5.22e+03 |
| rollout/return          | 90.1     |
| rollout/return_history  | 93.8     |
| total/duration          | 1.49e+03 |
| total/episodes          | 5.22e+03 |
| total/epochs            | 1        |
| total/steps             | 609998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 2.02     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 611000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 612000
Best mean reward: 94.27 - Last mean reward per episode: 93.80
Num timesteps: 613000
Best mean reward: 94.27 - Last mean reward per episode: 93.80
Num timesteps: 614000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 615000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 616000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 617000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 618000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 619000
Best mean reward: 94.27 - Last mean reward per episode: 93.64
Num timesteps: 620000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
--------------------------------------
| reference_Q_mean        | 47.8     |
| reference_Q_std         | 8.42     |
| reference_action_mean   | -0.905   |
| reference_action_std    | 0.406    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 6.14     |
| rollout/Q_mean          | 56.7     |
| rollout/actions_mean    | 0.136    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 116      |
| rollout/episodes        | 5.34e+03 |
| rollout/return          | 90.2     |
| rollout/return_history  | 93.6     |
| total/duration          | 1.51e+03 |
| total/episodes          | 5.34e+03 |
| total/epochs            | 1        |
| total/steps             | 619998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.274    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 621000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 622000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 623000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 624000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 625000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 626000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 627000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 628000
Best mean reward: 94.27 - Last mean reward per episode: 93.23
Num timesteps: 629000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
Num timesteps: 630000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
--------------------------------------
| reference_Q_mean        | 46.9     |
| reference_Q_std         | 8.82     |
| reference_action_mean   | -0.889   |
| reference_action_std    | 0.447    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 6.32     |
| rollout/Q_mean          | 56.9     |
| rollout/actions_mean    | 0.134    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 116      |
| rollout/episodes        | 5.44e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 92.7     |
| total/duration          | 1.54e+03 |
| total/episodes          | 5.44e+03 |
| total/epochs            | 1        |
| total/steps             | 629998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 1.11     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 631000
Best mean reward: 94.27 - Last mean reward per episode: 92.61
Num timesteps: 632000
Best mean reward: 94.27 - Last mean reward per episode: 92.57
Num timesteps: 633000
Best mean reward: 94.27 - Last mean reward per episode: 92.51
Num timesteps: 634000
Best mean reward: 94.27 - Last mean reward per episode: 92.52
Num timesteps: 635000
Best mean reward: 94.27 - Last mean reward per episode: 92.61
Num timesteps: 636000
Best mean reward: 94.27 - Last mean reward per episode: 92.41
Num timesteps: 637000
Best mean reward: 94.27 - Last mean reward per episode: 92.62
Num timesteps: 638000
Best mean reward: 94.27 - Last mean reward per episode: 92.45
Num timesteps: 639000
Best mean reward: 94.27 - Last mean reward per episode: 92.96
Num timesteps: 640000
Best mean reward: 94.27 - Last mean reward per episode: 92.73
--------------------------------------
| reference_Q_mean        | 48.7     |
| reference_Q_std         | 7.31     |
| reference_action_mean   | -0.823   |
| reference_action_std    | 0.554    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 6.07     |
| rollout/Q_mean          | 57.1     |
| rollout/actions_mean    | 0.135    |
| rollout/actions_std     | 0.815    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 5.54e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 92.7     |
| total/duration          | 1.56e+03 |
| total/episodes          | 5.54e+03 |
| total/epochs            | 1        |
| total/steps             | 639998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.293    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 641000
Best mean reward: 94.27 - Last mean reward per episode: 91.23
Num timesteps: 642000
Best mean reward: 94.27 - Last mean reward per episode: 91.13
Num timesteps: 643000
Best mean reward: 94.27 - Last mean reward per episode: 91.07
Num timesteps: 644000
Best mean reward: 94.27 - Last mean reward per episode: 91.07
Num timesteps: 645000
Best mean reward: 94.27 - Last mean reward per episode: 90.97
Num timesteps: 646000
Best mean reward: 94.27 - Last mean reward per episode: 89.23
Num timesteps: 647000
Best mean reward: 94.27 - Last mean reward per episode: 89.18
Num timesteps: 648000
Best mean reward: 94.27 - Last mean reward per episode: 89.24
Num timesteps: 649000
Best mean reward: 94.27 - Last mean reward per episode: 89.16
Num timesteps: 650000
Best mean reward: 94.27 - Last mean reward per episode: 89.25
--------------------------------------
| reference_Q_mean        | 48       |
| reference_Q_std         | 7.19     |
| reference_action_mean   | -0.806   |
| reference_action_std    | 0.579    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 6.42     |
| rollout/Q_mean          | 57.3     |
| rollout/actions_mean    | 0.134    |
| rollout/actions_std     | 0.815    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 5.63e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 89.3     |
| total/duration          | 1.59e+03 |
| total/episodes          | 5.63e+03 |
| total/epochs            | 1        |
| total/steps             | 649998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.397    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 651000
Best mean reward: 94.27 - Last mean reward per episode: 89.33
Num timesteps: 652000
Best mean reward: 94.27 - Last mean reward per episode: 90.75
Num timesteps: 653000
Best mean reward: 94.27 - Last mean reward per episode: 90.81
Num timesteps: 654000
Best mean reward: 94.27 - Last mean reward per episode: 90.62
Num timesteps: 655000
Best mean reward: 94.27 - Last mean reward per episode: 90.64
Num timesteps: 656000
Best mean reward: 94.27 - Last mean reward per episode: 89.18
Num timesteps: 657000
Best mean reward: 94.27 - Last mean reward per episode: 91.06
Num timesteps: 658000
Best mean reward: 94.27 - Last mean reward per episode: 91.20
Num timesteps: 659000
Best mean reward: 94.27 - Last mean reward per episode: 91.37
Num timesteps: 660000
Best mean reward: 94.27 - Last mean reward per episode: 91.49
--------------------------------------
| reference_Q_mean        | 48.4     |
| reference_Q_std         | 7.15     |
| reference_action_mean   | -0.906   |
| reference_action_std    | 0.405    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 6.46     |
| rollout/Q_mean          | 57.4     |
| rollout/actions_mean    | 0.135    |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 5.73e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 91.5     |
| total/duration          | 1.61e+03 |
| total/episodes          | 5.73e+03 |
| total/epochs            | 1        |
| total/steps             | 659998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.344    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 661000
Best mean reward: 94.27 - Last mean reward per episode: 91.53
Num timesteps: 662000
Best mean reward: 94.27 - Last mean reward per episode: 91.71
Num timesteps: 663000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
Num timesteps: 664000
Best mean reward: 94.27 - Last mean reward per episode: 92.01
Num timesteps: 665000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 666000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 667000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 668000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 669000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 670000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
--------------------------------------
| reference_Q_mean        | 48.4     |
| reference_Q_std         | 7.3      |
| reference_action_mean   | -0.913   |
| reference_action_std    | 0.397    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 6.44     |
| rollout/Q_mean          | 57.6     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 5.85e+03 |
| rollout/return          | 90.4     |
| rollout/return_history  | 93.5     |
| total/duration          | 1.64e+03 |
| total/episodes          | 5.85e+03 |
| total/epochs            | 1        |
| total/steps             | 669998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.37     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 671000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 672000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 673000
Best mean reward: 94.27 - Last mean reward per episode: 91.86
Num timesteps: 674000
Best mean reward: 94.27 - Last mean reward per episode: 91.73
Num timesteps: 675000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 676000
Best mean reward: 94.27 - Last mean reward per episode: 91.83
Num timesteps: 677000
Best mean reward: 94.27 - Last mean reward per episode: 91.81
Num timesteps: 678000
Best mean reward: 94.27 - Last mean reward per episode: 91.83
Num timesteps: 679000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 680000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 7.09     |
| reference_action_mean   | -0.898   |
| reference_action_std    | 0.423    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 6.34     |
| rollout/Q_mean          | 57.8     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 114      |
| rollout/episodes        | 5.96e+03 |
| rollout/return          | 90.4     |
| rollout/return_history  | 93.6     |
| total/duration          | 1.66e+03 |
| total/episodes          | 5.96e+03 |
| total/epochs            | 1        |
| total/steps             | 679998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 2.01     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 681000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 682000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 683000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 684000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 685000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 686000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 687000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 688000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 689000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 690000
Best mean reward: 94.27 - Last mean reward per episode: 93.93
--------------------------------------
| reference_Q_mean        | 49.1     |
| reference_Q_std         | 6.92     |
| reference_action_mean   | -0.823   |
| reference_action_std    | 0.548    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 6.21     |
| rollout/Q_mean          | 58       |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 114      |
| rollout/episodes        | 6.08e+03 |
| rollout/return          | 90.5     |
| rollout/return_history  | 93.9     |
| total/duration          | 1.68e+03 |
| total/episodes          | 6.08e+03 |
| total/epochs            | 1        |
| total/steps             | 689998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 1.34     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 691000
Best mean reward: 94.27 - Last mean reward per episode: 94.20
Num timesteps: 692000
Best mean reward: 94.27 - Last mean reward per episode: 94.14
Num timesteps: 693000
Best mean reward: 94.27 - Last mean reward per episode: 94.06
Num timesteps: 694000
Best mean reward: 94.27 - Last mean reward per episode: 94.07
Num timesteps: 695000
Best mean reward: 94.27 - Last mean reward per episode: 94.10
Num timesteps: 696000
Best mean reward: 94.27 - Last mean reward per episode: 94.05
Num timesteps: 697000
Best mean reward: 94.27 - Last mean reward per episode: 93.95
Num timesteps: 698000
Best mean reward: 94.27 - Last mean reward per episode: 93.84
Num timesteps: 699000
Best mean reward: 94.27 - Last mean reward per episode: 93.84
Num timesteps: 700000
Best mean reward: 94.27 - Last mean reward per episode: 93.77
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 6.43     |
| reference_action_mean   | -0.728   |
| reference_action_std    | 0.676    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 5.93     |
| rollout/Q_mean          | 58.2     |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 6.2e+03  |
| rollout/return          | 90.5     |
| rollout/return_history  | 93.8     |
| total/duration          | 1.71e+03 |
| total/episodes          | 6.2e+03  |
| total/epochs            | 1        |
| total/steps             | 699998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.395    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 701000
Best mean reward: 94.27 - Last mean reward per episode: 93.71
Num timesteps: 702000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 703000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 704000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 705000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
Num timesteps: 706000
Best mean reward: 94.27 - Last mean reward per episode: 92.78
Num timesteps: 707000
Best mean reward: 94.27 - Last mean reward per episode: 92.72
Num timesteps: 708000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
Num timesteps: 709000
Best mean reward: 94.27 - Last mean reward per episode: 92.66
Num timesteps: 710000
Best mean reward: 94.27 - Last mean reward per episode: 92.43
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 6.74     |
| reference_action_mean   | -0.824   |
| reference_action_std    | 0.525    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 5.83     |
| rollout/Q_mean          | 58.3     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 6.28e+03 |
| rollout/return          | 90.6     |
| rollout/return_history  | 92.4     |
| total/duration          | 1.73e+03 |
| total/episodes          | 6.28e+03 |
| total/epochs            | 1        |
| total/steps             | 709998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.421    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 711000
Best mean reward: 94.27 - Last mean reward per episode: 92.25
Num timesteps: 712000
Best mean reward: 94.27 - Last mean reward per episode: 90.43
Num timesteps: 713000
Best mean reward: 94.27 - Last mean reward per episode: 90.30
Num timesteps: 714000
Best mean reward: 94.27 - Last mean reward per episode: 90.25
Num timesteps: 715000
Best mean reward: 94.27 - Last mean reward per episode: 90.36
Num timesteps: 716000
Best mean reward: 94.27 - Last mean reward per episode: 90.41
Num timesteps: 717000
Best mean reward: 94.27 - Last mean reward per episode: 90.82
Num timesteps: 718000
Best mean reward: 94.27 - Last mean reward per episode: 91.09
Num timesteps: 719000
Best mean reward: 94.27 - Last mean reward per episode: 91.23
Num timesteps: 720000
Best mean reward: 94.27 - Last mean reward per episode: 91.23
--------------------------------------
| reference_Q_mean        | 49.8     |
| reference_Q_std         | 6.46     |
| reference_action_mean   | -0.823   |
| reference_action_std    | 0.544    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 5.51     |
| rollout/Q_mean          | 58.4     |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 6.37e+03 |
| rollout/return          | 90.6     |
| rollout/return_history  | 91.2     |
| total/duration          | 1.76e+03 |
| total/episodes          | 6.37e+03 |
| total/epochs            | 1        |
| total/steps             | 719998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 0.419    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 721000
Best mean reward: 94.27 - Last mean reward per episode: 91.69
Num timesteps: 722000
Best mean reward: 94.27 - Last mean reward per episode: 93.90
Num timesteps: 723000
Best mean reward: 94.27 - Last mean reward per episode: 94.11
Num timesteps: 724000
Best mean reward: 94.27 - Last mean reward per episode: 94.10
Num timesteps: 725000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 726000
Best mean reward: 94.27 - Last mean reward per episode: 92.39
Num timesteps: 727000
Best mean reward: 94.27 - Last mean reward per episode: 92.34
Num timesteps: 728000
Best mean reward: 94.27 - Last mean reward per episode: 92.38
Num timesteps: 729000
Best mean reward: 94.27 - Last mean reward per episode: 92.36
Num timesteps: 730000
Best mean reward: 94.27 - Last mean reward per episode: 92.29
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 6.11     |
| reference_action_mean   | -0.834   |
| reference_action_std    | 0.525    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 5.42     |
| rollout/Q_mean          | 58.6     |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 6.48e+03 |
| rollout/return          | 90.6     |
| rollout/return_history  | 92.3     |
| total/duration          | 1.78e+03 |
| total/episodes          | 6.48e+03 |
| total/epochs            | 1        |
| total/steps             | 729998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.461    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 731000
Best mean reward: 94.27 - Last mean reward per episode: 92.25
Num timesteps: 732000
Best mean reward: 94.27 - Last mean reward per episode: 92.17
Num timesteps: 733000
Best mean reward: 94.27 - Last mean reward per episode: 92.09
Num timesteps: 734000
Best mean reward: 94.27 - Last mean reward per episode: 92.46
Num timesteps: 735000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 736000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 737000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 738000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 739000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 740000
Best mean reward: 94.27 - Last mean reward per episode: 92.30
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 5.68     |
| reference_action_mean   | -0.831   |
| reference_action_std    | 0.531    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 5.39     |
| rollout/Q_mean          | 58.7     |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 6.57e+03 |
| rollout/return          | 90.6     |
| rollout/return_history  | 92.3     |
| total/duration          | 1.81e+03 |
| total/episodes          | 6.57e+03 |
| total/epochs            | 1        |
| total/steps             | 739998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -68.5    |
| train/loss_critic       | 1.33     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 741000
Best mean reward: 94.27 - Last mean reward per episode: 92.25
Num timesteps: 742000
Best mean reward: 94.27 - Last mean reward per episode: 92.29
Num timesteps: 743000
Best mean reward: 94.27 - Last mean reward per episode: 92.35
Num timesteps: 744000
Best mean reward: 94.27 - Last mean reward per episode: 92.27
Num timesteps: 745000
Best mean reward: 94.27 - Last mean reward per episode: 92.35
Num timesteps: 746000
Best mean reward: 94.27 - Last mean reward per episode: 92.39
Num timesteps: 747000
Best mean reward: 94.27 - Last mean reward per episode: 92.29
Num timesteps: 748000
Best mean reward: 94.27 - Last mean reward per episode: 92.33
Num timesteps: 749000
Best mean reward: 94.27 - Last mean reward per episode: 92.40
Num timesteps: 750000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 5.88     |
| reference_action_mean   | -0.83    |
| reference_action_std    | 0.535    |
| reference_actor_Q_mean  | 49.6     |
| reference_actor_Q_std   | 5.82     |
| rollout/Q_mean          | 58.9     |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 6.68e+03 |
| rollout/return          | 90.7     |
| rollout/return_history  | 93.7     |
| total/duration          | 1.84e+03 |
| total/episodes          | 6.68e+03 |
| total/epochs            | 1        |
| total/steps             | 749998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 0.319    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 751000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 752000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 753000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 754000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 755000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 756000
Best mean reward: 94.27 - Last mean reward per episode: 93.87
Num timesteps: 757000
Best mean reward: 94.27 - Last mean reward per episode: 93.88
Num timesteps: 758000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 759000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 760000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
--------------------------------------
| reference_Q_mean        | 48.8     |
| reference_Q_std         | 6.21     |
| reference_action_mean   | -0.529   |
| reference_action_std    | 0.79     |
| reference_actor_Q_mean  | 48.7     |
| reference_actor_Q_std   | 6.26     |
| rollout/Q_mean          | 59       |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 6.79e+03 |
| rollout/return          | 90.7     |
| rollout/return_history  | 93.7     |
| total/duration          | 1.86e+03 |
| total/episodes          | 6.79e+03 |
| total/epochs            | 1        |
| total/steps             | 759998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.429    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 761000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 762000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 763000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 764000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 765000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 766000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 767000
Best mean reward: 94.27 - Last mean reward per episode: 93.17
Num timesteps: 768000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 769000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 770000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
--------------------------------------
| reference_Q_mean        | 48.9     |
| reference_Q_std         | 6.25     |
| reference_action_mean   | -0.709   |
| reference_action_std    | 0.653    |
| reference_actor_Q_mean  | 48.9     |
| reference_actor_Q_std   | 6.16     |
| rollout/Q_mean          | 59.1     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 6.89e+03 |
| rollout/return          | 90.8     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.88e+03 |
| total/episodes          | 6.89e+03 |
| total/epochs            | 1        |
| total/steps             | 769998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.281    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 771000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 772000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
Num timesteps: 773000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 774000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 775000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 776000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 777000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 778000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 779000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 780000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
--------------------------------------
| reference_Q_mean        | 48.8     |
| reference_Q_std         | 6.38     |
| reference_action_mean   | -0.587   |
| reference_action_std    | 0.747    |
| reference_actor_Q_mean  | 49.3     |
| reference_actor_Q_std   | 6.16     |
| rollout/Q_mean          | 59.3     |
| rollout/actions_mean    | 0.139    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 7e+03    |
| rollout/return          | 90.8     |
| rollout/return_history  | 93.1     |
| total/duration          | 1.91e+03 |
| total/episodes          | 7e+03    |
| total/epochs            | 1        |
| total/steps             | 779998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 0.38     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 781000
Best mean reward: 94.27 - Last mean reward per episode: 91.43
Num timesteps: 782000
Best mean reward: 94.27 - Last mean reward per episode: 91.46
Num timesteps: 783000
Best mean reward: 94.27 - Last mean reward per episode: 91.46
Num timesteps: 784000
Best mean reward: 94.27 - Last mean reward per episode: 91.41
Num timesteps: 785000
Best mean reward: 94.27 - Last mean reward per episode: 91.45
Num timesteps: 786000
Best mean reward: 94.27 - Last mean reward per episode: 92.08
Num timesteps: 787000
Best mean reward: 94.27 - Last mean reward per episode: 92.13
Num timesteps: 788000
Best mean reward: 94.27 - Last mean reward per episode: 92.05
Num timesteps: 789000
Best mean reward: 94.27 - Last mean reward per episode: 92.04
Num timesteps: 790000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
--------------------------------------
| reference_Q_mean        | 48.9     |
| reference_Q_std         | 6.33     |
| reference_action_mean   | -0.927   |
| reference_action_std    | 0.35     |
| reference_actor_Q_mean  | 49.3     |
| reference_actor_Q_std   | 6.27     |
| rollout/Q_mean          | 59.4     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 7.11e+03 |
| rollout/return          | 90.8     |
| rollout/return_history  | 93.6     |
| total/duration          | 1.93e+03 |
| total/episodes          | 7.11e+03 |
| total/epochs            | 1        |
| total/steps             | 789998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.242    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 791000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 792000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 793000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 794000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
Num timesteps: 795000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 796000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 797000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 798000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
Num timesteps: 799000
Best mean reward: 94.27 - Last mean reward per episode: 93.14
Num timesteps: 800000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
--------------------------------------
| reference_Q_mean        | 48.3     |
| reference_Q_std         | 6.69     |
| reference_action_mean   | -0.853   |
| reference_action_std    | 0.497    |
| reference_actor_Q_mean  | 48.8     |
| reference_actor_Q_std   | 6.59     |
| rollout/Q_mean          | 59.5     |
| rollout/actions_mean    | 0.137    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 7.2e+03  |
| rollout/return          | 90.8     |
| rollout/return_history  | 92.8     |
| total/duration          | 1.96e+03 |
| total/episodes          | 7.2e+03  |
| total/epochs            | 1        |
| total/steps             | 799998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 0.501    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 801000
Best mean reward: 94.27 - Last mean reward per episode: 92.83
Num timesteps: 802000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
Num timesteps: 803000
Best mean reward: 94.27 - Last mean reward per episode: 92.89
Num timesteps: 804000
Best mean reward: 94.27 - Last mean reward per episode: 93.05
Num timesteps: 805000
Best mean reward: 94.27 - Last mean reward per episode: 92.99
Num timesteps: 806000
Best mean reward: 94.27 - Last mean reward per episode: 92.91
Num timesteps: 807000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 808000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 809000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 810000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
--------------------------------------
| reference_Q_mean        | 48.5     |
| reference_Q_std         | 6.76     |
| reference_action_mean   | -0.75    |
| reference_action_std    | 0.642    |
| reference_actor_Q_mean  | 49       |
| reference_actor_Q_std   | 6.6      |
| rollout/Q_mean          | 59.6     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 7.32e+03 |
| rollout/return          | 90.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.99e+03 |
| total/episodes          | 7.32e+03 |
| total/epochs            | 1        |
| total/steps             | 809998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 1.28     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 811000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 812000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
Num timesteps: 813000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 814000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 815000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 816000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 817000
Best mean reward: 94.27 - Last mean reward per episode: 93.18
Num timesteps: 818000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
Num timesteps: 819000
Best mean reward: 94.27 - Last mean reward per episode: 92.96
Num timesteps: 820000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 6.58     |
| reference_action_mean   | -0.569   |
| reference_action_std    | 0.797    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 6.46     |
| rollout/Q_mean          | 59.8     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 7.42e+03 |
| rollout/return          | 90.9     |
| rollout/return_history  | 93.2     |
| total/duration          | 2.01e+03 |
| total/episodes          | 7.42e+03 |
| total/epochs            | 1        |
| total/steps             | 819998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 1.35     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 821000
Best mean reward: 94.27 - Last mean reward per episode: 93.12
Num timesteps: 822000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 823000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 824000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 825000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 826000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 827000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
Num timesteps: 828000
Best mean reward: 94.27 - Last mean reward per episode: 92.81
Num timesteps: 829000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 830000
Best mean reward: 94.27 - Last mean reward per episode: 92.82
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 5.84     |
| reference_action_mean   | -0.729   |
| reference_action_std    | 0.667    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 5.93     |
| rollout/Q_mean          | 59.9     |
| rollout/actions_mean    | 0.138    |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 7.53e+03 |
| rollout/return          | 90.9     |
| rollout/return_history  | 92.8     |
| total/duration          | 2.04e+03 |
| total/episodes          | 7.53e+03 |
| total/epochs            | 1        |
| total/steps             | 829998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.566    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 831000
Best mean reward: 94.27 - Last mean reward per episode: 92.92
Num timesteps: 832000
Best mean reward: 94.27 - Last mean reward per episode: 93.02
Num timesteps: 833000
Best mean reward: 94.27 - Last mean reward per episode: 93.02
Num timesteps: 834000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 835000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 836000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 837000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 838000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 839000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 840000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 5.63     |
| reference_action_mean   | -0.337   |
| reference_action_std    | 0.918    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 5.79     |
| rollout/Q_mean          | 60       |
| rollout/actions_mean    | 0.14     |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 7.65e+03 |
| rollout/return          | 91       |
| rollout/return_history  | 93.4     |
| total/duration          | 2.06e+03 |
| total/episodes          | 7.65e+03 |
| total/epochs            | 1        |
| total/steps             | 839998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.477    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 841000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 842000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 843000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 844000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 845000
Best mean reward: 94.27 - Last mean reward per episode: 93.23
Num timesteps: 846000
Best mean reward: 94.27 - Last mean reward per episode: 93.17
Num timesteps: 847000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 848000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 849000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 850000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 5.82     |
| reference_action_mean   | -0.254   |
| reference_action_std    | 0.937    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 5.77     |
| rollout/Q_mean          | 60.1     |
| rollout/actions_mean    | 0.14     |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 7.75e+03 |
| rollout/return          | 91       |
| rollout/return_history  | 92.8     |
| total/duration          | 2.09e+03 |
| total/episodes          | 7.75e+03 |
| total/epochs            | 1        |
| total/steps             | 849998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.43     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 851000
Best mean reward: 94.27 - Last mean reward per episode: 92.82
Num timesteps: 852000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
Num timesteps: 853000
Best mean reward: 94.27 - Last mean reward per episode: 92.93
Num timesteps: 854000
Best mean reward: 94.27 - Last mean reward per episode: 92.87
Num timesteps: 855000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 856000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 857000
Best mean reward: 94.27 - Last mean reward per episode: 92.58
Num timesteps: 858000
Best mean reward: 94.27 - Last mean reward per episode: 92.50
Num timesteps: 859000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 860000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 5.72     |
| reference_action_mean   | -0.23    |
| reference_action_std    | 0.947    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 5.65     |
| rollout/Q_mean          | 60.3     |
| rollout/actions_mean    | 0.141    |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 7.86e+03 |
| rollout/return          | 91       |
| rollout/return_history  | 93       |
| total/duration          | 2.11e+03 |
| total/episodes          | 7.86e+03 |
| total/epochs            | 1        |
| total/steps             | 859998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.378    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 861000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 862000
Best mean reward: 94.27 - Last mean reward per episode: 92.85
Num timesteps: 863000
Best mean reward: 94.27 - Last mean reward per episode: 92.83
Num timesteps: 864000
Best mean reward: 94.27 - Last mean reward per episode: 92.87
Num timesteps: 865000
Best mean reward: 94.27 - Last mean reward per episode: 92.99
Num timesteps: 866000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 867000
Best mean reward: 94.27 - Last mean reward per episode: 93.12
Num timesteps: 868000
Best mean reward: 94.27 - Last mean reward per episode: 93.18
Num timesteps: 869000
Best mean reward: 94.27 - Last mean reward per episode: 93.23
Num timesteps: 870000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 5.76     |
| reference_action_mean   | -0.285   |
| reference_action_std    | 0.924    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 5.68     |
| rollout/Q_mean          | 60.4     |
| rollout/actions_mean    | 0.143    |
| rollout/actions_std     | 0.821    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 7.97e+03 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.2     |
| total/duration          | 2.14e+03 |
| total/episodes          | 7.97e+03 |
| total/epochs            | 1        |
| total/steps             | 869998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.543    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 871000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 872000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 873000
Best mean reward: 94.27 - Last mean reward per episode: 93.23
Num timesteps: 874000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 875000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 876000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 877000
Best mean reward: 94.27 - Last mean reward per episode: 93.15
Num timesteps: 878000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 879000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 880000
Best mean reward: 94.27 - Last mean reward per episode: 93.03
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 5.88     |
| reference_action_mean   | -0.372   |
| reference_action_std    | 0.895    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 5.79     |
| rollout/Q_mean          | 60.5     |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.821    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 8.08e+03 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93       |
| total/duration          | 2.17e+03 |
| total/episodes          | 8.08e+03 |
| total/epochs            | 1        |
| total/steps             | 879998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.355    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 881000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 882000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
Num timesteps: 883000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
Num timesteps: 884000
Best mean reward: 94.27 - Last mean reward per episode: 92.76
Num timesteps: 885000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 886000
Best mean reward: 94.27 - Last mean reward per episode: 92.85
Num timesteps: 887000
Best mean reward: 94.27 - Last mean reward per episode: 92.48
Num timesteps: 888000
Best mean reward: 94.27 - Last mean reward per episode: 92.52
Num timesteps: 889000
Best mean reward: 94.27 - Last mean reward per episode: 92.62
Num timesteps: 890000
Best mean reward: 94.27 - Last mean reward per episode: 93.18
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 5.78     |
| reference_action_mean   | -0.257   |
| reference_action_std    | 0.931    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 5.8      |
| rollout/Q_mean          | 60.6     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.821    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 8.19e+03 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.2     |
| total/duration          | 2.19e+03 |
| total/episodes          | 8.19e+03 |
| total/epochs            | 1        |
| total/steps             | 889998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.456    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 891000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 892000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 893000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 894000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 895000
Best mean reward: 94.27 - Last mean reward per episode: 93.16
Num timesteps: 896000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 897000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 898000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 899000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 900000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 5.63     |
| reference_action_mean   | -0.313   |
| reference_action_std    | 0.916    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 5.66     |
| rollout/Q_mean          | 60.7     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.821    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.3e+03  |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 2.22e+03 |
| total/episodes          | 8.3e+03  |
| total/epochs            | 1        |
| total/steps             | 899998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.524    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 901000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
Num timesteps: 902000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 903000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 904000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 905000
Best mean reward: 94.27 - Last mean reward per episode: 91.88
Num timesteps: 906000
Best mean reward: 94.27 - Last mean reward per episode: 91.89
Num timesteps: 907000
Best mean reward: 94.27 - Last mean reward per episode: 92.04
Num timesteps: 908000
Best mean reward: 94.27 - Last mean reward per episode: 91.99
Num timesteps: 909000
Best mean reward: 94.27 - Last mean reward per episode: 92.15
Num timesteps: 910000
Best mean reward: 94.27 - Last mean reward per episode: 92.15
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 5.8      |
| reference_action_mean   | -0.411   |
| reference_action_std    | 0.881    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 5.84     |
| rollout/Q_mean          | 60.8     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.822    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.41e+03 |
| rollout/return          | 91.2     |
| rollout/return_history  | 92.2     |
| total/duration          | 2.25e+03 |
| total/episodes          | 8.41e+03 |
| total/epochs            | 1        |
| total/steps             | 909998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.415    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 911000
Best mean reward: 94.27 - Last mean reward per episode: 92.17
Num timesteps: 912000
Best mean reward: 94.27 - Last mean reward per episode: 92.20
Num timesteps: 913000
Best mean reward: 94.27 - Last mean reward per episode: 90.58
Num timesteps: 914000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
Num timesteps: 915000
Best mean reward: 94.27 - Last mean reward per episode: 91.99
Num timesteps: 916000
Best mean reward: 94.27 - Last mean reward per episode: 91.93
Num timesteps: 917000
Best mean reward: 94.27 - Last mean reward per episode: 91.87
Num timesteps: 918000
Best mean reward: 94.27 - Last mean reward per episode: 91.95
Num timesteps: 919000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
Num timesteps: 920000
Best mean reward: 94.27 - Last mean reward per episode: 92.05
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 5.88     |
| reference_action_mean   | -0.278   |
| reference_action_std    | 0.929    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 5.9      |
| rollout/Q_mean          | 60.9     |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.822    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.52e+03 |
| rollout/return          | 91.2     |
| rollout/return_history  | 92.1     |
| total/duration          | 2.27e+03 |
| total/episodes          | 8.52e+03 |
| total/epochs            | 1        |
| total/steps             | 919998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 1.03     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 921000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 922000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 923000
Best mean reward: 94.27 - Last mean reward per episode: 93.85
Num timesteps: 924000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 925000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 926000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 927000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 928000
Best mean reward: 94.27 - Last mean reward per episode: 91.79
Num timesteps: 929000
Best mean reward: 94.27 - Last mean reward per episode: 91.61
Num timesteps: 930000
Best mean reward: 94.27 - Last mean reward per episode: 91.43
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 6.33     |
| reference_action_mean   | -0.274   |
| reference_action_std    | 0.942    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 6.41     |
| rollout/Q_mean          | 61       |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.823    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.62e+03 |
| rollout/return          | 91.2     |
| rollout/return_history  | 91.4     |
| total/duration          | 2.3e+03  |
| total/episodes          | 8.62e+03 |
| total/epochs            | 1        |
| total/steps             | 929998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.459    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 931000
Best mean reward: 94.27 - Last mean reward per episode: 91.48
Num timesteps: 932000
Best mean reward: 94.27 - Last mean reward per episode: 91.44
Num timesteps: 933000
Best mean reward: 94.27 - Last mean reward per episode: 91.42
Num timesteps: 934000
Best mean reward: 94.27 - Last mean reward per episode: 91.57
Num timesteps: 935000
Best mean reward: 94.27 - Last mean reward per episode: 91.64
Num timesteps: 936000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 937000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 938000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 939000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 940000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 6.48     |
| reference_action_mean   | -0.612   |
| reference_action_std    | 0.78     |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 6.43     |
| rollout/Q_mean          | 61.1     |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.823    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 8.74e+03 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.32e+03 |
| total/episodes          | 8.74e+03 |
| total/epochs            | 1        |
| total/steps             | 939998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.42     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 941000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 942000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 943000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 944000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 945000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 946000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 947000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 948000
Best mean reward: 94.27 - Last mean reward per episode: 93.86
Num timesteps: 949000
Best mean reward: 94.27 - Last mean reward per episode: 93.88
Num timesteps: 950000
Best mean reward: 94.27 - Last mean reward per episode: 93.85
--------------------------------------
| reference_Q_mean        | 52.3     |
| reference_Q_std         | 6.46     |
| reference_action_mean   | -0.635   |
| reference_action_std    | 0.74     |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 6.43     |
| rollout/Q_mean          | 61.2     |
| rollout/actions_mean    | 0.143    |
| rollout/actions_std     | 0.824    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 8.87e+03 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.8     |
| total/duration          | 2.35e+03 |
| total/episodes          | 8.87e+03 |
| total/epochs            | 1        |
| total/steps             | 949998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -71.1    |
| train/loss_critic       | 1.21     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 951000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 952000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 953000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 954000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 955000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 956000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 957000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 958000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 959000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 960000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 6.47     |
| reference_action_mean   | -0.561   |
| reference_action_std    | 0.812    |
| reference_actor_Q_mean  | 53.4     |
| reference_actor_Q_std   | 6.38     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.824    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 8.99e+03 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.37e+03 |
| total/episodes          | 8.99e+03 |
| total/epochs            | 1        |
| total/steps             | 959998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.298    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 961000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 962000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 963000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 964000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 965000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 966000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 967000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 968000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 969000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 970000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
--------------------------------------
| reference_Q_mean        | 52.8     |
| reference_Q_std         | 6.3      |
| reference_action_mean   | -0.443   |
| reference_action_std    | 0.877    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 6.29     |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 9.11e+03 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.6     |
| total/duration          | 2.4e+03  |
| total/episodes          | 9.11e+03 |
| total/epochs            | 1        |
| total/steps             | 969998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.38     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 971000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 972000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 973000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 974000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 975000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 976000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 977000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 978000
Best mean reward: 94.27 - Last mean reward per episode: 93.14
Num timesteps: 979000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 980000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 5.91     |
| reference_action_mean   | -0.329   |
| reference_action_std    | 0.929    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 6.06     |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.146    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 9.21e+03 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.1     |
| total/duration          | 2.43e+03 |
| total/episodes          | 9.21e+03 |
| total/epochs            | 1        |
| total/steps             | 979998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.349    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 981000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 982000
Best mean reward: 94.27 - Last mean reward per episode: 93.15
Num timesteps: 983000
Best mean reward: 94.27 - Last mean reward per episode: 93.18
Num timesteps: 984000
Best mean reward: 94.27 - Last mean reward per episode: 93.15
Num timesteps: 985000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 986000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 987000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 988000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 989000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 990000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 5.86     |
| reference_action_mean   | -0.398   |
| reference_action_std    | 0.904    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 6.24     |
| rollout/Q_mean          | 61.6     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 9.33e+03 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.45e+03 |
| total/episodes          | 9.33e+03 |
| total/epochs            | 1        |
| total/steps             | 989998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.396    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 991000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 992000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 993000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 994000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 995000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 996000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 997000
Best mean reward: 94.27 - Last mean reward per episode: 93.71
Num timesteps: 998000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 999000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 1000000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 5.62     |
| reference_action_mean   | -0.238   |
| reference_action_std    | 0.949    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 5.78     |
| rollout/Q_mean          | 61.7     |
| rollout/actions_mean    | 0.148    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 9.44e+03 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.48e+03 |
| total/episodes          | 9.44e+03 |
| total/epochs            | 1        |
| total/steps             | 999998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.5      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1001000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 1002000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1003000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 1004000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 1005000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 1006000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 1007000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 1008000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 1009000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
Num timesteps: 1010000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 5.87     |
| reference_action_mean   | -0.241   |
| reference_action_std    | 0.946    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 5.92     |
| rollout/Q_mean          | 61.8     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 9.56e+03 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.5e+03  |
| total/episodes          | 9.56e+03 |
| total/epochs            | 1        |
| total/steps             | 1009998  |
| total/steps_per_second  | 403      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.402    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1011000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 1012000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 1013000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 1014000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 1015000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 1016000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 1017000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 1018000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 1019000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 1020000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 5.72     |
| reference_action_mean   | -0.494   |
| reference_action_std    | 0.856    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 5.87     |
| rollout/Q_mean          | 61.8     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 9.67e+03 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.1     |
| total/duration          | 2.53e+03 |
| total/episodes          | 9.67e+03 |
| total/epochs            | 1        |
| total/steps             | 1019998  |
| total/steps_per_second  | 403      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 0.461    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1021000
Best mean reward: 94.27 - Last mean reward per episode: 93.02
Num timesteps: 1022000
Best mean reward: 94.27 - Last mean reward per episode: 92.95
Num timesteps: 1023000
Best mean reward: 94.27 - Last mean reward per episode: 93.12
Num timesteps: 1024000
Best mean reward: 94.27 - Last mean reward per episode: 93.14
Num timesteps: 1025000
Best mean reward: 94.27 - Last mean reward per episode: 91.65
Num timesteps: 1026000
Best mean reward: 94.27 - Last mean reward per episode: 91.67
Num timesteps: 1027000
Best mean reward: 94.27 - Last mean reward per episode: 90.91
Num timesteps: 1028000
Best mean reward: 94.27 - Last mean reward per episode: 91.14
Num timesteps: 1029000
Best mean reward: 94.27 - Last mean reward per episode: 91.39
Num timesteps: 1030000
Best mean reward: 94.27 - Last mean reward per episode: 91.25
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 5.75     |
| reference_action_mean   | -0.0286  |
| reference_action_std    | 0.987    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 5.82     |
| rollout/Q_mean          | 61.9     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 9.77e+03 |
| rollout/return          | 91.4     |
| rollout/return_history  | 91.2     |
| total/duration          | 2.55e+03 |
| total/episodes          | 9.77e+03 |
| total/epochs            | 1        |
| total/steps             | 1029998  |
| total/steps_per_second  | 403      |
| train/loss_actor        | -68.6    |
| train/loss_critic       | 0.409    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1031000
Best mean reward: 94.27 - Last mean reward per episode: 91.17
Num timesteps: 1032000
Best mean reward: 94.27 - Last mean reward per episode: 91.08
Num timesteps: 1033000
Best mean reward: 94.27 - Last mean reward per episode: 91.02
Num timesteps: 1034000
Best mean reward: 94.27 - Last mean reward per episode: 90.87
Num timesteps: 1035000
Best mean reward: 94.27 - Last mean reward per episode: 92.35
Num timesteps: 1036000
Best mean reward: 94.27 - Last mean reward per episode: 92.29
Num timesteps: 1037000
Best mean reward: 94.27 - Last mean reward per episode: 91.61
Num timesteps: 1038000
Best mean reward: 94.27 - Last mean reward per episode: 91.63
Num timesteps: 1039000
Best mean reward: 94.27 - Last mean reward per episode: 91.67
Num timesteps: 1040000
Best mean reward: 94.27 - Last mean reward per episode: 91.69
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 5.63     |
| reference_action_mean   | -0.0436  |
| reference_action_std    | 0.981    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 5.59     |
| rollout/Q_mean          | 61.9     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 9.86e+03 |
| rollout/return          | 91.4     |
| rollout/return_history  | 91.7     |
| total/duration          | 2.58e+03 |
| total/episodes          | 9.86e+03 |
| total/epochs            | 1        |
| total/steps             | 1039998  |
| total/steps_per_second  | 403      |
| train/loss_actor        | -68.4    |
| train/loss_critic       | 0.555    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1041000
Best mean reward: 94.27 - Last mean reward per episode: 90.26
Num timesteps: 1042000
Best mean reward: 94.27 - Last mean reward per episode: 90.29
Num timesteps: 1043000
Best mean reward: 94.27 - Last mean reward per episode: 90.39
Num timesteps: 1044000
Best mean reward: 94.27 - Last mean reward per episode: 90.38
Num timesteps: 1045000
Best mean reward: 94.27 - Last mean reward per episode: 90.34
Num timesteps: 1046000
Best mean reward: 94.27 - Last mean reward per episode: 90.29
Num timesteps: 1047000
Best mean reward: 94.27 - Last mean reward per episode: 88.80
Num timesteps: 1048000
Best mean reward: 94.27 - Last mean reward per episode: 90.09
Num timesteps: 1049000
Best mean reward: 94.27 - Last mean reward per episode: 89.91
Num timesteps: 1050000
Best mean reward: 94.27 - Last mean reward per episode: 89.77
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 5.65     |
| reference_action_mean   | -0.0765  |
| reference_action_std    | 0.972    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 5.63     |
| rollout/Q_mean          | 62       |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 9.96e+03 |
| rollout/return          | 91.4     |
| rollout/return_history  | 89.8     |
| total/duration          | 2.61e+03 |
| total/episodes          | 9.96e+03 |
| total/epochs            | 1        |
| total/steps             | 1049998  |
| total/steps_per_second  | 402      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 0.595    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1051000
Best mean reward: 94.27 - Last mean reward per episode: 91.37
Num timesteps: 1052000
Best mean reward: 94.27 - Last mean reward per episode: 91.29
Num timesteps: 1053000
Best mean reward: 94.27 - Last mean reward per episode: 91.20
Num timesteps: 1054000
Best mean reward: 94.27 - Last mean reward per episode: 91.25
Num timesteps: 1055000
Best mean reward: 94.27 - Last mean reward per episode: 91.22
Num timesteps: 1056000
Best mean reward: 94.27 - Last mean reward per episode: 91.17
Num timesteps: 1057000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 1058000
Best mean reward: 94.27 - Last mean reward per episode: 92.69
Num timesteps: 1059000
Best mean reward: 94.27 - Last mean reward per episode: 92.57
Num timesteps: 1060000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 6.29     |
| reference_action_mean   | -0.226   |
| reference_action_std    | 0.939    |
| reference_actor_Q_mean  | 49.8     |
| reference_actor_Q_std   | 6.28     |
| rollout/Q_mean          | 62       |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.01e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 92.7     |
| total/duration          | 2.64e+03 |
| total/episodes          | 1.01e+04 |
| total/epochs            | 1        |
| total/steps             | 1059998  |
| total/steps_per_second  | 402      |
| train/loss_actor        | -67.1    |
| train/loss_critic       | 0.557    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1061000
Best mean reward: 94.27 - Last mean reward per episode: 92.65
Num timesteps: 1062000
Best mean reward: 94.27 - Last mean reward per episode: 91.15
Num timesteps: 1063000
Best mean reward: 94.27 - Last mean reward per episode: 91.20
Num timesteps: 1064000
Best mean reward: 94.27 - Last mean reward per episode: 91.22
Num timesteps: 1065000
Best mean reward: 94.27 - Last mean reward per episode: 91.17
Num timesteps: 1066000
Best mean reward: 94.27 - Last mean reward per episode: 89.52
Num timesteps: 1067000
Best mean reward: 94.27 - Last mean reward per episode: 89.64
Num timesteps: 1068000
Best mean reward: 94.27 - Last mean reward per episode: 89.74
Num timesteps: 1069000
Best mean reward: 94.27 - Last mean reward per episode: 89.76
Num timesteps: 1070000
Best mean reward: 94.27 - Last mean reward per episode: 89.75
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 6.51     |
| reference_action_mean   | -0.419   |
| reference_action_std    | 0.882    |
| reference_actor_Q_mean  | 50       |
| reference_actor_Q_std   | 6.51     |
| rollout/Q_mean          | 62.1     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.01e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 89.7     |
| total/duration          | 2.66e+03 |
| total/episodes          | 1.01e+04 |
| total/epochs            | 1        |
| total/steps             | 1069998  |
| total/steps_per_second  | 402      |
| train/loss_actor        | -67.2    |
| train/loss_critic       | 0.48     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1071000
Best mean reward: 94.27 - Last mean reward per episode: 89.62
Num timesteps: 1072000
Best mean reward: 94.27 - Last mean reward per episode: 89.52
Num timesteps: 1073000
Best mean reward: 94.27 - Last mean reward per episode: 89.40
Num timesteps: 1074000
Best mean reward: 94.27 - Last mean reward per episode: 90.78
Num timesteps: 1075000
Best mean reward: 94.27 - Last mean reward per episode: 90.75
Num timesteps: 1076000
Best mean reward: 94.27 - Last mean reward per episode: 88.94
Num timesteps: 1077000
Best mean reward: 94.27 - Last mean reward per episode: 88.88
Num timesteps: 1078000
Best mean reward: 94.27 - Last mean reward per episode: 88.91
Num timesteps: 1079000
Best mean reward: 94.27 - Last mean reward per episode: 90.67
Num timesteps: 1080000
Best mean reward: 94.27 - Last mean reward per episode: 90.67
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 6.68     |
| reference_action_mean   | -0.539   |
| reference_action_std    | 0.823    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 6.64     |
| rollout/Q_mean          | 62.1     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 106      |
| rollout/episodes        | 1.02e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 90.7     |
| total/duration          | 2.69e+03 |
| total/episodes          | 1.02e+04 |
| total/epochs            | 1        |
| total/steps             | 1079998  |
| total/steps_per_second  | 402      |
| train/loss_actor        | -67.6    |
| train/loss_critic       | 1.83     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1081000
Best mean reward: 94.27 - Last mean reward per episode: 90.83
Num timesteps: 1082000
Best mean reward: 94.27 - Last mean reward per episode: 91.18
Num timesteps: 1083000
Best mean reward: 94.27 - Last mean reward per episode: 91.56
Num timesteps: 1084000
Best mean reward: 94.27 - Last mean reward per episode: 91.35
Num timesteps: 1085000
Best mean reward: 94.27 - Last mean reward per episode: 93.20
Num timesteps: 1086000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 1087000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
Num timesteps: 1088000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 1089000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 1090000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 6.45     |
| reference_action_mean   | -0.692   |
| reference_action_std    | 0.705    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 6.44     |
| rollout/Q_mean          | 62.2     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.825    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.03e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.71e+03 |
| total/episodes          | 1.03e+04 |
| total/epochs            | 1        |
| total/steps             | 1089998  |
| total/steps_per_second  | 402      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 1.21     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1091000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 1092000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 1093000
Best mean reward: 94.27 - Last mean reward per episode: 93.77
Num timesteps: 1094000
Best mean reward: 94.27 - Last mean reward per episode: 93.14
Num timesteps: 1095000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 1096000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 1097000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 1098000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 1099000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 1100000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 6.66     |
| reference_action_mean   | -0.522   |
| reference_action_std    | 0.838    |
| reference_actor_Q_mean  | 49.9     |
| reference_actor_Q_std   | 6.64     |
| rollout/Q_mean          | 62.3     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.826    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.05e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.3     |
| total/duration          | 2.74e+03 |
| total/episodes          | 1.05e+04 |
| total/epochs            | 1        |
| total/steps             | 1099998  |
| total/steps_per_second  | 401      |
| train/loss_actor        | -68.4    |
| train/loss_critic       | 0.689    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1101000
Best mean reward: 94.27 - Last mean reward per episode: 93.97
Num timesteps: 1102000
Best mean reward: 94.27 - Last mean reward per episode: 93.86
Num timesteps: 1103000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 1104000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 1105000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 1106000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 1107000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 1108000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 1109000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1110000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 6.72     |
| reference_action_mean   | -0.581   |
| reference_action_std    | 0.803    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 6.64     |
| rollout/Q_mean          | 62.4     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.826    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.06e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.7     |
| total/duration          | 2.77e+03 |
| total/episodes          | 1.06e+04 |
| total/epochs            | 1        |
| total/steps             | 1109998  |
| total/steps_per_second  | 401      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 0.81     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1111000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1112000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 1113000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1114000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1115000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 1116000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 1117000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 1118000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 1119000
Best mean reward: 94.27 - Last mean reward per episode: 93.16
Num timesteps: 1120000
Best mean reward: 94.27 - Last mean reward per episode: 92.58
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 6.67     |
| reference_action_mean   | -0.773   |
| reference_action_std    | 0.613    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 6.62     |
| rollout/Q_mean          | 62.4     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.826    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.07e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 92.6     |
| total/duration          | 2.79e+03 |
| total/episodes          | 1.07e+04 |
| total/epochs            | 1        |
| total/steps             | 1119998  |
| total/steps_per_second  | 401      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.795    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1121000
Best mean reward: 94.27 - Last mean reward per episode: 92.38
Num timesteps: 1122000
Best mean reward: 94.27 - Last mean reward per episode: 92.45
Num timesteps: 1123000
Best mean reward: 94.27 - Last mean reward per episode: 92.41
Num timesteps: 1124000
Best mean reward: 94.27 - Last mean reward per episode: 92.28
Num timesteps: 1125000
Best mean reward: 94.27 - Last mean reward per episode: 92.38
Num timesteps: 1126000
Best mean reward: 94.27 - Last mean reward per episode: 92.38
Num timesteps: 1127000
Best mean reward: 94.27 - Last mean reward per episode: 92.57
Num timesteps: 1128000
Best mean reward: 94.27 - Last mean reward per episode: 92.44
Num timesteps: 1129000
Best mean reward: 94.27 - Last mean reward per episode: 92.58
Num timesteps: 1130000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 6.19     |
| reference_action_mean   | -0.811   |
| reference_action_std    | 0.566    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 6.11     |
| rollout/Q_mean          | 62.5     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.826    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.08e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.6     |
| total/duration          | 2.82e+03 |
| total/episodes          | 1.08e+04 |
| total/epochs            | 1        |
| total/steps             | 1129998  |
| total/steps_per_second  | 401      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 1        |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1131000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 1132000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 1133000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 1134000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 1135000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 1136000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
Num timesteps: 1137000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 1138000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 1139000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1140000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 6.19     |
| reference_action_mean   | -0.684   |
| reference_action_std    | 0.713    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 6.03     |
| rollout/Q_mean          | 62.6     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.827    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.09e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.1     |
| total/duration          | 2.85e+03 |
| total/episodes          | 1.09e+04 |
| total/epochs            | 1        |
| total/steps             | 1139998  |
| total/steps_per_second  | 401      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.606    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1141000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 1142000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1143000
Best mean reward: 94.27 - Last mean reward per episode: 91.42
Num timesteps: 1144000
Best mean reward: 94.27 - Last mean reward per episode: 91.39
Num timesteps: 1145000
Best mean reward: 94.27 - Last mean reward per episode: 91.40
Num timesteps: 1146000
Best mean reward: 94.27 - Last mean reward per episode: 91.55
Num timesteps: 1147000
Best mean reward: 94.27 - Last mean reward per episode: 91.51
Num timesteps: 1148000
Best mean reward: 94.27 - Last mean reward per episode: 91.99
Num timesteps: 1149000
Best mean reward: 94.27 - Last mean reward per episode: 91.96
Num timesteps: 1150000
Best mean reward: 94.27 - Last mean reward per episode: 91.95
--------------------------------------
| reference_Q_mean        | 49.2     |
| reference_Q_std         | 6.24     |
| reference_action_mean   | -0.702   |
| reference_action_std    | 0.696    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 5.98     |
| rollout/Q_mean          | 62.6     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.827    |
| rollout/episode_steps   | 105      |
| rollout/episodes        | 1.1e+04  |
| rollout/return          | 91.5     |
| rollout/return_history  | 91.9     |
| total/duration          | 2.87e+03 |
| total/episodes          | 1.1e+04  |
| total/epochs            | 1        |
| total/steps             | 1149998  |
| total/steps_per_second  | 401      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.401    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1151000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1152000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 1153000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 1154000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 1155000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 1156000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 1157000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 1158000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 1159000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 1160000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 6.15     |
| reference_action_mean   | -0.732   |
| reference_action_std    | 0.677    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 5.91     |
| rollout/Q_mean          | 62.7     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.828    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.11e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.4     |
| total/duration          | 2.9e+03  |
| total/episodes          | 1.11e+04 |
| total/epochs            | 1        |
| total/steps             | 1159998  |
| total/steps_per_second  | 400      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.378    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1161000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 1162000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 1163000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 1164000
Best mean reward: 94.27 - Last mean reward per episode: 93.84
Num timesteps: 1165000
Best mean reward: 94.27 - Last mean reward per episode: 93.80
Num timesteps: 1166000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 1167000
Best mean reward: 94.27 - Last mean reward per episode: 93.83
Num timesteps: 1168000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 1169000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 1170000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 6.13     |
| reference_action_mean   | -0.738   |
| reference_action_std    | 0.661    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 5.97     |
| rollout/Q_mean          | 62.8     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.828    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.12e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.7     |
| total/duration          | 2.92e+03 |
| total/episodes          | 1.12e+04 |
| total/epochs            | 1        |
| total/steps             | 1169998  |
| total/steps_per_second  | 400      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.526    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1171000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 1172000
Best mean reward: 94.27 - Last mean reward per episode: 91.89
Num timesteps: 1173000
Best mean reward: 94.27 - Last mean reward per episode: 91.85
Num timesteps: 1174000
Best mean reward: 94.27 - Last mean reward per episode: 92.00
Num timesteps: 1175000
Best mean reward: 94.27 - Last mean reward per episode: 91.98
Num timesteps: 1176000
Best mean reward: 94.27 - Last mean reward per episode: 92.00
Num timesteps: 1177000
Best mean reward: 94.27 - Last mean reward per episode: 91.44
Num timesteps: 1178000
Best mean reward: 94.27 - Last mean reward per episode: 91.45
Num timesteps: 1179000
Best mean reward: 94.27 - Last mean reward per episode: 91.43
Num timesteps: 1180000
Best mean reward: 94.27 - Last mean reward per episode: 91.53
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 6.18     |
| reference_action_mean   | -0.761   |
| reference_action_std    | 0.637    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 6.03     |
| rollout/Q_mean          | 62.8     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.828    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.13e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 91.5     |
| total/duration          | 2.95e+03 |
| total/episodes          | 1.13e+04 |
| total/epochs            | 1        |
| total/steps             | 1179998  |
| total/steps_per_second  | 400      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.349    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1181000
Best mean reward: 94.27 - Last mean reward per episode: 90.09
Num timesteps: 1182000
Best mean reward: 94.27 - Last mean reward per episode: 91.72
Num timesteps: 1183000
Best mean reward: 94.27 - Last mean reward per episode: 91.83
Num timesteps: 1184000
Best mean reward: 94.27 - Last mean reward per episode: 91.80
Num timesteps: 1185000
Best mean reward: 94.27 - Last mean reward per episode: 91.76
Num timesteps: 1186000
Best mean reward: 94.27 - Last mean reward per episode: 91.75
Num timesteps: 1187000
Best mean reward: 94.27 - Last mean reward per episode: 90.85
Num timesteps: 1188000
Best mean reward: 94.27 - Last mean reward per episode: 90.79
Num timesteps: 1189000
Best mean reward: 94.27 - Last mean reward per episode: 90.78
Num timesteps: 1190000
Best mean reward: 94.27 - Last mean reward per episode: 92.17
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 6.29     |
| reference_action_mean   | -0.843   |
| reference_action_std    | 0.523    |
| reference_actor_Q_mean  | 50.2     |
| reference_actor_Q_std   | 6.18     |
| rollout/Q_mean          | 62.9     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.828    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.14e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 92.2     |
| total/duration          | 2.98e+03 |
| total/episodes          | 1.14e+04 |
| total/epochs            | 1        |
| total/steps             | 1189998  |
| total/steps_per_second  | 400      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 1.19     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1191000
Best mean reward: 94.27 - Last mean reward per episode: 92.17
Num timesteps: 1192000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
Num timesteps: 1193000
Best mean reward: 94.27 - Last mean reward per episode: 92.04
Num timesteps: 1194000
Best mean reward: 94.27 - Last mean reward per episode: 92.10
Num timesteps: 1195000
Best mean reward: 94.27 - Last mean reward per episode: 91.99
Num timesteps: 1196000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 1197000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 1198000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 1199000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 1200000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 6.3      |
| reference_action_mean   | -0.823   |
| reference_action_std    | 0.555    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 6.14     |
| rollout/Q_mean          | 62.9     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.828    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.15e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.4     |
| total/duration          | 3e+03    |
| total/episodes          | 1.15e+04 |
| total/epochs            | 1        |
| total/steps             | 1199998  |
| total/steps_per_second  | 399      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.829    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1201000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 1202000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 1203000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 1204000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1205000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 1206000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 1207000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 1208000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 1209000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 1210000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
--------------------------------------
| reference_Q_mean        | 49.2     |
| reference_Q_std         | 6.33     |
| reference_action_mean   | -0.845   |
| reference_action_std    | 0.524    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 6.05     |
| rollout/Q_mean          | 63       |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.828    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.17e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.03e+03 |
| total/episodes          | 1.17e+04 |
| total/epochs            | 1        |
| total/steps             | 1209998  |
| total/steps_per_second  | 399      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.347    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1211000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 1212000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 1213000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 1214000
Best mean reward: 94.27 - Last mean reward per episode: 91.72
Num timesteps: 1215000
Best mean reward: 94.27 - Last mean reward per episode: 91.71
Num timesteps: 1216000
Best mean reward: 94.27 - Last mean reward per episode: 91.67
Num timesteps: 1217000
Best mean reward: 94.27 - Last mean reward per episode: 91.68
Num timesteps: 1218000
Best mean reward: 94.27 - Last mean reward per episode: 91.76
Num timesteps: 1219000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 1220000
Best mean reward: 94.27 - Last mean reward per episode: 91.87
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 6.54     |
| reference_action_mean   | -0.728   |
| reference_action_std    | 0.678    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 6.16     |
| rollout/Q_mean          | 63.1     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.18e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 91.9     |
| total/duration          | 3.06e+03 |
| total/episodes          | 1.18e+04 |
| total/epochs            | 1        |
| total/steps             | 1219998  |
| total/steps_per_second  | 399      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.776    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1221000
Best mean reward: 94.27 - Last mean reward per episode: 92.11
Num timesteps: 1222000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 1223000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 1224000
Best mean reward: 94.27 - Last mean reward per episode: 92.17
Num timesteps: 1225000
Best mean reward: 94.27 - Last mean reward per episode: 92.16
Num timesteps: 1226000
Best mean reward: 94.27 - Last mean reward per episode: 92.00
Num timesteps: 1227000
Best mean reward: 94.27 - Last mean reward per episode: 92.03
Num timesteps: 1228000
Best mean reward: 94.27 - Last mean reward per episode: 91.85
Num timesteps: 1229000
Best mean reward: 94.27 - Last mean reward per episode: 91.83
Num timesteps: 1230000
Best mean reward: 94.27 - Last mean reward per episode: 90.21
--------------------------------------
| reference_Q_mean        | 49.8     |
| reference_Q_std         | 6.81     |
| reference_action_mean   | -0.687   |
| reference_action_std    | 0.705    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 6.39     |
| rollout/Q_mean          | 63.1     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.19e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 90.2     |
| total/duration          | 3.08e+03 |
| total/episodes          | 1.19e+04 |
| total/epochs            | 1        |
| total/steps             | 1229998  |
| total/steps_per_second  | 399      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.306    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1231000
Best mean reward: 94.27 - Last mean reward per episode: 90.18
Num timesteps: 1232000
Best mean reward: 94.27 - Last mean reward per episode: 90.15
Num timesteps: 1233000
Best mean reward: 94.27 - Last mean reward per episode: 90.19
Num timesteps: 1234000
Best mean reward: 94.27 - Last mean reward per episode: 91.78
Num timesteps: 1235000
Best mean reward: 94.27 - Last mean reward per episode: 91.82
Num timesteps: 1236000
Best mean reward: 94.27 - Last mean reward per episode: 91.68
Num timesteps: 1237000
Best mean reward: 94.27 - Last mean reward per episode: 91.74
Num timesteps: 1238000
Best mean reward: 94.27 - Last mean reward per episode: 91.72
Num timesteps: 1239000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 1240000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 6.59     |
| reference_action_mean   | -0.736   |
| reference_action_std    | 0.674    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 6.13     |
| rollout/Q_mean          | 63.2     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 104      |
| rollout/episodes        | 1.2e+04  |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.11e+03 |
| total/episodes          | 1.2e+04  |
| total/epochs            | 1        |
| total/steps             | 1239998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.816    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1241000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1242000
Best mean reward: 94.27 - Last mean reward per episode: 93.02
Num timesteps: 1243000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 1244000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 1245000
Best mean reward: 94.27 - Last mean reward per episode: 92.95
Num timesteps: 1246000
Best mean reward: 94.27 - Last mean reward per episode: 93.14
Num timesteps: 1247000
Best mean reward: 94.27 - Last mean reward per episode: 93.12
Num timesteps: 1248000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1249000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 1250000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 6.28     |
| reference_action_mean   | -0.736   |
| reference_action_std    | 0.674    |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 5.86     |
| rollout/Q_mean          | 63.2     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.21e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.4     |
| total/duration          | 3.14e+03 |
| total/episodes          | 1.21e+04 |
| total/epochs            | 1        |
| total/steps             | 1249998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.827    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1251000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 1252000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 1253000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 1254000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1255000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1256000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1257000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1258000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 1259000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 1260000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 6.03     |
| reference_action_mean   | -0.737   |
| reference_action_std    | 0.664    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 5.69     |
| rollout/Q_mean          | 63.3     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.22e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.5     |
| total/duration          | 3.17e+03 |
| total/episodes          | 1.22e+04 |
| total/epochs            | 1        |
| total/steps             | 1259998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -70      |
| train/loss_critic       | 1.39     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1261000
Best mean reward: 94.27 - Last mean reward per episode: 91.87
Num timesteps: 1262000
Best mean reward: 94.27 - Last mean reward per episode: 91.96
Num timesteps: 1263000
Best mean reward: 94.27 - Last mean reward per episode: 91.98
Num timesteps: 1264000
Best mean reward: 94.27 - Last mean reward per episode: 91.95
Num timesteps: 1265000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
Num timesteps: 1266000
Best mean reward: 94.27 - Last mean reward per episode: 91.98
Num timesteps: 1267000
Best mean reward: 94.27 - Last mean reward per episode: 92.05
Num timesteps: 1268000
Best mean reward: 94.27 - Last mean reward per episode: 90.43
Num timesteps: 1269000
Best mean reward: 94.27 - Last mean reward per episode: 90.44
Num timesteps: 1270000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 6.03     |
| reference_action_mean   | -0.718   |
| reference_action_std    | 0.694    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 5.83     |
| rollout/Q_mean          | 63.3     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.23e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 92       |
| total/duration          | 3.19e+03 |
| total/episodes          | 1.23e+04 |
| total/epochs            | 1        |
| total/steps             | 1269998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.419    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1271000
Best mean reward: 94.27 - Last mean reward per episode: 91.92
Num timesteps: 1272000
Best mean reward: 94.27 - Last mean reward per episode: 91.90
Num timesteps: 1273000
Best mean reward: 94.27 - Last mean reward per episode: 91.89
Num timesteps: 1274000
Best mean reward: 94.27 - Last mean reward per episode: 91.73
Num timesteps: 1275000
Best mean reward: 94.27 - Last mean reward per episode: 91.75
Num timesteps: 1276000
Best mean reward: 94.27 - Last mean reward per episode: 91.73
Num timesteps: 1277000
Best mean reward: 94.27 - Last mean reward per episode: 91.24
Num timesteps: 1278000
Best mean reward: 94.27 - Last mean reward per episode: 91.22
Num timesteps: 1279000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
Num timesteps: 1280000
Best mean reward: 94.27 - Last mean reward per episode: 92.95
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 5.94     |
| reference_action_mean   | -0.687   |
| reference_action_std    | 0.714    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 5.89     |
| rollout/Q_mean          | 63.4     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.24e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 92.9     |
| total/duration          | 3.22e+03 |
| total/episodes          | 1.24e+04 |
| total/epochs            | 1        |
| total/steps             | 1279998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.36     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1281000
Best mean reward: 94.27 - Last mean reward per episode: 92.61
Num timesteps: 1282000
Best mean reward: 94.27 - Last mean reward per episode: 92.64
Num timesteps: 1283000
Best mean reward: 94.27 - Last mean reward per episode: 92.63
Num timesteps: 1284000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
Num timesteps: 1285000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
Num timesteps: 1286000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 1287000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 1288000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 1289000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 1290000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 6.15     |
| reference_action_mean   | -0.707   |
| reference_action_std    | 0.69     |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 6.02     |
| rollout/Q_mean          | 63.4     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.25e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.25e+03 |
| total/episodes          | 1.25e+04 |
| total/epochs            | 1        |
| total/steps             | 1289998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.357    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1291000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 1292000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 1293000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 1294000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 1295000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1296000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1297000
Best mean reward: 94.27 - Last mean reward per episode: 92.19
Num timesteps: 1298000
Best mean reward: 94.27 - Last mean reward per episode: 92.10
Num timesteps: 1299000
Best mean reward: 94.27 - Last mean reward per episode: 91.98
Num timesteps: 1300000
Best mean reward: 94.27 - Last mean reward per episode: 91.74
--------------------------------------
| reference_Q_mean        | 49.8     |
| reference_Q_std         | 6.23     |
| reference_action_mean   | -0.722   |
| reference_action_std    | 0.672    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 6.13     |
| rollout/Q_mean          | 63.5     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.26e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 91.7     |
| total/duration          | 3.28e+03 |
| total/episodes          | 1.26e+04 |
| total/epochs            | 1        |
| total/steps             | 1299998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.358    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1301000
Best mean reward: 94.27 - Last mean reward per episode: 91.76
Num timesteps: 1302000
Best mean reward: 94.27 - Last mean reward per episode: 91.73
Num timesteps: 1303000
Best mean reward: 94.27 - Last mean reward per episode: 91.70
Num timesteps: 1304000
Best mean reward: 94.27 - Last mean reward per episode: 91.76
Num timesteps: 1305000
Best mean reward: 94.27 - Last mean reward per episode: 91.81
Num timesteps: 1306000
Best mean reward: 94.27 - Last mean reward per episode: 91.82
Num timesteps: 1307000
Best mean reward: 94.27 - Last mean reward per episode: 91.78
Num timesteps: 1308000
Best mean reward: 94.27 - Last mean reward per episode: 92.23
Num timesteps: 1309000
Best mean reward: 94.27 - Last mean reward per episode: 92.20
Num timesteps: 1310000
Best mean reward: 94.27 - Last mean reward per episode: 92.17
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 6.42     |
| reference_action_mean   | -0.703   |
| reference_action_std    | 0.705    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 6.22     |
| rollout/Q_mean          | 63.6     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.27e+04 |
| rollout/return          | 91.6     |
| rollout/return_history  | 92.2     |
| total/duration          | 3.3e+03  |
| total/episodes          | 1.27e+04 |
| total/epochs            | 1        |
| total/steps             | 1309998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.365    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1311000
Best mean reward: 94.27 - Last mean reward per episode: 92.20
Num timesteps: 1312000
Best mean reward: 94.27 - Last mean reward per episode: 92.14
Num timesteps: 1313000
Best mean reward: 94.27 - Last mean reward per episode: 92.07
Num timesteps: 1314000
Best mean reward: 94.27 - Last mean reward per episode: 92.02
Num timesteps: 1315000
Best mean reward: 94.27 - Last mean reward per episode: 91.93
Num timesteps: 1316000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 1317000
Best mean reward: 94.27 - Last mean reward per episode: 93.29
Num timesteps: 1318000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 1319000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 1320000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 6.61     |
| reference_action_mean   | -0.753   |
| reference_action_std    | 0.652    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 6.36     |
| rollout/Q_mean          | 63.6     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.28e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93.2     |
| total/duration          | 3.33e+03 |
| total/episodes          | 1.28e+04 |
| total/epochs            | 1        |
| total/steps             | 1319998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.446    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1321000
Best mean reward: 94.27 - Last mean reward per episode: 93.16
Num timesteps: 1322000
Best mean reward: 94.27 - Last mean reward per episode: 93.14
Num timesteps: 1323000
Best mean reward: 94.27 - Last mean reward per episode: 93.17
Num timesteps: 1324000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 1325000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 1326000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 1327000
Best mean reward: 94.27 - Last mean reward per episode: 93.15
Num timesteps: 1328000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1329000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 1330000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 6.83     |
| reference_action_mean   | -0.717   |
| reference_action_std    | 0.694    |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 6.56     |
| rollout/Q_mean          | 63.7     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.83     |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.29e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93.1     |
| total/duration          | 3.36e+03 |
| total/episodes          | 1.29e+04 |
| total/epochs            | 1        |
| total/steps             | 1329998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.563    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1331000
Best mean reward: 94.27 - Last mean reward per episode: 93.11
Num timesteps: 1332000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 1333000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 1334000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 1335000
Best mean reward: 94.27 - Last mean reward per episode: 92.48
Num timesteps: 1336000
Best mean reward: 94.27 - Last mean reward per episode: 92.85
Num timesteps: 1337000
Best mean reward: 94.27 - Last mean reward per episode: 92.83
Num timesteps: 1338000
Best mean reward: 94.27 - Last mean reward per episode: 92.81
Num timesteps: 1339000
Best mean reward: 94.27 - Last mean reward per episode: 92.85
Num timesteps: 1340000
Best mean reward: 94.27 - Last mean reward per episode: 92.81
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 6.6      |
| reference_action_mean   | -0.74    |
| reference_action_std    | 0.664    |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 6.39     |
| rollout/Q_mean          | 63.7     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.831    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.31e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 92.8     |
| total/duration          | 3.38e+03 |
| total/episodes          | 1.31e+04 |
| total/epochs            | 1        |
| total/steps             | 1339998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.969    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1341000
Best mean reward: 94.27 - Last mean reward per episode: 92.89
Num timesteps: 1342000
Best mean reward: 94.27 - Last mean reward per episode: 92.87
Num timesteps: 1343000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1344000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 1345000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 1346000
Best mean reward: 94.27 - Last mean reward per episode: 92.98
Num timesteps: 1347000
Best mean reward: 94.27 - Last mean reward per episode: 92.92
Num timesteps: 1348000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 1349000
Best mean reward: 94.27 - Last mean reward per episode: 93.04
Num timesteps: 1350000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 6.56     |
| reference_action_mean   | -0.821   |
| reference_action_std    | 0.56     |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 6.28     |
| rollout/Q_mean          | 63.8     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.831    |
| rollout/episode_steps   | 103      |
| rollout/episodes        | 1.32e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93       |
| total/duration          | 3.41e+03 |
| total/episodes          | 1.32e+04 |
| total/epochs            | 1        |
| total/steps             | 1349998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.404    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1351000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 1352000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 1353000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1354000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 1355000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 1356000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 1357000
Best mean reward: 94.27 - Last mean reward per episode: 92.46
Num timesteps: 1358000
Best mean reward: 94.27 - Last mean reward per episode: 92.52
Num timesteps: 1359000
Best mean reward: 94.27 - Last mean reward per episode: 92.61
Num timesteps: 1360000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 6.33     |
| reference_action_mean   | -0.906   |
| reference_action_std    | 0.423    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 6.12     |
| rollout/Q_mean          | 63.8     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.831    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.33e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 92.8     |
| total/duration          | 3.43e+03 |
| total/episodes          | 1.33e+04 |
| total/epochs            | 1        |
| total/steps             | 1359998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.425    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1361000
Best mean reward: 94.27 - Last mean reward per episode: 92.87
Num timesteps: 1362000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 1363000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 1364000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 1365000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 1366000
Best mean reward: 94.27 - Last mean reward per episode: 93.77
Num timesteps: 1367000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 1368000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1369000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 1370000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 6.16     |
| reference_action_mean   | -0.816   |
| reference_action_std    | 0.555    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 5.97     |
| rollout/Q_mean          | 63.9     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.831    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.34e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.46e+03 |
| total/episodes          | 1.34e+04 |
| total/epochs            | 1        |
| total/steps             | 1369998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.325    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1371000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1372000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 1373000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1374000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 1375000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 1376000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 1377000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 1378000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 1379000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1380000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
--------------------------------------
| reference_Q_mean        | 50.6     |
| reference_Q_std         | 6.16     |
| reference_action_mean   | -0.787   |
| reference_action_std    | 0.607    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 5.9      |
| rollout/Q_mean          | 63.9     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.831    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.35e+04 |
| rollout/return          | 91.7     |
| rollout/return_history  | 93.7     |
| total/duration          | 3.49e+03 |
| total/episodes          | 1.35e+04 |
| total/epochs            | 1        |
| total/steps             | 1379998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.343    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1381000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 1382000
Best mean reward: 94.27 - Last mean reward per episode: 93.91
Num timesteps: 1383000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 1384000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 1385000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 1386000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 1387000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 1388000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 1389000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 1390000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 6.1      |
| reference_action_mean   | -0.761   |
| reference_action_std    | 0.642    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 5.79     |
| rollout/Q_mean          | 64       |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.36e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.52e+03 |
| total/episodes          | 1.36e+04 |
| total/epochs            | 1        |
| total/steps             | 1389998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 0.292    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1391000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 1392000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 1393000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1394000
Best mean reward: 94.27 - Last mean reward per episode: 93.71
Num timesteps: 1395000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 1396000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 1397000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 1398000
Best mean reward: 94.27 - Last mean reward per episode: 93.80
Num timesteps: 1399000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 1400000
Best mean reward: 94.27 - Last mean reward per episode: 93.84
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 6.11     |
| reference_action_mean   | -0.737   |
| reference_action_std    | 0.661    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 5.78     |
| rollout/Q_mean          | 64       |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.38e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.8     |
| total/duration          | 3.54e+03 |
| total/episodes          | 1.38e+04 |
| total/epochs            | 1        |
| total/steps             | 1399998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 0.269    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1401000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 1402000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1403000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 1404000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 1405000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 1406000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 1407000
Best mean reward: 94.27 - Last mean reward per episode: 93.18
Num timesteps: 1408000
Best mean reward: 94.27 - Last mean reward per episode: 93.20
Num timesteps: 1409000
Best mean reward: 94.27 - Last mean reward per episode: 93.15
Num timesteps: 1410000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 5.78     |
| reference_action_mean   | -0.768   |
| reference_action_std    | 0.626    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 5.57     |
| rollout/Q_mean          | 64.1     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.39e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93       |
| total/duration          | 3.57e+03 |
| total/episodes          | 1.39e+04 |
| total/epochs            | 1        |
| total/steps             | 1409998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.307    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1411000
Best mean reward: 94.27 - Last mean reward per episode: 92.99
Num timesteps: 1412000
Best mean reward: 94.27 - Last mean reward per episode: 93.02
Num timesteps: 1413000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1414000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 1415000
Best mean reward: 94.27 - Last mean reward per episode: 93.03
Num timesteps: 1416000
Best mean reward: 94.27 - Last mean reward per episode: 92.98
Num timesteps: 1417000
Best mean reward: 94.27 - Last mean reward per episode: 92.48
Num timesteps: 1418000
Best mean reward: 94.27 - Last mean reward per episode: 92.10
Num timesteps: 1419000
Best mean reward: 94.27 - Last mean reward per episode: 91.95
Num timesteps: 1420000
Best mean reward: 94.27 - Last mean reward per episode: 91.70
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 5.29     |
| reference_action_mean   | -0.795   |
| reference_action_std    | 0.595    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 5.14     |
| rollout/Q_mean          | 64.1     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.4e+04  |
| rollout/return          | 91.8     |
| rollout/return_history  | 91.7     |
| total/duration          | 3.6e+03  |
| total/episodes          | 1.4e+04  |
| total/epochs            | 1        |
| total/steps             | 1419998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.391    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1421000
Best mean reward: 94.27 - Last mean reward per episode: 91.62
Num timesteps: 1422000
Best mean reward: 94.27 - Last mean reward per episode: 91.57
Num timesteps: 1423000
Best mean reward: 94.27 - Last mean reward per episode: 91.52
Num timesteps: 1424000
Best mean reward: 94.27 - Last mean reward per episode: 91.34
Num timesteps: 1425000
Best mean reward: 94.27 - Last mean reward per episode: 90.99
Num timesteps: 1426000
Best mean reward: 94.27 - Last mean reward per episode: 90.74
Num timesteps: 1427000
Best mean reward: 94.27 - Last mean reward per episode: 90.47
Num timesteps: 1428000
Best mean reward: 94.27 - Last mean reward per episode: 90.27
Num timesteps: 1429000
Best mean reward: 94.27 - Last mean reward per episode: 90.15
Num timesteps: 1430000
Best mean reward: 94.27 - Last mean reward per episode: 90.70
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 5.36     |
| reference_action_mean   | -0.703   |
| reference_action_std    | 0.708    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 5.27     |
| rollout/Q_mean          | 64.1     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.832    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.4e+04  |
| rollout/return          | 91.8     |
| rollout/return_history  | 90.7     |
| total/duration          | 3.62e+03 |
| total/episodes          | 1.4e+04  |
| total/epochs            | 1        |
| total/steps             | 1429998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 0.465    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1431000
Best mean reward: 94.27 - Last mean reward per episode: 90.69
Num timesteps: 1432000
Best mean reward: 94.27 - Last mean reward per episode: 90.67
Num timesteps: 1433000
Best mean reward: 94.27 - Last mean reward per episode: 90.64
Num timesteps: 1434000
Best mean reward: 94.27 - Last mean reward per episode: 90.52
Num timesteps: 1435000
Best mean reward: 94.27 - Last mean reward per episode: 90.50
Num timesteps: 1436000
Best mean reward: 94.27 - Last mean reward per episode: 90.33
Num timesteps: 1437000
Best mean reward: 94.27 - Last mean reward per episode: 90.56
Num timesteps: 1438000
Best mean reward: 94.27 - Last mean reward per episode: 90.39
Num timesteps: 1439000
Best mean reward: 94.27 - Last mean reward per episode: 90.57
Num timesteps: 1440000
Best mean reward: 94.27 - Last mean reward per episode: 90.98
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 5.37     |
| reference_action_mean   | -0.828   |
| reference_action_std    | 0.558    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 5.27     |
| rollout/Q_mean          | 64.1     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.41e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 91       |
| total/duration          | 3.65e+03 |
| total/episodes          | 1.41e+04 |
| total/epochs            | 1        |
| total/steps             | 1439998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -66.6    |
| train/loss_critic       | 0.458    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1441000
Best mean reward: 94.27 - Last mean reward per episode: 91.44
Num timesteps: 1442000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 1443000
Best mean reward: 94.27 - Last mean reward per episode: 91.92
Num timesteps: 1444000
Best mean reward: 94.27 - Last mean reward per episode: 92.36
Num timesteps: 1445000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
Num timesteps: 1446000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 1447000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 1448000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 1449000
Best mean reward: 94.27 - Last mean reward per episode: 92.98
Num timesteps: 1450000
Best mean reward: 94.27 - Last mean reward per episode: 93.04
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 5.57     |
| reference_action_mean   | -0.774   |
| reference_action_std    | 0.619    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 5.54     |
| rollout/Q_mean          | 64.1     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.42e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93       |
| total/duration          | 3.68e+03 |
| total/episodes          | 1.42e+04 |
| total/epochs            | 1        |
| total/steps             | 1449998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -66.8    |
| train/loss_critic       | 0.474    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1451000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 1452000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
Num timesteps: 1453000
Best mean reward: 94.27 - Last mean reward per episode: 92.87
Num timesteps: 1454000
Best mean reward: 94.27 - Last mean reward per episode: 92.88
Num timesteps: 1455000
Best mean reward: 94.27 - Last mean reward per episode: 91.49
Num timesteps: 1456000
Best mean reward: 94.27 - Last mean reward per episode: 91.44
Num timesteps: 1457000
Best mean reward: 94.27 - Last mean reward per episode: 91.62
Num timesteps: 1458000
Best mean reward: 94.27 - Last mean reward per episode: 91.48
Num timesteps: 1459000
Best mean reward: 94.27 - Last mean reward per episode: 91.67
Num timesteps: 1460000
Best mean reward: 94.27 - Last mean reward per episode: 91.66
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 6.05     |
| reference_action_mean   | -0.66    |
| reference_action_std    | 0.731    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 5.98     |
| rollout/Q_mean          | 64.2     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.43e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 91.7     |
| total/duration          | 3.71e+03 |
| total/episodes          | 1.43e+04 |
| total/epochs            | 1        |
| total/steps             | 1459998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -66.6    |
| train/loss_critic       | 0.395    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1461000
Best mean reward: 94.27 - Last mean reward per episode: 91.98
Num timesteps: 1462000
Best mean reward: 94.27 - Last mean reward per episode: 91.94
Num timesteps: 1463000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 1464000
Best mean reward: 94.27 - Last mean reward per episode: 93.05
Num timesteps: 1465000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 1466000
Best mean reward: 94.27 - Last mean reward per episode: 93.04
Num timesteps: 1467000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 1468000
Best mean reward: 94.27 - Last mean reward per episode: 93.15
Num timesteps: 1469000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
Num timesteps: 1470000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 6.36     |
| reference_action_mean   | -0.694   |
| reference_action_std    | 0.704    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 6.23     |
| rollout/Q_mean          | 64.2     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.44e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.73e+03 |
| total/episodes          | 1.44e+04 |
| total/epochs            | 1        |
| total/steps             | 1469998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -67.2    |
| train/loss_critic       | 0.376    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1471000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 1472000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 1473000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 1474000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 1475000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 1476000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
Num timesteps: 1477000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 1478000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 1479000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 1480000
Best mean reward: 94.27 - Last mean reward per episode: 93.29
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 6.29     |
| reference_action_mean   | -0.813   |
| reference_action_std    | 0.576    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 6.18     |
| rollout/Q_mean          | 64.2     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.46e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.76e+03 |
| total/episodes          | 1.46e+04 |
| total/epochs            | 1        |
| total/steps             | 1479998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -68.4    |
| train/loss_critic       | 0.506    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1481000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 1482000
Best mean reward: 94.27 - Last mean reward per episode: 92.87
Num timesteps: 1483000
Best mean reward: 94.27 - Last mean reward per episode: 92.92
Num timesteps: 1484000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 1485000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 1486000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 1487000
Best mean reward: 94.27 - Last mean reward per episode: 92.81
Num timesteps: 1488000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 1489000
Best mean reward: 94.27 - Last mean reward per episode: 92.75
Num timesteps: 1490000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 5.7      |
| reference_action_mean   | -0.803   |
| reference_action_std    | 0.578    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 5.68     |
| rollout/Q_mean          | 64.3     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 102      |
| rollout/episodes        | 1.47e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 92.9     |
| total/duration          | 3.79e+03 |
| total/episodes          | 1.47e+04 |
| total/epochs            | 1        |
| total/steps             | 1489998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 0.441    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1491000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 1492000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 1493000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
Num timesteps: 1494000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 1495000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 1496000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
Num timesteps: 1497000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 1498000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 1499000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 1500000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 5.5      |
| reference_action_mean   | -0.753   |
| reference_action_std    | 0.638    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 5.51     |
| rollout/Q_mean          | 64.3     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.833    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.48e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.81e+03 |
| total/episodes          | 1.48e+04 |
| total/epochs            | 1        |
| total/steps             | 1499998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.429    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1501000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 1502000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 1503000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 1504000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 1505000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 1506000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 1507000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 1508000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 1509000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 1510000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
--------------------------------------
| reference_Q_mean        | 52.8     |
| reference_Q_std         | 5.23     |
| reference_action_mean   | -0.375   |
| reference_action_std    | 0.907    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 5.21     |
| rollout/Q_mean          | 64.3     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.49e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.84e+03 |
| total/episodes          | 1.49e+04 |
| total/epochs            | 1        |
| total/steps             | 1509998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.456    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1511000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 1512000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 1513000
Best mean reward: 94.27 - Last mean reward per episode: 92.99
Num timesteps: 1514000
Best mean reward: 94.27 - Last mean reward per episode: 92.78
Num timesteps: 1515000
Best mean reward: 94.27 - Last mean reward per episode: 92.61
Num timesteps: 1516000
Best mean reward: 94.27 - Last mean reward per episode: 92.59
Num timesteps: 1517000
Best mean reward: 94.27 - Last mean reward per episode: 92.50
Num timesteps: 1518000
Best mean reward: 94.27 - Last mean reward per episode: 92.37
Num timesteps: 1519000
Best mean reward: 94.27 - Last mean reward per episode: 92.30
Num timesteps: 1520000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 5.03     |
| reference_action_mean   | -0.219   |
| reference_action_std    | 0.961    |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 5.02     |
| rollout/Q_mean          | 64.4     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.5e+04  |
| rollout/return          | 91.8     |
| rollout/return_history  | 92.1     |
| total/duration          | 3.87e+03 |
| total/episodes          | 1.5e+04  |
| total/epochs            | 1        |
| total/steps             | 1519998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.473    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1521000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
Num timesteps: 1522000
Best mean reward: 94.27 - Last mean reward per episode: 92.03
Num timesteps: 1523000
Best mean reward: 94.27 - Last mean reward per episode: 92.19
Num timesteps: 1524000
Best mean reward: 94.27 - Last mean reward per episode: 92.14
Num timesteps: 1525000
Best mean reward: 94.27 - Last mean reward per episode: 92.22
Num timesteps: 1526000
Best mean reward: 94.27 - Last mean reward per episode: 92.35
Num timesteps: 1527000
Best mean reward: 94.27 - Last mean reward per episode: 92.30
Num timesteps: 1528000
Best mean reward: 94.27 - Last mean reward per episode: 92.27
Num timesteps: 1529000
Best mean reward: 94.27 - Last mean reward per episode: 92.11
Num timesteps: 1530000
Best mean reward: 94.27 - Last mean reward per episode: 92.26
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 5.12     |
| reference_action_mean   | -0.49    |
| reference_action_std    | 0.837    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 5.15     |
| rollout/Q_mean          | 64.4     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.51e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 92.3     |
| total/duration          | 3.89e+03 |
| total/episodes          | 1.51e+04 |
| total/epochs            | 1        |
| total/steps             | 1529998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.539    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1531000
Best mean reward: 94.27 - Last mean reward per episode: 92.37
Num timesteps: 1532000
Best mean reward: 94.27 - Last mean reward per episode: 92.52
Num timesteps: 1533000
Best mean reward: 94.27 - Last mean reward per episode: 92.51
Num timesteps: 1534000
Best mean reward: 94.27 - Last mean reward per episode: 90.89
Num timesteps: 1535000
Best mean reward: 94.27 - Last mean reward per episode: 90.93
Num timesteps: 1536000
Best mean reward: 94.27 - Last mean reward per episode: 89.49
Num timesteps: 1537000
Best mean reward: 94.27 - Last mean reward per episode: 87.87
Num timesteps: 1538000
Best mean reward: 94.27 - Last mean reward per episode: 87.90
Num timesteps: 1539000
Best mean reward: 94.27 - Last mean reward per episode: 87.98
Num timesteps: 1540000
Best mean reward: 94.27 - Last mean reward per episode: 88.16
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 5.59     |
| reference_action_mean   | -0.635   |
| reference_action_std    | 0.738    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 5.65     |
| rollout/Q_mean          | 64.4     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.52e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 88.2     |
| total/duration          | 3.92e+03 |
| total/episodes          | 1.52e+04 |
| total/epochs            | 1        |
| total/steps             | 1539998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -68.6    |
| train/loss_critic       | 1.24     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1541000
Best mean reward: 94.27 - Last mean reward per episode: 86.81
Num timesteps: 1542000
Best mean reward: 94.27 - Last mean reward per episode: 86.82
Num timesteps: 1543000
Best mean reward: 94.27 - Last mean reward per episode: 86.84
Num timesteps: 1544000
Best mean reward: 94.27 - Last mean reward per episode: 86.77
Num timesteps: 1545000
Best mean reward: 94.27 - Last mean reward per episode: 88.53
Num timesteps: 1546000
Best mean reward: 94.27 - Last mean reward per episode: 90.06
Num timesteps: 1547000
Best mean reward: 94.27 - Last mean reward per episode: 91.68
Num timesteps: 1548000
Best mean reward: 94.27 - Last mean reward per episode: 91.65
Num timesteps: 1549000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 1550000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 6.12     |
| reference_action_mean   | -0.829   |
| reference_action_std    | 0.547    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 6.07     |
| rollout/Q_mean          | 64.5     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.53e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.95e+03 |
| total/episodes          | 1.53e+04 |
| total/epochs            | 1        |
| total/steps             | 1549998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -68.1    |
| train/loss_critic       | 0.396    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1551000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 1552000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 1553000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 1554000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 1555000
Best mean reward: 94.27 - Last mean reward per episode: 92.95
Num timesteps: 1556000
Best mean reward: 94.27 - Last mean reward per episode: 92.94
Num timesteps: 1557000
Best mean reward: 94.27 - Last mean reward per episode: 92.93
Num timesteps: 1558000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
Num timesteps: 1559000
Best mean reward: 94.27 - Last mean reward per episode: 91.36
Num timesteps: 1560000
Best mean reward: 94.27 - Last mean reward per episode: 91.24
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 6.01     |
| reference_action_mean   | -0.853   |
| reference_action_std    | 0.496    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 6.09     |
| rollout/Q_mean          | 64.5     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.834    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.54e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 91.2     |
| total/duration          | 3.98e+03 |
| total/episodes          | 1.54e+04 |
| total/epochs            | 1        |
| total/steps             | 1559998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 1.02     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1561000
Best mean reward: 94.27 - Last mean reward per episode: 91.15
Num timesteps: 1562000
Best mean reward: 94.27 - Last mean reward per episode: 91.22
Num timesteps: 1563000
Best mean reward: 94.27 - Last mean reward per episode: 91.18
Num timesteps: 1564000
Best mean reward: 94.27 - Last mean reward per episode: 91.15
Num timesteps: 1565000
Best mean reward: 94.27 - Last mean reward per episode: 91.14
Num timesteps: 1566000
Best mean reward: 94.27 - Last mean reward per episode: 91.24
Num timesteps: 1567000
Best mean reward: 94.27 - Last mean reward per episode: 91.40
Num timesteps: 1568000
Best mean reward: 94.27 - Last mean reward per episode: 92.88
Num timesteps: 1569000
Best mean reward: 94.27 - Last mean reward per episode: 92.85
Num timesteps: 1570000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 5.93     |
| reference_action_mean   | -0.882   |
| reference_action_std    | 0.457    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 6        |
| rollout/Q_mean          | 64.5     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.55e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 92.8     |
| total/duration          | 4e+03    |
| total/episodes          | 1.55e+04 |
| total/epochs            | 1        |
| total/steps             | 1569998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.948    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1571000
Best mean reward: 94.27 - Last mean reward per episode: 92.75
Num timesteps: 1572000
Best mean reward: 94.27 - Last mean reward per episode: 92.78
Num timesteps: 1573000
Best mean reward: 94.27 - Last mean reward per episode: 92.88
Num timesteps: 1574000
Best mean reward: 94.27 - Last mean reward per episode: 93.02
Num timesteps: 1575000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 1576000
Best mean reward: 94.27 - Last mean reward per episode: 91.19
Num timesteps: 1577000
Best mean reward: 94.27 - Last mean reward per episode: 91.15
Num timesteps: 1578000
Best mean reward: 94.27 - Last mean reward per episode: 91.12
Num timesteps: 1579000
Best mean reward: 94.27 - Last mean reward per episode: 91.24
Num timesteps: 1580000
Best mean reward: 94.27 - Last mean reward per episode: 91.23
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 5.85     |
| reference_action_mean   | -0.906   |
| reference_action_std    | 0.406    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 5.94     |
| rollout/Q_mean          | 64.5     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.56e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 91.2     |
| total/duration          | 4.03e+03 |
| total/episodes          | 1.56e+04 |
| total/epochs            | 1        |
| total/steps             | 1579998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.4      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1581000
Best mean reward: 94.27 - Last mean reward per episode: 91.30
Num timesteps: 1582000
Best mean reward: 94.27 - Last mean reward per episode: 91.26
Num timesteps: 1583000
Best mean reward: 94.27 - Last mean reward per episode: 91.28
Num timesteps: 1584000
Best mean reward: 94.27 - Last mean reward per episode: 91.29
Num timesteps: 1585000
Best mean reward: 94.27 - Last mean reward per episode: 93.12
Num timesteps: 1586000
Best mean reward: 94.27 - Last mean reward per episode: 93.04
Num timesteps: 1587000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 1588000
Best mean reward: 94.27 - Last mean reward per episode: 92.76
Num timesteps: 1589000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 1590000
Best mean reward: 94.27 - Last mean reward per episode: 92.81
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 5.75     |
| reference_action_mean   | -0.56    |
| reference_action_std    | 0.812    |
| reference_actor_Q_mean  | 50.2     |
| reference_actor_Q_std   | 5.86     |
| rollout/Q_mean          | 64.6     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.57e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 92.8     |
| total/duration          | 4.06e+03 |
| total/episodes          | 1.57e+04 |
| total/epochs            | 1        |
| total/steps             | 1589998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 0.35     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1591000
Best mean reward: 94.27 - Last mean reward per episode: 92.85
Num timesteps: 1592000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 1593000
Best mean reward: 94.27 - Last mean reward per episode: 92.69
Num timesteps: 1594000
Best mean reward: 94.27 - Last mean reward per episode: 92.71
Num timesteps: 1595000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
Num timesteps: 1596000
Best mean reward: 94.27 - Last mean reward per episode: 92.78
Num timesteps: 1597000
Best mean reward: 94.27 - Last mean reward per episode: 91.03
Num timesteps: 1598000
Best mean reward: 94.27 - Last mean reward per episode: 91.36
Num timesteps: 1599000
Best mean reward: 94.27 - Last mean reward per episode: 91.34
Num timesteps: 1600000
Best mean reward: 94.27 - Last mean reward per episode: 91.31
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 5.86     |
| reference_action_mean   | -0.71    |
| reference_action_std    | 0.689    |
| reference_actor_Q_mean  | 49.7     |
| reference_actor_Q_std   | 5.94     |
| rollout/Q_mean          | 64.6     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.58e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 91.3     |
| total/duration          | 4.08e+03 |
| total/episodes          | 1.58e+04 |
| total/epochs            | 1        |
| total/steps             | 1599998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.432    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1601000
Best mean reward: 94.27 - Last mean reward per episode: 91.33
Num timesteps: 1602000
Best mean reward: 94.27 - Last mean reward per episode: 91.48
Num timesteps: 1603000
Best mean reward: 94.27 - Last mean reward per episode: 91.34
Num timesteps: 1604000
Best mean reward: 94.27 - Last mean reward per episode: 91.28
Num timesteps: 1605000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 1606000
Best mean reward: 94.27 - Last mean reward per episode: 92.99
Num timesteps: 1607000
Best mean reward: 94.27 - Last mean reward per episode: 93.02
Num timesteps: 1608000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
Num timesteps: 1609000
Best mean reward: 94.27 - Last mean reward per episode: 92.87
Num timesteps: 1610000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 6.01     |
| reference_action_mean   | -0.776   |
| reference_action_std    | 0.608    |
| reference_actor_Q_mean  | 49.4     |
| reference_actor_Q_std   | 5.98     |
| rollout/Q_mean          | 64.6     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.59e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 92.8     |
| total/duration          | 4.11e+03 |
| total/episodes          | 1.59e+04 |
| total/epochs            | 1        |
| total/steps             | 1609998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.508    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1611000
Best mean reward: 94.27 - Last mean reward per episode: 92.81
Num timesteps: 1612000
Best mean reward: 94.27 - Last mean reward per episode: 92.88
Num timesteps: 1613000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
Num timesteps: 1614000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 1615000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
Num timesteps: 1616000
Best mean reward: 94.27 - Last mean reward per episode: 92.73
Num timesteps: 1617000
Best mean reward: 94.27 - Last mean reward per episode: 92.60
Num timesteps: 1618000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 1619000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 1620000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
--------------------------------------
| reference_Q_mean        | 48.3     |
| reference_Q_std         | 6.2      |
| reference_action_mean   | -0.799   |
| reference_action_std    | 0.573    |
| reference_actor_Q_mean  | 49.4     |
| reference_actor_Q_std   | 6.1      |
| rollout/Q_mean          | 64.6     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.835    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.6e+04  |
| rollout/return          | 91.8     |
| rollout/return_history  | 92.8     |
| total/duration          | 4.14e+03 |
| total/episodes          | 1.6e+04  |
| total/epochs            | 1        |
| total/steps             | 1619998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 0.546    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1621000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 1622000
Best mean reward: 94.27 - Last mean reward per episode: 92.62
Num timesteps: 1623000
Best mean reward: 94.27 - Last mean reward per episode: 92.76
Num timesteps: 1624000
Best mean reward: 94.27 - Last mean reward per episode: 92.72
Num timesteps: 1625000
Best mean reward: 94.27 - Last mean reward per episode: 92.69
Num timesteps: 1626000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
Num timesteps: 1627000
Best mean reward: 94.27 - Last mean reward per episode: 92.95
Num timesteps: 1628000
Best mean reward: 94.27 - Last mean reward per episode: 92.99
Num timesteps: 1629000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 1630000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 6.23     |
| reference_action_mean   | -0.885   |
| reference_action_std    | 0.445    |
| reference_actor_Q_mean  | 49.7     |
| reference_actor_Q_std   | 6.07     |
| rollout/Q_mean          | 64.7     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.61e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 93.3     |
| total/duration          | 4.17e+03 |
| total/episodes          | 1.61e+04 |
| total/epochs            | 1        |
| total/steps             | 1629998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.536    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1631000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 1632000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 1633000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 1634000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 1635000
Best mean reward: 94.27 - Last mean reward per episode: 93.29
Num timesteps: 1636000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
Num timesteps: 1637000
Best mean reward: 94.27 - Last mean reward per episode: 92.78
Num timesteps: 1638000
Best mean reward: 94.27 - Last mean reward per episode: 92.72
Num timesteps: 1639000
Best mean reward: 94.27 - Last mean reward per episode: 92.19
Num timesteps: 1640000
Best mean reward: 94.27 - Last mean reward per episode: 92.05
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 6        |
| reference_action_mean   | -0.899   |
| reference_action_std    | 0.426    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 5.85     |
| rollout/Q_mean          | 64.7     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.62e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 92       |
| total/duration          | 4.2e+03  |
| total/episodes          | 1.62e+04 |
| total/epochs            | 1        |
| total/steps             | 1639998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 1.24     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1641000
Best mean reward: 94.27 - Last mean reward per episode: 91.93
Num timesteps: 1642000
Best mean reward: 94.27 - Last mean reward per episode: 91.86
Num timesteps: 1643000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 1644000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 1645000
Best mean reward: 94.27 - Last mean reward per episode: 91.80
Num timesteps: 1646000
Best mean reward: 94.27 - Last mean reward per episode: 92.11
Num timesteps: 1647000
Best mean reward: 94.27 - Last mean reward per episode: 92.17
Num timesteps: 1648000
Best mean reward: 94.27 - Last mean reward per episode: 92.63
Num timesteps: 1649000
Best mean reward: 94.27 - Last mean reward per episode: 92.75
Num timesteps: 1650000
Best mean reward: 94.27 - Last mean reward per episode: 92.68
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 5.74     |
| reference_action_mean   | -0.883   |
| reference_action_std    | 0.457    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 5.54     |
| rollout/Q_mean          | 64.7     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.63e+04 |
| rollout/return          | 91.8     |
| rollout/return_history  | 92.7     |
| total/duration          | 4.22e+03 |
| total/episodes          | 1.63e+04 |
| total/epochs            | 1        |
| total/steps             | 1649998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 0.467    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1651000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 1652000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 1653000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 1654000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 1655000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 1656000
Best mean reward: 94.27 - Last mean reward per episode: 92.73
Num timesteps: 1657000
Best mean reward: 94.27 - Last mean reward per episode: 92.79
Num timesteps: 1658000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
Num timesteps: 1659000
Best mean reward: 94.27 - Last mean reward per episode: 92.59
Num timesteps: 1660000
Best mean reward: 94.27 - Last mean reward per episode: 92.45
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 5.59     |
| reference_action_mean   | -0.903   |
| reference_action_std    | 0.413    |
| reference_actor_Q_mean  | 51.2     |
| reference_actor_Q_std   | 5.42     |
| rollout/Q_mean          | 64.7     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.64e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.5     |
| total/duration          | 4.25e+03 |
| total/episodes          | 1.64e+04 |
| total/epochs            | 1        |
| total/steps             | 1659998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -68.5    |
| train/loss_critic       | 0.446    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1661000
Best mean reward: 94.27 - Last mean reward per episode: 92.54
Num timesteps: 1662000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
Num timesteps: 1663000
Best mean reward: 94.27 - Last mean reward per episode: 92.57
Num timesteps: 1664000
Best mean reward: 94.27 - Last mean reward per episode: 92.64
Num timesteps: 1665000
Best mean reward: 94.27 - Last mean reward per episode: 92.75
Num timesteps: 1666000
Best mean reward: 94.27 - Last mean reward per episode: 92.68
Num timesteps: 1667000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
Num timesteps: 1668000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 1669000
Best mean reward: 94.27 - Last mean reward per episode: 92.65
Num timesteps: 1670000
Best mean reward: 94.27 - Last mean reward per episode: 92.64
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 5.58     |
| reference_action_mean   | -0.902   |
| reference_action_std    | 0.421    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 5.44     |
| rollout/Q_mean          | 64.8     |
| rollout/actions_mean    | 0.159    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.65e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.6     |
| total/duration          | 4.28e+03 |
| total/episodes          | 1.65e+04 |
| total/epochs            | 1        |
| total/steps             | 1669998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -68.6    |
| train/loss_critic       | 0.423    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1671000
Best mean reward: 94.27 - Last mean reward per episode: 92.55
Num timesteps: 1672000
Best mean reward: 94.27 - Last mean reward per episode: 92.44
Num timesteps: 1673000
Best mean reward: 94.27 - Last mean reward per episode: 92.50
Num timesteps: 1674000
Best mean reward: 94.27 - Last mean reward per episode: 92.44
Num timesteps: 1675000
Best mean reward: 94.27 - Last mean reward per episode: 92.29
Num timesteps: 1676000
Best mean reward: 94.27 - Last mean reward per episode: 92.19
Num timesteps: 1677000
Best mean reward: 94.27 - Last mean reward per episode: 92.15
Num timesteps: 1678000
Best mean reward: 94.27 - Last mean reward per episode: 92.07
Num timesteps: 1679000
Best mean reward: 94.27 - Last mean reward per episode: 92.47
Num timesteps: 1680000
Best mean reward: 94.27 - Last mean reward per episode: 92.59
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 5.63     |
| reference_action_mean   | -0.91    |
| reference_action_std    | 0.407    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 5.52     |
| rollout/Q_mean          | 64.8     |
| rollout/actions_mean    | 0.159    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.66e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.6     |
| total/duration          | 4.3e+03  |
| total/episodes          | 1.66e+04 |
| total/epochs            | 1        |
| total/steps             | 1679998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 0.523    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1681000
Best mean reward: 94.27 - Last mean reward per episode: 92.63
Num timesteps: 1682000
Best mean reward: 94.27 - Last mean reward per episode: 92.58
Num timesteps: 1683000
Best mean reward: 94.27 - Last mean reward per episode: 92.58
Num timesteps: 1684000
Best mean reward: 94.27 - Last mean reward per episode: 92.63
Num timesteps: 1685000
Best mean reward: 94.27 - Last mean reward per episode: 92.60
Num timesteps: 1686000
Best mean reward: 94.27 - Last mean reward per episode: 92.66
Num timesteps: 1687000
Best mean reward: 94.27 - Last mean reward per episode: 91.12
Num timesteps: 1688000
Best mean reward: 94.27 - Last mean reward per episode: 91.17
Num timesteps: 1689000
Best mean reward: 94.27 - Last mean reward per episode: 91.17
Num timesteps: 1690000
Best mean reward: 94.27 - Last mean reward per episode: 91.15
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 5.87     |
| reference_action_mean   | -0.863   |
| reference_action_std    | 0.479    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 5.79     |
| rollout/Q_mean          | 64.8     |
| rollout/actions_mean    | 0.16     |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.67e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 91.1     |
| total/duration          | 4.33e+03 |
| total/episodes          | 1.67e+04 |
| total/epochs            | 1        |
| total/steps             | 1689998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 1.48     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1691000
Best mean reward: 94.27 - Last mean reward per episode: 91.13
Num timesteps: 1692000
Best mean reward: 94.27 - Last mean reward per episode: 91.10
Num timesteps: 1693000
Best mean reward: 94.27 - Last mean reward per episode: 91.10
Num timesteps: 1694000
Best mean reward: 94.27 - Last mean reward per episode: 91.16
Num timesteps: 1695000
Best mean reward: 94.27 - Last mean reward per episode: 91.17
Num timesteps: 1696000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 1697000
Best mean reward: 94.27 - Last mean reward per episode: 92.54
Num timesteps: 1698000
Best mean reward: 94.27 - Last mean reward per episode: 92.50
Num timesteps: 1699000
Best mean reward: 94.27 - Last mean reward per episode: 92.51
Num timesteps: 1700000
Best mean reward: 94.27 - Last mean reward per episode: 92.48
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 6.06     |
| reference_action_mean   | -0.832   |
| reference_action_std    | 0.522    |
| reference_actor_Q_mean  | 50       |
| reference_actor_Q_std   | 6.04     |
| rollout/Q_mean          | 64.8     |
| rollout/actions_mean    | 0.16     |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.68e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.5     |
| total/duration          | 4.36e+03 |
| total/episodes          | 1.68e+04 |
| total/epochs            | 1        |
| total/steps             | 1699998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 0.55     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1701000
Best mean reward: 94.27 - Last mean reward per episode: 92.62
Num timesteps: 1702000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
Num timesteps: 1703000
Best mean reward: 94.27 - Last mean reward per episode: 91.26
Num timesteps: 1704000
Best mean reward: 94.27 - Last mean reward per episode: 91.18
Num timesteps: 1705000
Best mean reward: 94.27 - Last mean reward per episode: 91.11
Num timesteps: 1706000
Best mean reward: 94.27 - Last mean reward per episode: 91.08
Num timesteps: 1707000
Best mean reward: 94.27 - Last mean reward per episode: 91.18
Num timesteps: 1708000
Best mean reward: 94.27 - Last mean reward per episode: 91.37
Num timesteps: 1709000
Best mean reward: 94.27 - Last mean reward per episode: 91.17
Num timesteps: 1710000
Best mean reward: 94.27 - Last mean reward per episode: 91.29
--------------------------------------
| reference_Q_mean        | 48.7     |
| reference_Q_std         | 6.27     |
| reference_action_mean   | -0.865   |
| reference_action_std    | 0.485    |
| reference_actor_Q_mean  | 49.6     |
| reference_actor_Q_std   | 6.08     |
| rollout/Q_mean          | 64.9     |
| rollout/actions_mean    | 0.16     |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.69e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 91.3     |
| total/duration          | 4.39e+03 |
| total/episodes          | 1.69e+04 |
| total/epochs            | 1        |
| total/steps             | 1709998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 0.55     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1711000
Best mean reward: 94.27 - Last mean reward per episode: 91.22
Num timesteps: 1712000
Best mean reward: 94.27 - Last mean reward per episode: 92.59
Num timesteps: 1713000
Best mean reward: 94.27 - Last mean reward per episode: 92.59
Num timesteps: 1714000
Best mean reward: 94.27 - Last mean reward per episode: 92.72
Num timesteps: 1715000
Best mean reward: 94.27 - Last mean reward per episode: 92.83
Num timesteps: 1716000
Best mean reward: 94.27 - Last mean reward per episode: 92.69
Num timesteps: 1717000
Best mean reward: 94.27 - Last mean reward per episode: 92.76
Num timesteps: 1718000
Best mean reward: 94.27 - Last mean reward per episode: 92.99
Num timesteps: 1719000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 1720000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
--------------------------------------
| reference_Q_mean        | 49.2     |
| reference_Q_std         | 6.11     |
| reference_action_mean   | -0.937   |
| reference_action_std    | 0.337    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 5.91     |
| rollout/Q_mean          | 64.9     |
| rollout/actions_mean    | 0.161    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.71e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.8     |
| total/duration          | 4.41e+03 |
| total/episodes          | 1.71e+04 |
| total/epochs            | 1        |
| total/steps             | 1719998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 0.569    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1721000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
Num timesteps: 1722000
Best mean reward: 94.27 - Last mean reward per episode: 92.51
Num timesteps: 1723000
Best mean reward: 94.27 - Last mean reward per episode: 90.88
Num timesteps: 1724000
Best mean reward: 94.27 - Last mean reward per episode: 90.86
Num timesteps: 1725000
Best mean reward: 94.27 - Last mean reward per episode: 90.90
Num timesteps: 1726000
Best mean reward: 94.27 - Last mean reward per episode: 90.89
Num timesteps: 1727000
Best mean reward: 94.27 - Last mean reward per episode: 90.95
Num timesteps: 1728000
Best mean reward: 94.27 - Last mean reward per episode: 90.90
Num timesteps: 1729000
Best mean reward: 94.27 - Last mean reward per episode: 90.88
Num timesteps: 1730000
Best mean reward: 94.27 - Last mean reward per episode: 90.90
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 6.3      |
| reference_action_mean   | -0.931   |
| reference_action_std    | 0.348    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 6.02     |
| rollout/Q_mean          | 64.9     |
| rollout/actions_mean    | 0.161    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.72e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 90.9     |
| total/duration          | 4.44e+03 |
| total/episodes          | 1.72e+04 |
| total/epochs            | 1        |
| total/steps             | 1729998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.48     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1731000
Best mean reward: 94.27 - Last mean reward per episode: 90.98
Num timesteps: 1732000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 1733000
Best mean reward: 94.27 - Last mean reward per episode: 92.94
Num timesteps: 1734000
Best mean reward: 94.27 - Last mean reward per episode: 93.04
Num timesteps: 1735000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 1736000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 1737000
Best mean reward: 94.27 - Last mean reward per episode: 92.99
Num timesteps: 1738000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
Num timesteps: 1739000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
Num timesteps: 1740000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 6.13     |
| reference_action_mean   | -0.928   |
| reference_action_std    | 0.356    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 6.03     |
| rollout/Q_mean          | 64.9     |
| rollout/actions_mean    | 0.162    |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.73e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.9     |
| total/duration          | 4.47e+03 |
| total/episodes          | 1.73e+04 |
| total/epochs            | 1        |
| total/steps             | 1739998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.705    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1741000
Best mean reward: 94.27 - Last mean reward per episode: 92.98
Num timesteps: 1742000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 1743000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1744000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 1745000
Best mean reward: 94.27 - Last mean reward per episode: 92.96
Num timesteps: 1746000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 1747000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 1748000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 1749000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 1750000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 6.23     |
| reference_action_mean   | -0.909   |
| reference_action_std    | 0.407    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 6.2      |
| rollout/Q_mean          | 65       |
| rollout/actions_mean    | 0.161    |
| rollout/actions_std     | 0.837    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.74e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.5     |
| total/duration          | 4.5e+03  |
| total/episodes          | 1.74e+04 |
| total/epochs            | 1        |
| total/steps             | 1749998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.19     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1751000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 1752000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 1753000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1754000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 1755000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 1756000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 1757000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
Num timesteps: 1758000
Best mean reward: 94.27 - Last mean reward per episode: 93.29
Num timesteps: 1759000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 1760000
Best mean reward: 94.27 - Last mean reward per episode: 93.16
--------------------------------------
| reference_Q_mean        | 48.8     |
| reference_Q_std         | 6.2      |
| reference_action_mean   | -0.37    |
| reference_action_std    | 0.897    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 6.37     |
| rollout/Q_mean          | 65       |
| rollout/actions_mean    | 0.16     |
| rollout/actions_std     | 0.837    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.75e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.2     |
| total/duration          | 4.52e+03 |
| total/episodes          | 1.75e+04 |
| total/epochs            | 1        |
| total/steps             | 1759998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 1.03     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1761000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 1762000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 1763000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1764000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 1765000
Best mean reward: 94.27 - Last mean reward per episode: 92.82
Num timesteps: 1766000
Best mean reward: 94.27 - Last mean reward per episode: 92.76
Num timesteps: 1767000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
Num timesteps: 1768000
Best mean reward: 94.27 - Last mean reward per episode: 91.13
Num timesteps: 1769000
Best mean reward: 94.27 - Last mean reward per episode: 91.13
Num timesteps: 1770000
Best mean reward: 94.27 - Last mean reward per episode: 91.22
--------------------------------------
| reference_Q_mean        | 47.7     |
| reference_Q_std         | 6.68     |
| reference_action_mean   | -0.213   |
| reference_action_std    | 0.949    |
| reference_actor_Q_mean  | 48.5     |
| reference_actor_Q_std   | 6.9      |
| rollout/Q_mean          | 65       |
| rollout/actions_mean    | 0.159    |
| rollout/actions_std     | 0.837    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.76e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 91.2     |
| total/duration          | 4.55e+03 |
| total/episodes          | 1.76e+04 |
| total/epochs            | 1        |
| total/steps             | 1769998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.4      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1771000
Best mean reward: 94.27 - Last mean reward per episode: 91.23
Num timesteps: 1772000
Best mean reward: 94.27 - Last mean reward per episode: 91.21
Num timesteps: 1773000
Best mean reward: 94.27 - Last mean reward per episode: 91.24
Num timesteps: 1774000
Best mean reward: 94.27 - Last mean reward per episode: 91.49
Num timesteps: 1775000
Best mean reward: 94.27 - Last mean reward per episode: 91.52
Num timesteps: 1776000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 1777000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 1778000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 1779000
Best mean reward: 94.27 - Last mean reward per episode: 92.99
Num timesteps: 1780000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
--------------------------------------
| reference_Q_mean        | 47.8     |
| reference_Q_std         | 6.76     |
| reference_action_mean   | -0.379   |
| reference_action_std    | 0.909    |
| reference_actor_Q_mean  | 48.5     |
| reference_actor_Q_std   | 6.8      |
| rollout/Q_mean          | 65.1     |
| rollout/actions_mean    | 0.159    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.77e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93       |
| total/duration          | 4.58e+03 |
| total/episodes          | 1.77e+04 |
| total/epochs            | 1        |
| total/steps             | 1779998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 1.46     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1781000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 1782000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 1783000
Best mean reward: 94.27 - Last mean reward per episode: 92.43
Num timesteps: 1784000
Best mean reward: 94.27 - Last mean reward per episode: 92.35
Num timesteps: 1785000
Best mean reward: 94.27 - Last mean reward per episode: 92.43
Num timesteps: 1786000
Best mean reward: 94.27 - Last mean reward per episode: 92.39
Num timesteps: 1787000
Best mean reward: 94.27 - Last mean reward per episode: 92.32
Num timesteps: 1788000
Best mean reward: 94.27 - Last mean reward per episode: 92.69
Num timesteps: 1789000
Best mean reward: 94.27 - Last mean reward per episode: 92.59
Num timesteps: 1790000
Best mean reward: 94.27 - Last mean reward per episode: 92.61
--------------------------------------
| reference_Q_mean        | 48.1     |
| reference_Q_std         | 6.81     |
| reference_action_mean   | -0.534   |
| reference_action_std    | 0.823    |
| reference_actor_Q_mean  | 48.7     |
| reference_actor_Q_std   | 6.88     |
| rollout/Q_mean          | 65.1     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.78e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.6     |
| total/duration          | 4.6e+03  |
| total/episodes          | 1.78e+04 |
| total/epochs            | 1        |
| total/steps             | 1789998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 1.24     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1791000
Best mean reward: 94.27 - Last mean reward per episode: 92.75
Num timesteps: 1792000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 1793000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 1794000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 1795000
Best mean reward: 94.27 - Last mean reward per episode: 93.11
Num timesteps: 1796000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 1797000
Best mean reward: 94.27 - Last mean reward per episode: 93.18
Num timesteps: 1798000
Best mean reward: 94.27 - Last mean reward per episode: 93.18
Num timesteps: 1799000
Best mean reward: 94.27 - Last mean reward per episode: 93.14
Num timesteps: 1800000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
--------------------------------------
| reference_Q_mean        | 48.1     |
| reference_Q_std         | 7.15     |
| reference_action_mean   | -0.603   |
| reference_action_std    | 0.788    |
| reference_actor_Q_mean  | 48.8     |
| reference_actor_Q_std   | 7.19     |
| rollout/Q_mean          | 65.1     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 101      |
| rollout/episodes        | 1.79e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.1     |
| total/duration          | 4.63e+03 |
| total/episodes          | 1.79e+04 |
| total/epochs            | 1        |
| total/steps             | 1799998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 1.05     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1801000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 1802000
Best mean reward: 94.27 - Last mean reward per episode: 93.04
Num timesteps: 1803000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 1804000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 1805000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 1806000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1807000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1808000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1809000
Best mean reward: 94.27 - Last mean reward per episode: 93.71
Num timesteps: 1810000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
--------------------------------------
| reference_Q_mean        | 48.9     |
| reference_Q_std         | 7.49     |
| reference_action_mean   | -0.595   |
| reference_action_std    | 0.793    |
| reference_actor_Q_mean  | 49.5     |
| reference_actor_Q_std   | 7.55     |
| rollout/Q_mean          | 65.1     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.8e+04  |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.7     |
| total/duration          | 4.66e+03 |
| total/episodes          | 1.8e+04  |
| total/epochs            | 1        |
| total/steps             | 1809998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.714    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1811000
Best mean reward: 94.27 - Last mean reward per episode: 93.77
Num timesteps: 1812000
Best mean reward: 94.27 - Last mean reward per episode: 93.77
Num timesteps: 1813000
Best mean reward: 94.27 - Last mean reward per episode: 93.80
Num timesteps: 1814000
Best mean reward: 94.27 - Last mean reward per episode: 92.18
Num timesteps: 1815000
Best mean reward: 94.27 - Last mean reward per episode: 92.03
Num timesteps: 1816000
Best mean reward: 94.27 - Last mean reward per episode: 92.15
Num timesteps: 1817000
Best mean reward: 94.27 - Last mean reward per episode: 92.14
Num timesteps: 1818000
Best mean reward: 94.27 - Last mean reward per episode: 92.02
Num timesteps: 1819000
Best mean reward: 94.27 - Last mean reward per episode: 92.06
Num timesteps: 1820000
Best mean reward: 94.27 - Last mean reward per episode: 92.11
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 7.76     |
| reference_action_mean   | -0.554   |
| reference_action_std    | 0.816    |
| reference_actor_Q_mean  | 50       |
| reference_actor_Q_std   | 7.76     |
| rollout/Q_mean          | 65.2     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.81e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.1     |
| total/duration          | 4.68e+03 |
| total/episodes          | 1.81e+04 |
| total/epochs            | 1        |
| total/steps             | 1819998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.763    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1821000
Best mean reward: 94.27 - Last mean reward per episode: 92.16
Num timesteps: 1822000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1823000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 1824000
Best mean reward: 94.27 - Last mean reward per episode: 93.80
Num timesteps: 1825000
Best mean reward: 94.27 - Last mean reward per episode: 93.90
Num timesteps: 1826000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 1827000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 1828000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 1829000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 1830000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 7.99     |
| reference_action_mean   | -0.572   |
| reference_action_std    | 0.808    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 8.09     |
| rollout/Q_mean          | 65.2     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.838    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.83e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 4.71e+03 |
| total/episodes          | 1.83e+04 |
| total/epochs            | 1        |
| total/steps             | 1829998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.73     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1831000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 1832000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 1833000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 1834000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 1835000
Best mean reward: 94.27 - Last mean reward per episode: 93.64
Num timesteps: 1836000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 1837000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1838000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 1839000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 1840000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 8.25     |
| reference_action_mean   | -0.749   |
| reference_action_std    | 0.633    |
| reference_actor_Q_mean  | 50.2     |
| reference_actor_Q_std   | 8.52     |
| rollout/Q_mean          | 65.2     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.84e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.6     |
| total/duration          | 4.74e+03 |
| total/episodes          | 1.84e+04 |
| total/epochs            | 1        |
| total/steps             | 1839998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 1.52     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1841000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 1842000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 1843000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 1844000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1845000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1846000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 1847000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1848000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 1849000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 1850000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
--------------------------------------
| reference_Q_mean        | 48.3     |
| reference_Q_std         | 8.26     |
| reference_action_mean   | -0.835   |
| reference_action_std    | 0.511    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 8.44     |
| rollout/Q_mean          | 65.3     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.85e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 93.7     |
| total/duration          | 4.77e+03 |
| total/episodes          | 1.85e+04 |
| total/epochs            | 1        |
| total/steps             | 1849998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.536    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1851000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 1852000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 1853000
Best mean reward: 94.27 - Last mean reward per episode: 92.22
Num timesteps: 1854000
Best mean reward: 94.27 - Last mean reward per episode: 92.17
Num timesteps: 1855000
Best mean reward: 94.27 - Last mean reward per episode: 92.20
Num timesteps: 1856000
Best mean reward: 94.27 - Last mean reward per episode: 92.20
Num timesteps: 1857000
Best mean reward: 94.27 - Last mean reward per episode: 92.14
Num timesteps: 1858000
Best mean reward: 94.27 - Last mean reward per episode: 92.09
Num timesteps: 1859000
Best mean reward: 94.27 - Last mean reward per episode: 92.14
Num timesteps: 1860000
Best mean reward: 94.27 - Last mean reward per episode: 92.19
--------------------------------------
| reference_Q_mean        | 48.4     |
| reference_Q_std         | 7.74     |
| reference_action_mean   | -0.889   |
| reference_action_std    | 0.431    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 7.8      |
| rollout/Q_mean          | 65.3     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.9     |
| rollout/episodes        | 1.86e+04 |
| rollout/return          | 91.9     |
| rollout/return_history  | 92.2     |
| total/duration          | 4.8e+03  |
| total/episodes          | 1.86e+04 |
| total/epochs            | 1        |
| total/steps             | 1859998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.509    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1861000
Best mean reward: 94.27 - Last mean reward per episode: 92.19
Num timesteps: 1862000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 1863000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 1864000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 1865000
Best mean reward: 94.27 - Last mean reward per episode: 93.71
Num timesteps: 1866000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 1867000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 1868000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1869000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 1870000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
--------------------------------------
| reference_Q_mean        | 48.2     |
| reference_Q_std         | 8.04     |
| reference_action_mean   | -0.833   |
| reference_action_std    | 0.538    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 7.87     |
| rollout/Q_mean          | 65.3     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.9     |
| rollout/episodes        | 1.87e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.6     |
| total/duration          | 4.82e+03 |
| total/episodes          | 1.87e+04 |
| total/epochs            | 1        |
| total/steps             | 1869998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.769    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1871000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 1872000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 1873000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 1874000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 1875000
Best mean reward: 94.27 - Last mean reward per episode: 93.02
Num timesteps: 1876000
Best mean reward: 94.27 - Last mean reward per episode: 92.75
Num timesteps: 1877000
Best mean reward: 94.27 - Last mean reward per episode: 92.70
Num timesteps: 1878000
Best mean reward: 94.27 - Last mean reward per episode: 92.81
Num timesteps: 1879000
Best mean reward: 94.27 - Last mean reward per episode: 92.80
Num timesteps: 1880000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 48.1     |
| reference_Q_std         | 8.24     |
| reference_action_mean   | -0.898   |
| reference_action_std    | 0.43     |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 7.95     |
| rollout/Q_mean          | 65.3     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.9     |
| rollout/episodes        | 1.88e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.1     |
| total/duration          | 4.85e+03 |
| total/episodes          | 1.88e+04 |
| total/epochs            | 1        |
| total/steps             | 1879998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.747    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1881000
Best mean reward: 94.27 - Last mean reward per episode: 93.20
Num timesteps: 1882000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 1883000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 1884000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 1885000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 1886000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 1887000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 1888000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 1889000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 1890000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
--------------------------------------
| reference_Q_mean        | 47.2     |
| reference_Q_std         | 8.33     |
| reference_action_mean   | -0.909   |
| reference_action_std    | 0.41     |
| reference_actor_Q_mean  | 50       |
| reference_actor_Q_std   | 7.79     |
| rollout/Q_mean          | 65.4     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.8     |
| rollout/episodes        | 1.89e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.5     |
| total/duration          | 4.88e+03 |
| total/episodes          | 1.89e+04 |
| total/epochs            | 1        |
| total/steps             | 1889998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -71.1    |
| train/loss_critic       | 0.641    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1891000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 1892000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1893000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 1894000
Best mean reward: 94.27 - Last mean reward per episode: 93.89
Num timesteps: 1895000
Best mean reward: 94.27 - Last mean reward per episode: 93.84
Num timesteps: 1896000
Best mean reward: 94.27 - Last mean reward per episode: 92.27
Num timesteps: 1897000
Best mean reward: 94.27 - Last mean reward per episode: 92.22
Num timesteps: 1898000
Best mean reward: 94.27 - Last mean reward per episode: 92.20
Num timesteps: 1899000
Best mean reward: 94.27 - Last mean reward per episode: 92.28
Num timesteps: 1900000
Best mean reward: 94.27 - Last mean reward per episode: 92.22
--------------------------------------
| reference_Q_mean        | 47.6     |
| reference_Q_std         | 8.21     |
| reference_action_mean   | -0.91    |
| reference_action_std    | 0.409    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 7.61     |
| rollout/Q_mean          | 65.4     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.7     |
| rollout/episodes        | 1.91e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 92.2     |
| total/duration          | 4.91e+03 |
| total/episodes          | 1.91e+04 |
| total/epochs            | 1        |
| total/steps             | 1899998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -71.5    |
| train/loss_critic       | 0.681    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1901000
Best mean reward: 94.27 - Last mean reward per episode: 92.19
Num timesteps: 1902000
Best mean reward: 94.27 - Last mean reward per episode: 92.22
Num timesteps: 1903000
Best mean reward: 94.27 - Last mean reward per episode: 92.13
Num timesteps: 1904000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 1905000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 1906000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 1907000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 1908000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1909000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 1910000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
--------------------------------------
| reference_Q_mean        | 47.9     |
| reference_Q_std         | 7.87     |
| reference_action_mean   | -0.924   |
| reference_action_std    | 0.377    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 7.29     |
| rollout/Q_mean          | 65.4     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.6     |
| rollout/episodes        | 1.92e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.6     |
| total/duration          | 4.94e+03 |
| total/episodes          | 1.92e+04 |
| total/epochs            | 1        |
| total/steps             | 1909998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -72      |
| train/loss_critic       | 0.645    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1911000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1912000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 1913000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1914000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1915000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 1916000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1917000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1918000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 1919000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
Num timesteps: 1920000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
--------------------------------------
| reference_Q_mean        | 48.9     |
| reference_Q_std         | 7.71     |
| reference_action_mean   | -0.924   |
| reference_action_std    | 0.376    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 7.21     |
| rollout/Q_mean          | 65.5     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.6     |
| rollout/episodes        | 1.93e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.3     |
| total/duration          | 4.97e+03 |
| total/episodes          | 1.93e+04 |
| total/epochs            | 1        |
| total/steps             | 1919998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -71.8    |
| train/loss_critic       | 0.655    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1921000
Best mean reward: 94.27 - Last mean reward per episode: 92.85
Num timesteps: 1922000
Best mean reward: 94.27 - Last mean reward per episode: 92.84
Num timesteps: 1923000
Best mean reward: 94.27 - Last mean reward per episode: 92.88
Num timesteps: 1924000
Best mean reward: 94.27 - Last mean reward per episode: 92.87
Num timesteps: 1925000
Best mean reward: 94.27 - Last mean reward per episode: 91.43
Num timesteps: 1926000
Best mean reward: 94.27 - Last mean reward per episode: 91.50
Num timesteps: 1927000
Best mean reward: 94.27 - Last mean reward per episode: 91.55
Num timesteps: 1928000
Best mean reward: 94.27 - Last mean reward per episode: 91.54
Num timesteps: 1929000
Best mean reward: 94.27 - Last mean reward per episode: 91.59
Num timesteps: 1930000
Best mean reward: 94.27 - Last mean reward per episode: 91.94
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 7.46     |
| reference_action_mean   | -0.938   |
| reference_action_std    | 0.344    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 7.02     |
| rollout/Q_mean          | 65.5     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.6     |
| rollout/episodes        | 1.94e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 91.9     |
| total/duration          | 5e+03    |
| total/episodes          | 1.94e+04 |
| total/epochs            | 1        |
| total/steps             | 1929998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -72.4    |
| train/loss_critic       | 0.977    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1931000
Best mean reward: 94.27 - Last mean reward per episode: 91.94
Num timesteps: 1932000
Best mean reward: 94.27 - Last mean reward per episode: 91.61
Num timesteps: 1933000
Best mean reward: 94.27 - Last mean reward per episode: 91.60
Num timesteps: 1934000
Best mean reward: 94.27 - Last mean reward per episode: 91.70
Num timesteps: 1935000
Best mean reward: 94.27 - Last mean reward per episode: 92.98
Num timesteps: 1936000
Best mean reward: 94.27 - Last mean reward per episode: 92.94
Num timesteps: 1937000
Best mean reward: 94.27 - Last mean reward per episode: 92.94
Num timesteps: 1938000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 1939000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 1940000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 7.11     |
| reference_action_mean   | -0.937   |
| reference_action_std    | 0.346    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 6.77     |
| rollout/Q_mean          | 65.5     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.6     |
| rollout/episodes        | 1.95e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.2     |
| total/duration          | 5.02e+03 |
| total/episodes          | 1.95e+04 |
| total/epochs            | 1        |
| total/steps             | 1939998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -72.3    |
| train/loss_critic       | 0.836    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1941000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1942000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 1943000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1944000
Best mean reward: 94.27 - Last mean reward per episode: 93.64
Num timesteps: 1945000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1946000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1947000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 1948000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 1949000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 1950000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 6.86     |
| reference_action_mean   | -0.889   |
| reference_action_std    | 0.449    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 7.07     |
| rollout/Q_mean          | 65.6     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.839    |
| rollout/episode_steps   | 99.5     |
| rollout/episodes        | 1.96e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.3     |
| total/duration          | 5.05e+03 |
| total/episodes          | 1.96e+04 |
| total/epochs            | 1        |
| total/steps             | 1949998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -72.2    |
| train/loss_critic       | 0.768    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1951000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 1952000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 1953000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 1954000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 1955000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 1956000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 1957000
Best mean reward: 94.27 - Last mean reward per episode: 93.89
Num timesteps: 1958000
Best mean reward: 94.27 - Last mean reward per episode: 93.86
Num timesteps: 1959000
Best mean reward: 94.27 - Last mean reward per episode: 93.77
Num timesteps: 1960000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 7.11     |
| reference_action_mean   | -0.803   |
| reference_action_std    | 0.574    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 7.31     |
| rollout/Q_mean          | 65.6     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.3     |
| rollout/episodes        | 1.97e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.8     |
| total/duration          | 5.08e+03 |
| total/episodes          | 1.97e+04 |
| total/epochs            | 1        |
| total/steps             | 1959998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -72.3    |
| train/loss_critic       | 0.572    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1961000
Best mean reward: 94.27 - Last mean reward per episode: 93.70
Num timesteps: 1962000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 1963000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1964000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1965000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 1966000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 1967000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
Num timesteps: 1968000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 1969000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 1970000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
--------------------------------------
| reference_Q_mean        | 48.7     |
| reference_Q_std         | 7.23     |
| reference_action_mean   | -0.846   |
| reference_action_std    | 0.504    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 7.38     |
| rollout/Q_mean          | 65.6     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.3     |
| rollout/episodes        | 1.98e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.6     |
| total/duration          | 5.1e+03  |
| total/episodes          | 1.98e+04 |
| total/epochs            | 1        |
| total/steps             | 1969998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -72.3    |
| train/loss_critic       | 0.741    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1971000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1972000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 1973000
Best mean reward: 94.27 - Last mean reward per episode: 93.90
Num timesteps: 1974000
Best mean reward: 94.27 - Last mean reward per episode: 93.64
Num timesteps: 1975000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 1976000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 1977000
Best mean reward: 94.27 - Last mean reward per episode: 93.64
Num timesteps: 1978000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 1979000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 1980000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
--------------------------------------
| reference_Q_mean        | 48.5     |
| reference_Q_std         | 7.18     |
| reference_action_mean   | -0.737   |
| reference_action_std    | 0.663    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 7.02     |
| rollout/Q_mean          | 65.7     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.2     |
| rollout/episodes        | 2e+04    |
| rollout/return          | 92       |
| rollout/return_history  | 93.5     |
| total/duration          | 5.13e+03 |
| total/episodes          | 2e+04    |
| total/epochs            | 1        |
| total/steps             | 1979998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -72.3    |
| train/loss_critic       | 0.672    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1981000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 1982000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 1983000
Best mean reward: 94.27 - Last mean reward per episode: 91.51
Num timesteps: 1984000
Best mean reward: 94.27 - Last mean reward per episode: 91.58
Num timesteps: 1985000
Best mean reward: 94.27 - Last mean reward per episode: 91.46
Num timesteps: 1986000
Best mean reward: 94.27 - Last mean reward per episode: 91.19
Num timesteps: 1987000
Best mean reward: 94.27 - Last mean reward per episode: 91.00
Num timesteps: 1988000
Best mean reward: 94.27 - Last mean reward per episode: 90.89
Num timesteps: 1989000
Best mean reward: 94.27 - Last mean reward per episode: 90.91
Num timesteps: 1990000
Best mean reward: 94.27 - Last mean reward per episode: 90.84
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 7.56     |
| reference_action_mean   | -0.583   |
| reference_action_std    | 0.793    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 7.35     |
| rollout/Q_mean          | 65.7     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.3     |
| rollout/episodes        | 2e+04    |
| rollout/return          | 92       |
| rollout/return_history  | 90.8     |
| total/duration          | 5.16e+03 |
| total/episodes          | 2e+04    |
| total/epochs            | 1        |
| total/steps             | 1989998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -71.4    |
| train/loss_critic       | 1.68     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1991000
Best mean reward: 94.27 - Last mean reward per episode: 90.56
Num timesteps: 1992000
Best mean reward: 94.27 - Last mean reward per episode: 90.53
Num timesteps: 1993000
Best mean reward: 94.27 - Last mean reward per episode: 90.52
Num timesteps: 1994000
Best mean reward: 94.27 - Last mean reward per episode: 89.05
Num timesteps: 1995000
Best mean reward: 94.27 - Last mean reward per episode: 91.15
Num timesteps: 1996000
Best mean reward: 94.27 - Last mean reward per episode: 91.26
Num timesteps: 1997000
Best mean reward: 94.27 - Last mean reward per episode: 91.64
Num timesteps: 1998000
Best mean reward: 94.27 - Last mean reward per episode: 91.82
Num timesteps: 1999000
Best mean reward: 94.27 - Last mean reward per episode: 91.89
Num timesteps: 2000000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
--------------------------------------
| reference_Q_mean        | 48.9     |
| reference_Q_std         | 7.55     |
| reference_action_mean   | -0.752   |
| reference_action_std    | 0.646    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 7.53     |
| rollout/Q_mean          | 65.7     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.3     |
| rollout/episodes        | 2.02e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 92.1     |
| total/duration          | 5.19e+03 |
| total/episodes          | 2.02e+04 |
| total/epochs            | 1        |
| total/steps             | 1999998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -71.4    |
| train/loss_critic       | 0.763    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2001000
Best mean reward: 94.27 - Last mean reward per episode: 92.13
Num timesteps: 2002000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 2003000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 2004000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 2005000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 2006000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 2007000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 2008000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 2009000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 2010000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 7.03     |
| reference_action_mean   | -0.73    |
| reference_action_std    | 0.673    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 7.18     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.1     |
| rollout/episodes        | 2.03e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.7     |
| total/duration          | 5.22e+03 |
| total/episodes          | 2.03e+04 |
| total/epochs            | 1        |
| total/steps             | 2009998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -71      |
| train/loss_critic       | 1.84     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2011000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 2012000
Best mean reward: 94.27 - Last mean reward per episode: 93.74
Num timesteps: 2013000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 2014000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 2015000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 2016000
Best mean reward: 94.27 - Last mean reward per episode: 93.20
Num timesteps: 2017000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 2018000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 2019000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 2020000
Best mean reward: 94.27 - Last mean reward per episode: 93.02
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 6.67     |
| reference_action_mean   | -0.813   |
| reference_action_std    | 0.553    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 6.68     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.1     |
| rollout/episodes        | 2.04e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93       |
| total/duration          | 5.25e+03 |
| total/episodes          | 2.04e+04 |
| total/epochs            | 1        |
| total/steps             | 2019998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -71.2    |
| train/loss_critic       | 0.663    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2021000
Best mean reward: 94.27 - Last mean reward per episode: 92.91
Num timesteps: 2022000
Best mean reward: 94.27 - Last mean reward per episode: 92.90
Num timesteps: 2023000
Best mean reward: 94.27 - Last mean reward per episode: 92.83
Num timesteps: 2024000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 2025000
Best mean reward: 94.27 - Last mean reward per episode: 93.29
Num timesteps: 2026000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 2027000
Best mean reward: 94.27 - Last mean reward per episode: 93.23
Num timesteps: 2028000
Best mean reward: 94.27 - Last mean reward per episode: 93.29
Num timesteps: 2029000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 2030000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 6.72     |
| reference_action_mean   | -0.77    |
| reference_action_std    | 0.622    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 6.49     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99.1     |
| rollout/episodes        | 2.05e+04 |
| rollout/return          | 92       |
| rollout/return_history  | 93.5     |
| total/duration          | 5.27e+03 |
| total/episodes          | 2.05e+04 |
| total/epochs            | 1        |
| total/steps             | 2029998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 1.83     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2031000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2032000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 2033000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 2034000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 2035000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 2036000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 2037000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 2038000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 2039000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 2040000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
--------------------------------------
| reference_Q_mean        | 48.7     |
| reference_Q_std         | 7.07     |
| reference_action_mean   | -0.863   |
| reference_action_std    | 0.486    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 6.76     |
| rollout/Q_mean          | 65.8     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 99       |
| rollout/episodes        | 2.06e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.5     |
| total/duration          | 5.3e+03  |
| total/episodes          | 2.06e+04 |
| total/epochs            | 1        |
| total/steps             | 2039998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -71      |
| train/loss_critic       | 0.746    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2041000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 2042000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 2043000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 2044000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 2045000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 2046000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 2047000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 2048000
Best mean reward: 94.27 - Last mean reward per episode: 93.64
Num timesteps: 2049000
Best mean reward: 94.27 - Last mean reward per episode: 93.73
Num timesteps: 2050000
Best mean reward: 94.27 - Last mean reward per episode: 93.90
--------------------------------------
| reference_Q_mean        | 48.4     |
| reference_Q_std         | 7.09     |
| reference_action_mean   | -0.818   |
| reference_action_std    | 0.563    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 6.92     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.9     |
| rollout/episodes        | 2.07e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.9     |
| total/duration          | 5.33e+03 |
| total/episodes          | 2.07e+04 |
| total/epochs            | 1        |
| total/steps             | 2049998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -71.4    |
| train/loss_critic       | 0.592    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2051000
Best mean reward: 94.27 - Last mean reward per episode: 93.92
Num timesteps: 2052000
Best mean reward: 94.27 - Last mean reward per episode: 93.93
Num timesteps: 2053000
Best mean reward: 94.27 - Last mean reward per episode: 92.34
Num timesteps: 2054000
Best mean reward: 94.27 - Last mean reward per episode: 92.34
Num timesteps: 2055000
Best mean reward: 94.27 - Last mean reward per episode: 92.21
Num timesteps: 2056000
Best mean reward: 94.27 - Last mean reward per episode: 92.03
Num timesteps: 2057000
Best mean reward: 94.27 - Last mean reward per episode: 91.59
Num timesteps: 2058000
Best mean reward: 94.27 - Last mean reward per episode: 91.50
Num timesteps: 2059000
Best mean reward: 94.27 - Last mean reward per episode: 91.46
Num timesteps: 2060000
Best mean reward: 94.27 - Last mean reward per episode: 91.40
--------------------------------------
| reference_Q_mean        | 49.2     |
| reference_Q_std         | 6.97     |
| reference_action_mean   | -0.686   |
| reference_action_std    | 0.714    |
| reference_actor_Q_mean  | 51.2     |
| reference_actor_Q_std   | 6.72     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.9     |
| rollout/episodes        | 2.08e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 91.4     |
| total/duration          | 5.35e+03 |
| total/episodes          | 2.08e+04 |
| total/epochs            | 1        |
| total/steps             | 2059998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -71.1    |
| train/loss_critic       | 0.922    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2061000
Best mean reward: 94.27 - Last mean reward per episode: 91.36
Num timesteps: 2062000
Best mean reward: 94.27 - Last mean reward per episode: 91.32
Num timesteps: 2063000
Best mean reward: 94.27 - Last mean reward per episode: 92.54
Num timesteps: 2064000
Best mean reward: 94.27 - Last mean reward per episode: 92.55
Num timesteps: 2065000
Best mean reward: 94.27 - Last mean reward per episode: 92.67
Num timesteps: 2066000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 2067000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 2068000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 2069000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 2070000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
--------------------------------------
| reference_Q_mean        | 48.4     |
| reference_Q_std         | 7.09     |
| reference_action_mean   | -0.703   |
| reference_action_std    | 0.698    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 6.56     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.9     |
| rollout/episodes        | 2.09e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.3     |
| total/duration          | 5.38e+03 |
| total/episodes          | 2.09e+04 |
| total/epochs            | 1        |
| total/steps             | 2069998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 0.749    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2071000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 2072000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 2073000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 2074000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 2075000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
Num timesteps: 2076000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 2077000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 2078000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 2079000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 2080000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
--------------------------------------
| reference_Q_mean        | 48.3     |
| reference_Q_std         | 7.42     |
| reference_action_mean   | -0.79    |
| reference_action_std    | 0.597    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 6.95     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.8     |
| rollout/episodes        | 2.1e+04  |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.3     |
| total/duration          | 5.41e+03 |
| total/episodes          | 2.1e+04  |
| total/epochs            | 1        |
| total/steps             | 2079998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.81     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2081000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 2082000
Best mean reward: 94.27 - Last mean reward per episode: 93.23
Num timesteps: 2083000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 2084000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 2085000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 2086000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 2087000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 2088000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 2089000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 2090000
Best mean reward: 94.27 - Last mean reward per episode: 92.01
--------------------------------------
| reference_Q_mean        | 48.3     |
| reference_Q_std         | 7.55     |
| reference_action_mean   | -0.823   |
| reference_action_std    | 0.559    |
| reference_actor_Q_mean  | 49.7     |
| reference_actor_Q_std   | 7.24     |
| rollout/Q_mean          | 65.9     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.8     |
| rollout/episodes        | 2.12e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 92       |
| total/duration          | 5.44e+03 |
| total/episodes          | 2.12e+04 |
| total/epochs            | 1        |
| total/steps             | 2089998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 1.14     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2091000
Best mean reward: 94.27 - Last mean reward per episode: 92.04
Num timesteps: 2092000
Best mean reward: 94.27 - Last mean reward per episode: 92.06
Num timesteps: 2093000
Best mean reward: 94.27 - Last mean reward per episode: 92.05
Num timesteps: 2094000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
Num timesteps: 2095000
Best mean reward: 94.27 - Last mean reward per episode: 92.09
Num timesteps: 2096000
Best mean reward: 94.27 - Last mean reward per episode: 92.04
Num timesteps: 2097000
Best mean reward: 94.27 - Last mean reward per episode: 91.99
Num timesteps: 2098000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 2099000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 2100000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
--------------------------------------
| reference_Q_mean        | 48.4     |
| reference_Q_std         | 7.56     |
| reference_action_mean   | -0.804   |
| reference_action_std    | 0.584    |
| reference_actor_Q_mean  | 49.7     |
| reference_actor_Q_std   | 7.31     |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.13e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.3     |
| total/duration          | 5.46e+03 |
| total/episodes          | 2.13e+04 |
| total/epochs            | 1        |
| total/steps             | 2099998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 1.26     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2101000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 2102000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 2103000
Best mean reward: 94.27 - Last mean reward per episode: 93.16
Num timesteps: 2104000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 2105000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 2106000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2107000
Best mean reward: 94.27 - Last mean reward per episode: 91.83
Num timesteps: 2108000
Best mean reward: 94.27 - Last mean reward per episode: 91.87
Num timesteps: 2109000
Best mean reward: 94.27 - Last mean reward per episode: 91.86
Num timesteps: 2110000
Best mean reward: 94.27 - Last mean reward per episode: 91.73
--------------------------------------
| reference_Q_mean        | 48.1     |
| reference_Q_std         | 7.76     |
| reference_action_mean   | -0.78    |
| reference_action_std    | 0.619    |
| reference_actor_Q_mean  | 49.5     |
| reference_actor_Q_std   | 7.38     |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.14e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 91.7     |
| total/duration          | 5.49e+03 |
| total/episodes          | 2.14e+04 |
| total/epochs            | 1        |
| total/steps             | 2109998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 1.27     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2111000
Best mean reward: 94.27 - Last mean reward per episode: 91.70
Num timesteps: 2112000
Best mean reward: 94.27 - Last mean reward per episode: 91.67
Num timesteps: 2113000
Best mean reward: 94.27 - Last mean reward per episode: 91.73
Num timesteps: 2114000
Best mean reward: 94.27 - Last mean reward per episode: 91.71
Num timesteps: 2115000
Best mean reward: 94.27 - Last mean reward per episode: 91.43
Num timesteps: 2116000
Best mean reward: 94.27 - Last mean reward per episode: 90.99
Num timesteps: 2117000
Best mean reward: 94.27 - Last mean reward per episode: 90.55
Num timesteps: 2118000
Best mean reward: 94.27 - Last mean reward per episode: 92.28
Num timesteps: 2119000
Best mean reward: 94.27 - Last mean reward per episode: 92.20
Num timesteps: 2120000
Best mean reward: 94.27 - Last mean reward per episode: 92.35
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 7.94     |
| reference_action_mean   | -0.756   |
| reference_action_std    | 0.642    |
| reference_actor_Q_mean  | 50.2     |
| reference_actor_Q_std   | 7.35     |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.15e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 92.4     |
| total/duration          | 5.52e+03 |
| total/episodes          | 2.15e+04 |
| total/epochs            | 1        |
| total/steps             | 2119998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 1.25     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2121000
Best mean reward: 94.27 - Last mean reward per episode: 92.31
Num timesteps: 2122000
Best mean reward: 94.27 - Last mean reward per episode: 92.40
Num timesteps: 2123000
Best mean reward: 94.27 - Last mean reward per episode: 92.41
Num timesteps: 2124000
Best mean reward: 94.27 - Last mean reward per episode: 92.28
Num timesteps: 2125000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 2126000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 2127000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 2128000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 2129000
Best mean reward: 94.27 - Last mean reward per episode: 91.90
Num timesteps: 2130000
Best mean reward: 94.27 - Last mean reward per episode: 91.88
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 8.09     |
| reference_action_mean   | -0.754   |
| reference_action_std    | 0.651    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 7.31     |
| rollout/Q_mean          | 66       |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.84     |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.16e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 91.9     |
| total/duration          | 5.55e+03 |
| total/episodes          | 2.16e+04 |
| total/epochs            | 1        |
| total/steps             | 2129998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 1.39     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2131000
Best mean reward: 94.27 - Last mean reward per episode: 91.95
Num timesteps: 2132000
Best mean reward: 94.27 - Last mean reward per episode: 92.04
Num timesteps: 2133000
Best mean reward: 94.27 - Last mean reward per episode: 92.10
Num timesteps: 2134000
Best mean reward: 94.27 - Last mean reward per episode: 92.05
Num timesteps: 2135000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
Num timesteps: 2136000
Best mean reward: 94.27 - Last mean reward per episode: 92.21
Num timesteps: 2137000
Best mean reward: 94.27 - Last mean reward per episode: 92.30
Num timesteps: 2138000
Best mean reward: 94.27 - Last mean reward per episode: 92.37
Num timesteps: 2139000
Best mean reward: 94.27 - Last mean reward per episode: 92.28
Num timesteps: 2140000
Best mean reward: 94.27 - Last mean reward per episode: 92.28
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 7.8      |
| reference_action_mean   | -0.783   |
| reference_action_std    | 0.614    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 7.02     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.17e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 92.3     |
| total/duration          | 5.58e+03 |
| total/episodes          | 2.17e+04 |
| total/epochs            | 1        |
| total/steps             | 2139998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -71.3    |
| train/loss_critic       | 2.23     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2141000
Best mean reward: 94.27 - Last mean reward per episode: 92.27
Num timesteps: 2142000
Best mean reward: 94.27 - Last mean reward per episode: 92.31
Num timesteps: 2143000
Best mean reward: 94.27 - Last mean reward per episode: 92.33
Num timesteps: 2144000
Best mean reward: 94.27 - Last mean reward per episode: 92.31
Num timesteps: 2145000
Best mean reward: 94.27 - Last mean reward per episode: 92.10
Num timesteps: 2146000
Best mean reward: 94.27 - Last mean reward per episode: 90.20
Num timesteps: 2147000
Best mean reward: 94.27 - Last mean reward per episode: 90.12
Num timesteps: 2148000
Best mean reward: 94.27 - Last mean reward per episode: 91.44
Num timesteps: 2149000
Best mean reward: 94.27 - Last mean reward per episode: 91.46
Num timesteps: 2150000
Best mean reward: 94.27 - Last mean reward per episode: 91.37
--------------------------------------
| reference_Q_mean        | 49.1     |
| reference_Q_std         | 7.83     |
| reference_action_mean   | -0.614   |
| reference_action_std    | 0.775    |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 7.14     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.18e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 91.4     |
| total/duration          | 5.61e+03 |
| total/episodes          | 2.18e+04 |
| total/epochs            | 1        |
| total/steps             | 2149998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 1.97     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2151000
Best mean reward: 94.27 - Last mean reward per episode: 91.14
Num timesteps: 2152000
Best mean reward: 94.27 - Last mean reward per episode: 91.03
Num timesteps: 2153000
Best mean reward: 94.27 - Last mean reward per episode: 90.70
Num timesteps: 2154000
Best mean reward: 94.27 - Last mean reward per episode: 90.53
Num timesteps: 2155000
Best mean reward: 94.27 - Last mean reward per episode: 90.31
Num timesteps: 2156000
Best mean reward: 94.27 - Last mean reward per episode: 92.19
Num timesteps: 2157000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
Num timesteps: 2158000
Best mean reward: 94.27 - Last mean reward per episode: 92.06
Num timesteps: 2159000
Best mean reward: 94.27 - Last mean reward per episode: 91.94
Num timesteps: 2160000
Best mean reward: 94.27 - Last mean reward per episode: 91.82
--------------------------------------
| reference_Q_mean        | 49.2     |
| reference_Q_std         | 8.1      |
| reference_action_mean   | -0.667   |
| reference_action_std    | 0.738    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 7.63     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.19e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 91.8     |
| total/duration          | 5.63e+03 |
| total/episodes          | 2.19e+04 |
| total/epochs            | 1        |
| total/steps             | 2159998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 1.09     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2161000
Best mean reward: 94.27 - Last mean reward per episode: 91.78
Num timesteps: 2162000
Best mean reward: 94.27 - Last mean reward per episode: 91.70
Num timesteps: 2163000
Best mean reward: 94.27 - Last mean reward per episode: 91.85
Num timesteps: 2164000
Best mean reward: 94.27 - Last mean reward per episode: 91.92
Num timesteps: 2165000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
Num timesteps: 2166000
Best mean reward: 94.27 - Last mean reward per episode: 92.15
Num timesteps: 2167000
Best mean reward: 94.27 - Last mean reward per episode: 92.26
Num timesteps: 2168000
Best mean reward: 94.27 - Last mean reward per episode: 92.26
Num timesteps: 2169000
Best mean reward: 94.27 - Last mean reward per episode: 92.45
Num timesteps: 2170000
Best mean reward: 94.27 - Last mean reward per episode: 92.47
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 8.35     |
| reference_action_mean   | -0.782   |
| reference_action_std    | 0.62     |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 7.95     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.2e+04  |
| rollout/return          | 92.1     |
| rollout/return_history  | 92.5     |
| total/duration          | 5.66e+03 |
| total/episodes          | 2.2e+04  |
| total/epochs            | 1        |
| total/steps             | 2169998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -69      |
| train/loss_critic       | 1.37     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2171000
Best mean reward: 94.27 - Last mean reward per episode: 92.59
Num timesteps: 2172000
Best mean reward: 94.27 - Last mean reward per episode: 92.59
Num timesteps: 2173000
Best mean reward: 94.27 - Last mean reward per episode: 92.63
Num timesteps: 2174000
Best mean reward: 94.27 - Last mean reward per episode: 92.56
Num timesteps: 2175000
Best mean reward: 94.27 - Last mean reward per episode: 92.46
Num timesteps: 2176000
Best mean reward: 94.27 - Last mean reward per episode: 92.31
Num timesteps: 2177000
Best mean reward: 94.27 - Last mean reward per episode: 92.01
Num timesteps: 2178000
Best mean reward: 94.27 - Last mean reward per episode: 92.21
Num timesteps: 2179000
Best mean reward: 94.27 - Last mean reward per episode: 92.19
Num timesteps: 2180000
Best mean reward: 94.27 - Last mean reward per episode: 92.27
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 7.96     |
| reference_action_mean   | -0.953   |
| reference_action_std    | 0.303    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 7.57     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.21e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 92.3     |
| total/duration          | 5.69e+03 |
| total/episodes          | 2.21e+04 |
| total/epochs            | 1        |
| total/steps             | 2179998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -68.2    |
| train/loss_critic       | 1.68     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2181000
Best mean reward: 94.27 - Last mean reward per episode: 92.29
Num timesteps: 2182000
Best mean reward: 94.27 - Last mean reward per episode: 90.87
Num timesteps: 2183000
Best mean reward: 94.27 - Last mean reward per episode: 91.03
Num timesteps: 2184000
Best mean reward: 94.27 - Last mean reward per episode: 91.08
Num timesteps: 2185000
Best mean reward: 94.27 - Last mean reward per episode: 89.36
Num timesteps: 2186000
Best mean reward: 94.27 - Last mean reward per episode: 89.60
Num timesteps: 2187000
Best mean reward: 94.27 - Last mean reward per episode: 89.76
Num timesteps: 2188000
Best mean reward: 94.27 - Last mean reward per episode: 89.93
Num timesteps: 2189000
Best mean reward: 94.27 - Last mean reward per episode: 89.93
Num timesteps: 2190000
Best mean reward: 94.27 - Last mean reward per episode: 88.09
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 7.96     |
| reference_action_mean   | -0.953   |
| reference_action_std    | 0.303    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 7.83     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.8     |
| rollout/episodes        | 2.22e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 88.1     |
| total/duration          | 5.72e+03 |
| total/episodes          | 2.22e+04 |
| total/epochs            | 1        |
| total/steps             | 2189998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -66.5    |
| train/loss_critic       | 1.67     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2191000
Best mean reward: 94.27 - Last mean reward per episode: 88.05
Num timesteps: 2192000
Best mean reward: 94.27 - Last mean reward per episode: 88.20
Num timesteps: 2193000
Best mean reward: 94.27 - Last mean reward per episode: 89.78
Num timesteps: 2194000
Best mean reward: 94.27 - Last mean reward per episode: 89.92
Num timesteps: 2195000
Best mean reward: 94.27 - Last mean reward per episode: 91.79
Num timesteps: 2196000
Best mean reward: 94.27 - Last mean reward per episode: 91.92
Num timesteps: 2197000
Best mean reward: 94.27 - Last mean reward per episode: 91.86
Num timesteps: 2198000
Best mean reward: 94.27 - Last mean reward per episode: 93.84
Num timesteps: 2199000
Best mean reward: 94.27 - Last mean reward per episode: 93.93
Num timesteps: 2200000
Best mean reward: 94.27 - Last mean reward per episode: 93.90
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 7.79     |
| reference_action_mean   | -0.845   |
| reference_action_std    | 0.512    |
| reference_actor_Q_mean  | 49.8     |
| reference_actor_Q_std   | 7.74     |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.23e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.9     |
| total/duration          | 5.74e+03 |
| total/episodes          | 2.23e+04 |
| total/epochs            | 1        |
| total/steps             | 2199998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -66.7    |
| train/loss_critic       | 1.75     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2201000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 2202000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 2203000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 2204000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 2205000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 2206000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 2207000
Best mean reward: 94.27 - Last mean reward per episode: 93.64
Num timesteps: 2208000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 2209000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 2210000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 49.1     |
| reference_Q_std         | 7.73     |
| reference_action_mean   | -0.784   |
| reference_action_std    | 0.602    |
| reference_actor_Q_mean  | 49.5     |
| reference_actor_Q_std   | 7.7      |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.7     |
| rollout/episodes        | 2.24e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.1     |
| total/duration          | 5.77e+03 |
| total/episodes          | 2.24e+04 |
| total/epochs            | 1        |
| total/steps             | 2209998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -67.5    |
| train/loss_critic       | 1.58     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2211000
Best mean reward: 94.27 - Last mean reward per episode: 93.17
Num timesteps: 2212000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 2213000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 2214000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 2215000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 2216000
Best mean reward: 94.27 - Last mean reward per episode: 93.38
Num timesteps: 2217000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 2218000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 2219000
Best mean reward: 94.27 - Last mean reward per episode: 93.80
Num timesteps: 2220000
Best mean reward: 94.27 - Last mean reward per episode: 93.84
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 7.64     |
| reference_action_mean   | -0.642   |
| reference_action_std    | 0.748    |
| reference_actor_Q_mean  | 49.4     |
| reference_actor_Q_std   | 7.6      |
| rollout/Q_mean          | 66.1     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.6     |
| rollout/episodes        | 2.25e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.8     |
| total/duration          | 5.8e+03  |
| total/episodes          | 2.25e+04 |
| total/epochs            | 1        |
| total/steps             | 2219998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -68.6    |
| train/loss_critic       | 1.26     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2221000
Best mean reward: 94.27 - Last mean reward per episode: 93.78
Num timesteps: 2222000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 2223000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 2224000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 2225000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 2226000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
Num timesteps: 2227000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 2228000
Best mean reward: 94.27 - Last mean reward per episode: 93.07
Num timesteps: 2229000
Best mean reward: 94.27 - Last mean reward per episode: 93.05
Num timesteps: 2230000
Best mean reward: 94.27 - Last mean reward per episode: 92.88
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 7.65     |
| reference_action_mean   | -0.708   |
| reference_action_std    | 0.682    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 7.65     |
| rollout/Q_mean          | 66.2     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.6     |
| rollout/episodes        | 2.26e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 92.9     |
| total/duration          | 5.83e+03 |
| total/episodes          | 2.26e+04 |
| total/epochs            | 1        |
| total/steps             | 2229998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.49     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2231000
Best mean reward: 94.27 - Last mean reward per episode: 92.89
Num timesteps: 2232000
Best mean reward: 94.27 - Last mean reward per episode: 92.93
Num timesteps: 2233000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 2234000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 2235000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 2236000
Best mean reward: 94.27 - Last mean reward per episode: 93.64
Num timesteps: 2237000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 2238000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 2239000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 2240000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 7.78     |
| reference_action_mean   | -0.619   |
| reference_action_std    | 0.762    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 7.79     |
| rollout/Q_mean          | 66.2     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.6     |
| rollout/episodes        | 2.27e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.2     |
| total/duration          | 5.86e+03 |
| total/episodes          | 2.27e+04 |
| total/epochs            | 1        |
| total/steps             | 2239998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.2    |
| train/loss_critic       | 0.878    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2241000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 2242000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 2243000
Best mean reward: 94.27 - Last mean reward per episode: 93.19
Num timesteps: 2244000
Best mean reward: 94.27 - Last mean reward per episode: 93.15
Num timesteps: 2245000
Best mean reward: 94.27 - Last mean reward per episode: 93.04
Num timesteps: 2246000
Best mean reward: 94.27 - Last mean reward per episode: 93.20
Num timesteps: 2247000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
Num timesteps: 2248000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 2249000
Best mean reward: 94.27 - Last mean reward per episode: 93.39
Num timesteps: 2250000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 7.66     |
| reference_action_mean   | -0.711   |
| reference_action_std    | 0.684    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 7.55     |
| rollout/Q_mean          | 66.2     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.841    |
| rollout/episode_steps   | 98.5     |
| rollout/episodes        | 2.28e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.3     |
| total/duration          | 5.88e+03 |
| total/episodes          | 2.28e+04 |
| total/epochs            | 1        |
| total/steps             | 2249998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.2    |
| train/loss_critic       | 0.962    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2251000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 2252000
Best mean reward: 94.27 - Last mean reward per episode: 93.10
Num timesteps: 2253000
Best mean reward: 94.27 - Last mean reward per episode: 93.05
Num timesteps: 2254000
Best mean reward: 94.27 - Last mean reward per episode: 93.24
Num timesteps: 2255000
Best mean reward: 94.27 - Last mean reward per episode: 93.32
Num timesteps: 2256000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 2257000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 2258000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 2259000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 2260000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 7.77     |
| reference_action_mean   | -0.691   |
| reference_action_std    | 0.707    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 7.56     |
| rollout/Q_mean          | 66.2     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.4     |
| rollout/episodes        | 2.3e+04  |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.6     |
| total/duration          | 5.91e+03 |
| total/episodes          | 2.3e+04  |
| total/epochs            | 1        |
| total/steps             | 2259998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.7    |
| train/loss_critic       | 0.967    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2261000
Best mean reward: 94.27 - Last mean reward per episode: 93.57
Num timesteps: 2262000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 2263000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 2264000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 2265000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2266000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 2267000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 2268000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 2269000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 2270000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 7.62     |
| reference_action_mean   | -0.727   |
| reference_action_std    | 0.668    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 7.41     |
| rollout/Q_mean          | 66.3     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.3     |
| rollout/episodes        | 2.31e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.8     |
| total/duration          | 5.94e+03 |
| total/episodes          | 2.31e+04 |
| total/epochs            | 1        |
| total/steps             | 2269998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.8    |
| train/loss_critic       | 0.947    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2271000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 2272000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 2273000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 2274000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 2275000
Best mean reward: 94.27 - Last mean reward per episode: 93.84
Num timesteps: 2276000
Best mean reward: 94.27 - Last mean reward per episode: 93.79
Num timesteps: 2277000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
Num timesteps: 2278000
Best mean reward: 94.27 - Last mean reward per episode: 93.64
Num timesteps: 2279000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 2280000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 7.36     |
| reference_action_mean   | -0.736   |
| reference_action_std    | 0.659    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 7.23     |
| rollout/Q_mean          | 66.3     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.3     |
| rollout/episodes        | 2.32e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.5     |
| total/duration          | 5.96e+03 |
| total/episodes          | 2.32e+04 |
| total/epochs            | 1        |
| total/steps             | 2279998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -72.2    |
| train/loss_critic       | 0.989    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2281000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 2282000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 2283000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 2284000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 2285000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 2286000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 2287000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 2288000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 2289000
Best mean reward: 94.27 - Last mean reward per episode: 93.71
Num timesteps: 2290000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 7.27     |
| reference_action_mean   | -0.777   |
| reference_action_std    | 0.612    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 7.07     |
| rollout/Q_mean          | 66.3     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.2     |
| rollout/episodes        | 2.33e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.6     |
| total/duration          | 5.99e+03 |
| total/episodes          | 2.33e+04 |
| total/epochs            | 1        |
| total/steps             | 2289998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -72.1    |
| train/loss_critic       | 0.941    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2291000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 2292000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2293000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 2294000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 2295000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 2296000
Best mean reward: 94.27 - Last mean reward per episode: 93.40
Num timesteps: 2297000
Best mean reward: 94.27 - Last mean reward per episode: 93.34
Num timesteps: 2298000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 2299000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 2300000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 7.06     |
| reference_action_mean   | -0.89    |
| reference_action_std    | 0.442    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 6.84     |
| rollout/Q_mean          | 66.3     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.1     |
| rollout/episodes        | 2.34e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 6.02e+03 |
| total/episodes          | 2.34e+04 |
| total/epochs            | 1        |
| total/steps             | 2299998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -72.2    |
| train/loss_critic       | 1.02     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2301000
Best mean reward: 94.27 - Last mean reward per episode: 93.08
Num timesteps: 2302000
Best mean reward: 94.27 - Last mean reward per episode: 93.01
Num timesteps: 2303000
Best mean reward: 94.27 - Last mean reward per episode: 93.04
Num timesteps: 2304000
Best mean reward: 94.27 - Last mean reward per episode: 92.98
Num timesteps: 2305000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 2306000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 2307000
Best mean reward: 94.27 - Last mean reward per episode: 93.37
Num timesteps: 2308000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 2309000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 2310000
Best mean reward: 94.27 - Last mean reward per episode: 93.86
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 6.73     |
| reference_action_mean   | -0.848   |
| reference_action_std    | 0.515    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 6.53     |
| rollout/Q_mean          | 66.4     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98.1     |
| rollout/episodes        | 2.36e+04 |
| rollout/return          | 92.1     |
| rollout/return_history  | 93.9     |
| total/duration          | 6.04e+03 |
| total/episodes          | 2.36e+04 |
| total/epochs            | 1        |
| total/steps             | 2309998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.5    |
| train/loss_critic       | 0.942    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2311000
Best mean reward: 94.27 - Last mean reward per episode: 93.80
Num timesteps: 2312000
Best mean reward: 94.27 - Last mean reward per episode: 93.88
Num timesteps: 2313000
Best mean reward: 94.27 - Last mean reward per episode: 93.91
Num timesteps: 2314000
Best mean reward: 94.27 - Last mean reward per episode: 93.90
Num timesteps: 2315000
Best mean reward: 94.27 - Last mean reward per episode: 93.87
Num timesteps: 2316000
Best mean reward: 94.27 - Last mean reward per episode: 93.91
Num timesteps: 2317000
Best mean reward: 94.27 - Last mean reward per episode: 92.26
Num timesteps: 2318000
Best mean reward: 94.27 - Last mean reward per episode: 92.24
Num timesteps: 2319000
Best mean reward: 94.27 - Last mean reward per episode: 92.35
Num timesteps: 2320000
Best mean reward: 94.27 - Last mean reward per episode: 92.30
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 6.95     |
| reference_action_mean   | -0.751   |
| reference_action_std    | 0.648    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 6.69     |
| rollout/Q_mean          | 66.4     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.842    |
| rollout/episode_steps   | 98       |
| rollout/episodes        | 2.37e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 92.3     |
| total/duration          | 6.07e+03 |
| total/episodes          | 2.37e+04 |
| total/epochs            | 1        |
| total/steps             | 2319998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.9    |
| train/loss_critic       | 0.959    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2321000
Best mean reward: 94.27 - Last mean reward per episode: 92.30
Num timesteps: 2322000
Best mean reward: 94.27 - Last mean reward per episode: 92.24
Num timesteps: 2323000
Best mean reward: 94.27 - Last mean reward per episode: 92.26
Num timesteps: 2324000
Best mean reward: 94.27 - Last mean reward per episode: 93.89
Num timesteps: 2325000
Best mean reward: 94.27 - Last mean reward per episode: 93.93
Num timesteps: 2326000
Best mean reward: 94.27 - Last mean reward per episode: 93.94
Num timesteps: 2327000
Best mean reward: 94.27 - Last mean reward per episode: 93.93
Num timesteps: 2328000
Best mean reward: 94.27 - Last mean reward per episode: 93.97
Num timesteps: 2329000
Best mean reward: 94.27 - Last mean reward per episode: 94.00
Num timesteps: 2330000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 7.32     |
| reference_action_mean   | -0.767   |
| reference_action_std    | 0.632    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 7.07     |
| rollout/Q_mean          | 66.4     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.9     |
| rollout/episodes        | 2.38e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.8     |
| total/duration          | 6.1e+03  |
| total/episodes          | 2.38e+04 |
| total/epochs            | 1        |
| total/steps             | 2329998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.8    |
| train/loss_critic       | 1.02     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2331000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 2332000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 2333000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 2334000
Best mean reward: 94.27 - Last mean reward per episode: 93.23
Num timesteps: 2335000
Best mean reward: 94.27 - Last mean reward per episode: 93.25
Num timesteps: 2336000
Best mean reward: 94.27 - Last mean reward per episode: 93.18
Num timesteps: 2337000
Best mean reward: 94.27 - Last mean reward per episode: 93.12
Num timesteps: 2338000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
Num timesteps: 2339000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 2340000
Best mean reward: 94.27 - Last mean reward per episode: 93.72
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 7.16     |
| reference_action_mean   | -0.739   |
| reference_action_std    | 0.653    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 7.01     |
| rollout/Q_mean          | 66.4     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.8     |
| rollout/episodes        | 2.39e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.7     |
| total/duration          | 6.13e+03 |
| total/episodes          | 2.39e+04 |
| total/epochs            | 1        |
| total/steps             | 2339998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.8    |
| train/loss_critic       | 0.905    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2341000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 2342000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2343000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2344000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 2345000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 2346000
Best mean reward: 94.27 - Last mean reward per episode: 92.07
Num timesteps: 2347000
Best mean reward: 94.27 - Last mean reward per episode: 92.10
Num timesteps: 2348000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 2349000
Best mean reward: 94.27 - Last mean reward per episode: 91.85
Num timesteps: 2350000
Best mean reward: 94.27 - Last mean reward per episode: 92.01
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 6.94     |
| reference_action_mean   | -0.728   |
| reference_action_std    | 0.676    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 6.78     |
| rollout/Q_mean          | 66.4     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.8     |
| rollout/episodes        | 2.4e+04  |
| rollout/return          | 92.2     |
| rollout/return_history  | 92       |
| total/duration          | 6.15e+03 |
| total/episodes          | 2.4e+04  |
| total/epochs            | 1        |
| total/steps             | 2349998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.9    |
| train/loss_critic       | 1.1      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2351000
Best mean reward: 94.27 - Last mean reward per episode: 92.02
Num timesteps: 2352000
Best mean reward: 94.27 - Last mean reward per episode: 91.98
Num timesteps: 2353000
Best mean reward: 94.27 - Last mean reward per episode: 91.96
Num timesteps: 2354000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
Num timesteps: 2355000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 2356000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 2357000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 2358000
Best mean reward: 94.27 - Last mean reward per episode: 93.48
Num timesteps: 2359000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2360000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 6.61     |
| reference_action_mean   | -0.762   |
| reference_action_std    | 0.63     |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 6.46     |
| rollout/Q_mean          | 66.5     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.7     |
| rollout/episodes        | 2.41e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 6.18e+03 |
| total/episodes          | 2.41e+04 |
| total/epochs            | 1        |
| total/steps             | 2359998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.3    |
| train/loss_critic       | 2        |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2361000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 2362000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
Num timesteps: 2363000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 2364000
Best mean reward: 94.27 - Last mean reward per episode: 93.66
Num timesteps: 2365000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 2366000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 2367000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 2368000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 2369000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2370000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 6.44     |
| reference_action_mean   | -0.799   |
| reference_action_std    | 0.593    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 6.2      |
| rollout/Q_mean          | 66.5     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.7     |
| rollout/episodes        | 2.43e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.3     |
| total/duration          | 6.21e+03 |
| total/episodes          | 2.43e+04 |
| total/epochs            | 1        |
| total/steps             | 2369998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.4    |
| train/loss_critic       | 0.847    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2371000
Best mean reward: 94.27 - Last mean reward per episode: 93.31
Num timesteps: 2372000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 2373000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 2374000
Best mean reward: 94.27 - Last mean reward per episode: 92.77
Num timesteps: 2375000
Best mean reward: 94.27 - Last mean reward per episode: 92.91
Num timesteps: 2376000
Best mean reward: 94.27 - Last mean reward per episode: 92.89
Num timesteps: 2377000
Best mean reward: 94.27 - Last mean reward per episode: 92.97
Num timesteps: 2378000
Best mean reward: 94.27 - Last mean reward per episode: 92.93
Num timesteps: 2379000
Best mean reward: 94.27 - Last mean reward per episode: 93.15
Num timesteps: 2380000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 6.29     |
| reference_action_mean   | -0.798   |
| reference_action_std    | 0.596    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 6.02     |
| rollout/Q_mean          | 66.5     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.6     |
| rollout/episodes        | 2.44e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.4     |
| total/duration          | 6.24e+03 |
| total/episodes          | 2.44e+04 |
| total/epochs            | 1        |
| total/steps             | 2379998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.1    |
| train/loss_critic       | 0.96     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2381000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 2382000
Best mean reward: 94.27 - Last mean reward per episode: 93.69
Num timesteps: 2383000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 2384000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 2385000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 2386000
Best mean reward: 94.27 - Last mean reward per episode: 93.44
Num timesteps: 2387000
Best mean reward: 94.27 - Last mean reward per episode: 93.36
Num timesteps: 2388000
Best mean reward: 94.27 - Last mean reward per episode: 91.89
Num timesteps: 2389000
Best mean reward: 94.27 - Last mean reward per episode: 91.84
Num timesteps: 2390000
Best mean reward: 94.27 - Last mean reward per episode: 91.86
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 6.35     |
| reference_action_mean   | -0.795   |
| reference_action_std    | 0.6      |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 6.15     |
| rollout/Q_mean          | 66.5     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.6     |
| rollout/episodes        | 2.45e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.9     |
| total/duration          | 6.26e+03 |
| total/episodes          | 2.45e+04 |
| total/epochs            | 1        |
| total/steps             | 2389998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 1.01     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2391000
Best mean reward: 94.27 - Last mean reward per episode: 91.94
Num timesteps: 2392000
Best mean reward: 94.27 - Last mean reward per episode: 91.83
Num timesteps: 2393000
Best mean reward: 94.27 - Last mean reward per episode: 91.95
Num timesteps: 2394000
Best mean reward: 94.27 - Last mean reward per episode: 91.98
Num timesteps: 2395000
Best mean reward: 94.27 - Last mean reward per episode: 92.03
Num timesteps: 2396000
Best mean reward: 94.27 - Last mean reward per episode: 92.04
Num timesteps: 2397000
Best mean reward: 94.27 - Last mean reward per episode: 93.49
Num timesteps: 2398000
Best mean reward: 94.27 - Last mean reward per episode: 93.12
Num timesteps: 2399000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 2400000
Best mean reward: 94.27 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 6.53     |
| reference_action_mean   | -0.778   |
| reference_action_std    | 0.611    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 6.27     |
| rollout/Q_mean          | 66.5     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.6     |
| rollout/episodes        | 2.46e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.1     |
| total/duration          | 6.29e+03 |
| total/episodes          | 2.46e+04 |
| total/epochs            | 1        |
| total/steps             | 2399998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.3    |
| train/loss_critic       | 1.27     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2401000
Best mean reward: 94.27 - Last mean reward per episode: 93.11
Num timesteps: 2402000
Best mean reward: 94.27 - Last mean reward per episode: 93.12
Num timesteps: 2403000
Best mean reward: 94.27 - Last mean reward per episode: 93.05
Num timesteps: 2404000
Best mean reward: 94.27 - Last mean reward per episode: 92.98
Num timesteps: 2405000
Best mean reward: 94.27 - Last mean reward per episode: 93.00
Num timesteps: 2406000
Best mean reward: 94.27 - Last mean reward per episode: 92.74
Num timesteps: 2407000
Best mean reward: 94.27 - Last mean reward per episode: 93.18
Num timesteps: 2408000
Best mean reward: 94.27 - Last mean reward per episode: 93.11
Num timesteps: 2409000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 2410000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 6.85     |
| reference_action_mean   | -0.802   |
| reference_action_std    | 0.585    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 6.52     |
| rollout/Q_mean          | 66.6     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.6     |
| rollout/episodes        | 2.47e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.2     |
| total/duration          | 6.32e+03 |
| total/episodes          | 2.47e+04 |
| total/epochs            | 1        |
| total/steps             | 2409998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 1.2      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2411000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 2412000
Best mean reward: 94.27 - Last mean reward per episode: 93.29
Num timesteps: 2413000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 2414000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2415000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 2416000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 2417000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 2418000
Best mean reward: 94.27 - Last mean reward per episode: 93.35
Num timesteps: 2419000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 2420000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 7.33     |
| reference_action_mean   | -0.803   |
| reference_action_std    | 0.585    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 6.79     |
| rollout/Q_mean          | 66.6     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.5     |
| rollout/episodes        | 2.48e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.4     |
| total/duration          | 6.35e+03 |
| total/episodes          | 2.48e+04 |
| total/epochs            | 1        |
| total/steps             | 2419998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -71.2    |
| train/loss_critic       | 1.18     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2421000
Best mean reward: 94.27 - Last mean reward per episode: 93.45
Num timesteps: 2422000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2423000
Best mean reward: 94.27 - Last mean reward per episode: 93.54
Num timesteps: 2424000
Best mean reward: 94.27 - Last mean reward per episode: 93.43
Num timesteps: 2425000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 2426000
Best mean reward: 94.27 - Last mean reward per episode: 93.55
Num timesteps: 2427000
Best mean reward: 94.27 - Last mean reward per episode: 93.58
Num timesteps: 2428000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 2429000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 2430000
Best mean reward: 94.27 - Last mean reward per episode: 93.59
--------------------------------------
| reference_Q_mean        | 48.4     |
| reference_Q_std         | 7.55     |
| reference_action_mean   | -0.856   |
| reference_action_std    | 0.501    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 6.82     |
| rollout/Q_mean          | 66.6     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.4     |
| rollout/episodes        | 2.49e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.6     |
| total/duration          | 6.38e+03 |
| total/episodes          | 2.49e+04 |
| total/epochs            | 1        |
| total/steps             | 2429998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -71.3    |
| train/loss_critic       | 1.08     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2431000
Best mean reward: 94.27 - Last mean reward per episode: 93.53
Num timesteps: 2432000
Best mean reward: 94.27 - Last mean reward per episode: 93.81
Num timesteps: 2433000
Best mean reward: 94.27 - Last mean reward per episode: 93.90
Num timesteps: 2434000
Best mean reward: 94.27 - Last mean reward per episode: 93.80
Num timesteps: 2435000
Best mean reward: 94.27 - Last mean reward per episode: 93.82
Num timesteps: 2436000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 2437000
Best mean reward: 94.27 - Last mean reward per episode: 93.76
Num timesteps: 2438000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 2439000
Best mean reward: 94.27 - Last mean reward per episode: 93.75
Num timesteps: 2440000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 7.56     |
| reference_action_mean   | -0.883   |
| reference_action_std    | 0.459    |
| reference_actor_Q_mean  | 53.4     |
| reference_actor_Q_std   | 6.75     |
| rollout/Q_mean          | 66.6     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.843    |
| rollout/episode_steps   | 97.4     |
| rollout/episodes        | 2.51e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.6     |
| total/duration          | 6.4e+03  |
| total/episodes          | 2.51e+04 |
| total/epochs            | 1        |
| total/steps             | 2439998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -71.6    |
| train/loss_critic       | 0.98     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2441000
Best mean reward: 94.27 - Last mean reward per episode: 93.62
Num timesteps: 2442000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 2443000
Best mean reward: 94.27 - Last mean reward per episode: 93.63
Num timesteps: 2444000
Best mean reward: 94.27 - Last mean reward per episode: 93.30
Num timesteps: 2445000
Best mean reward: 94.27 - Last mean reward per episode: 93.26
Num timesteps: 2446000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 2447000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
Num timesteps: 2448000
Best mean reward: 94.27 - Last mean reward per episode: 93.17
Num timesteps: 2449000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 2450000
Best mean reward: 94.27 - Last mean reward per episode: 93.22
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 7.19     |
| reference_action_mean   | -0.895   |
| reference_action_std    | 0.437    |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 6.59     |
| rollout/Q_mean          | 66.6     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.3     |
| rollout/episodes        | 2.52e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.2     |
| total/duration          | 6.43e+03 |
| total/episodes          | 2.52e+04 |
| total/epochs            | 1        |
| total/steps             | 2449998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -71.6    |
| train/loss_critic       | 1.12     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2451000
Best mean reward: 94.27 - Last mean reward per episode: 93.21
Num timesteps: 2452000
Best mean reward: 94.27 - Last mean reward per episode: 93.51
Num timesteps: 2453000
Best mean reward: 94.27 - Last mean reward per episode: 93.60
Num timesteps: 2454000
Best mean reward: 94.27 - Last mean reward per episode: 93.61
Num timesteps: 2455000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 2456000
Best mean reward: 94.27 - Last mean reward per episode: 92.26
Num timesteps: 2457000
Best mean reward: 94.27 - Last mean reward per episode: 92.05
Num timesteps: 2458000
Best mean reward: 94.27 - Last mean reward per episode: 92.06
Num timesteps: 2459000
Best mean reward: 94.27 - Last mean reward per episode: 92.06
Num timesteps: 2460000
Best mean reward: 94.27 - Last mean reward per episode: 92.13
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 7.01     |
| reference_action_mean   | -0.907   |
| reference_action_std    | 0.421    |
| reference_actor_Q_mean  | 54       |
| reference_actor_Q_std   | 6.56     |
| rollout/Q_mean          | 66.7     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.3     |
| rollout/episodes        | 2.53e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 92.1     |
| total/duration          | 6.46e+03 |
| total/episodes          | 2.53e+04 |
| total/epochs            | 1        |
| total/steps             | 2459998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -71.9    |
| train/loss_critic       | 1.05     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2461000
Best mean reward: 94.27 - Last mean reward per episode: 92.14
Num timesteps: 2462000
Best mean reward: 94.27 - Last mean reward per episode: 92.15
Num timesteps: 2463000
Best mean reward: 94.27 - Last mean reward per episode: 92.12
Num timesteps: 2464000
Best mean reward: 94.27 - Last mean reward per episode: 90.41
Num timesteps: 2465000
Best mean reward: 94.27 - Last mean reward per episode: 90.32
Num timesteps: 2466000
Best mean reward: 94.27 - Last mean reward per episode: 92.02
Num timesteps: 2467000
Best mean reward: 94.27 - Last mean reward per episode: 92.02
Num timesteps: 2468000
Best mean reward: 94.27 - Last mean reward per episode: 91.97
Num timesteps: 2469000
Best mean reward: 94.27 - Last mean reward per episode: 91.87
Num timesteps: 2470000
Best mean reward: 94.27 - Last mean reward per episode: 91.77
--------------------------------------
| reference_Q_mean        | 49.8     |
| reference_Q_std         | 7.2      |
| reference_action_mean   | -0.884   |
| reference_action_std    | 0.456    |
| reference_actor_Q_mean  | 53.9     |
| reference_actor_Q_std   | 6.57     |
| rollout/Q_mean          | 66.7     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.3     |
| rollout/episodes        | 2.54e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 91.8     |
| total/duration          | 6.49e+03 |
| total/episodes          | 2.54e+04 |
| total/epochs            | 1        |
| total/steps             | 2469998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -71.9    |
| train/loss_critic       | 1.25     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2471000
Best mean reward: 94.27 - Last mean reward per episode: 91.75
Num timesteps: 2472000
Best mean reward: 94.27 - Last mean reward per episode: 91.58
Num timesteps: 2473000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
Num timesteps: 2474000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
Num timesteps: 2475000
Best mean reward: 94.27 - Last mean reward per episode: 92.93
Num timesteps: 2476000
Best mean reward: 94.27 - Last mean reward per episode: 92.93
Num timesteps: 2477000
Best mean reward: 94.27 - Last mean reward per episode: 92.86
Num timesteps: 2478000
Best mean reward: 94.27 - Last mean reward per episode: 92.96
Num timesteps: 2479000
Best mean reward: 94.27 - Last mean reward per episode: 93.12
Num timesteps: 2480000
Best mean reward: 94.27 - Last mean reward per episode: 93.28
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 7.3      |
| reference_action_mean   | -0.822   |
| reference_action_std    | 0.558    |
| reference_actor_Q_mean  | 54.1     |
| reference_actor_Q_std   | 6.62     |
| rollout/Q_mean          | 66.7     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.55e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.3     |
| total/duration          | 6.52e+03 |
| total/episodes          | 2.55e+04 |
| total/epochs            | 1        |
| total/steps             | 2479998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -71.8    |
| train/loss_critic       | 1.12     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2481000
Best mean reward: 94.27 - Last mean reward per episode: 93.27
Num timesteps: 2482000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 2483000
Best mean reward: 94.27 - Last mean reward per episode: 93.71
Num timesteps: 2484000
Best mean reward: 94.27 - Last mean reward per episode: 93.68
Num timesteps: 2485000
Best mean reward: 94.27 - Last mean reward per episode: 93.42
Num timesteps: 2486000
Best mean reward: 94.27 - Last mean reward per episode: 93.56
Num timesteps: 2487000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 2488000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
Num timesteps: 2489000
Best mean reward: 94.27 - Last mean reward per episode: 93.52
Num timesteps: 2490000
Best mean reward: 94.27 - Last mean reward per episode: 93.47
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 7.22     |
| reference_action_mean   | -0.824   |
| reference_action_std    | 0.557    |
| reference_actor_Q_mean  | 53.7     |
| reference_actor_Q_std   | 6.5      |
| rollout/Q_mean          | 66.7     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.56e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 6.54e+03 |
| total/episodes          | 2.56e+04 |
| total/epochs            | 1        |
| total/steps             | 2489998  |
| total/steps_per_second  | 380      |
| train/loss_actor        | -71.9    |
| train/loss_critic       | 0.949    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2491000
Best mean reward: 94.27 - Last mean reward per episode: 93.50
Num timesteps: 2492000
Best mean reward: 94.27 - Last mean reward per episode: 93.33
Num timesteps: 2493000
Best mean reward: 94.27 - Last mean reward per episode: 93.41
Num timesteps: 2494000
Best mean reward: 94.27 - Last mean reward per episode: 93.65
Num timesteps: 2495000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 2496000
Best mean reward: 94.27 - Last mean reward per episode: 93.67
Num timesteps: 2497000
Best mean reward: 94.27 - Last mean reward per episode: 93.46
Num timesteps: 2498000
Best mean reward: 94.27 - Last mean reward per episode: 93.23
Num timesteps: 2499000
Best mean reward: 94.27 - Last mean reward per episode: 93.13
Num timesteps: 2500000
Best mean reward: 94.27 - Last mean reward per episode: 93.06
--------------------------------------
| reference_Q_mean        | 48.9     |
| reference_Q_std         | 7.08     |
| reference_action_mean   | -0.787   |
| reference_action_std    | 0.588    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 6.25     |
| rollout/Q_mean          | 66.7     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.844    |
| rollout/episode_steps   | 97.2     |
| rollout/episodes        | 2.57e+04 |
| rollout/return          | 92.2     |
| rollout/return_history  | 93.1     |
| total/duration          | 6.58e+03 |
| total/episodes          | 2.57e+04 |
| total/epochs            | 1        |
| total/steps             | 2499998  |
| total/steps_per_second  | 380      |
| train/loss_actor        | -71.7    |
| train/loss_critic       | 2.25     |
| train/param_noise_di... | 0        |
--------------------------------------


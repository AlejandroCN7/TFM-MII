--------------------------------------------------------------------------
[[15591,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: instancia

Another transport will be used instead, although this may result in
lower performance.

NOTE: You can disable this warning by setting the MCA parameter
btl_base_warn_component_unused to 0.
--------------------------------------------------------------------------
Num timesteps: 1000
Best mean reward: -inf - Last mean reward per episode: -39.40
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 2000
Best mean reward: -39.40 - Last mean reward per episode: -24.03
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 3000
Best mean reward: -24.03 - Last mean reward per episode: -44.13
Num timesteps: 4000
Best mean reward: -24.03 - Last mean reward per episode: -42.52
Num timesteps: 5000
Best mean reward: -24.03 - Last mean reward per episode: -38.59
Num timesteps: 6000
Best mean reward: -24.03 - Last mean reward per episode: -47.73
Num timesteps: 7000
Best mean reward: -24.03 - Last mean reward per episode: -43.68
Num timesteps: 8000
Best mean reward: -24.03 - Last mean reward per episode: -43.93
Num timesteps: 9000
Best mean reward: -24.03 - Last mean reward per episode: -44.11
Num timesteps: 10000
Best mean reward: -24.03 - Last mean reward per episode: -41.51
--------------------------------------
| reference_Q_mean        | -0.0365  |
| reference_Q_std         | 0.0389   |
| reference_action_mean   | -0.0188  |
| reference_action_std    | 0.00709  |
| reference_actor_Q_mean  | 0.00197  |
| reference_actor_Q_std   | 0.00114  |
| rollout/Q_mean          | 0.0012   |
| rollout/actions_mean    | -0.216   |
| rollout/actions_std     | 0.607    |
| rollout/episode_steps   | 999      |
| rollout/episodes        | 10       |
| rollout/return          | -41.5    |
| rollout/return_history  | -41.5    |
| total/duration          | 25.3     |
| total/episodes          | 10       |
| total/epochs            | 1        |
| total/steps             | 9998     |
| total/steps_per_second  | 396      |
| train/loss_actor        | -0.00183 |
| train/loss_critic       | 2.23e-08 |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 11000
Best mean reward: -24.03 - Last mean reward per episode: -43.13
Num timesteps: 12000
Best mean reward: -24.03 - Last mean reward per episode: -43.73
Num timesteps: 13000
Best mean reward: -24.03 - Last mean reward per episode: -42.02
Num timesteps: 14000
Best mean reward: -24.03 - Last mean reward per episode: -40.62
Num timesteps: 15000
Best mean reward: -24.03 - Last mean reward per episode: -44.15
Num timesteps: 16000
Best mean reward: -24.03 - Last mean reward per episode: -41.92
Num timesteps: 17000
Best mean reward: -24.03 - Last mean reward per episode: -42.58
Num timesteps: 18000
Best mean reward: -24.03 - Last mean reward per episode: -43.70
Num timesteps: 19000
Best mean reward: -24.03 - Last mean reward per episode: -44.15
Num timesteps: 20000
Best mean reward: -24.03 - Last mean reward per episode: -42.90
--------------------------------------
| reference_Q_mean        | -0.0365  |
| reference_Q_std         | 0.0389   |
| reference_action_mean   | -0.027   |
| reference_action_std    | 0.00837  |
| reference_actor_Q_mean  | 0.00198  |
| reference_actor_Q_std   | 0.00105  |
| rollout/Q_mean          | 0.00156  |
| rollout/actions_mean    | -0.215   |
| rollout/actions_std     | 0.619    |
| rollout/episode_steps   | 999      |
| rollout/episodes        | 20       |
| rollout/return          | -42.9    |
| rollout/return_history  | -42.9    |
| total/duration          | 49.8     |
| total/episodes          | 20       |
| total/epochs            | 1        |
| total/steps             | 19998    |
| total/steps_per_second  | 402      |
| train/loss_actor        | -0.00191 |
| train/loss_critic       | 3.42e-08 |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 21000
Best mean reward: -24.03 - Last mean reward per episode: -41.98
Num timesteps: 22000
Best mean reward: -24.03 - Last mean reward per episode: -41.31
Num timesteps: 23000
Best mean reward: -24.03 - Last mean reward per episode: -40.73
Num timesteps: 24000
Best mean reward: -24.03 - Last mean reward per episode: -39.50
Num timesteps: 25000
Best mean reward: -24.03 - Last mean reward per episode: -39.05
Num timesteps: 26000
Best mean reward: -24.03 - Last mean reward per episode: -39.37
Num timesteps: 27000
Best mean reward: -24.03 - Last mean reward per episode: -34.55
Num timesteps: 28000
Best mean reward: -24.03 - Last mean reward per episode: -33.64
Num timesteps: 29000
Best mean reward: -24.03 - Last mean reward per episode: -33.10
Num timesteps: 30000
Best mean reward: -24.03 - Last mean reward per episode: -34.55
--------------------------------------
| reference_Q_mean        | -0.00318 |
| reference_Q_std         | 0.0407   |
| reference_action_mean   | -0.00473 |
| reference_action_std    | 0.0117   |
| reference_actor_Q_mean  | 0.0355   |
| reference_actor_Q_std   | 0.00588  |
| rollout/Q_mean          | 0.00296  |
| rollout/actions_mean    | -0.127   |
| rollout/actions_std     | 0.603    |
| rollout/episode_steps   | 990      |
| rollout/episodes        | 30       |
| rollout/return          | -34.6    |
| rollout/return_history  | -34.6    |
| total/duration          | 73.4     |
| total/episodes          | 30       |
| total/epochs            | 1        |
| total/steps             | 29998    |
| total/steps_per_second  | 409      |
| train/loss_actor        | -0.0352  |
| train/loss_critic       | 7.48e-06 |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 31000
Best mean reward: -24.03 - Last mean reward per episode: -34.64
Num timesteps: 32000
Best mean reward: -24.03 - Last mean reward per episode: -31.80
Num timesteps: 33000
Best mean reward: -24.03 - Last mean reward per episode: -32.45
Num timesteps: 34000
Best mean reward: -24.03 - Last mean reward per episode: -33.56
Num timesteps: 35000
Best mean reward: -24.03 - Last mean reward per episode: -33.19
Num timesteps: 36000
Best mean reward: -24.03 - Last mean reward per episode: -30.09
Num timesteps: 37000
Best mean reward: -24.03 - Last mean reward per episode: -30.39
Num timesteps: 38000
Best mean reward: -24.03 - Last mean reward per episode: -30.76
Num timesteps: 39000
Best mean reward: -24.03 - Last mean reward per episode: -30.32
Num timesteps: 40000
Best mean reward: -24.03 - Last mean reward per episode: -30.60
--------------------------------------
| reference_Q_mean        | 0.0344   |
| reference_Q_std         | 0.0648   |
| reference_action_mean   | -0.0448  |
| reference_action_std    | 0.155    |
| reference_actor_Q_mean  | 0.0772   |
| reference_actor_Q_std   | 0.0677   |
| rollout/Q_mean          | 0.019    |
| rollout/actions_mean    | -0.0903  |
| rollout/actions_std     | 0.619    |
| rollout/episode_steps   | 976      |
| rollout/episodes        | 40       |
| rollout/return          | -30.6    |
| rollout/return_history  | -30.6    |
| total/duration          | 98.5     |
| total/episodes          | 40       |
| total/epochs            | 1        |
| total/steps             | 39998    |
| total/steps_per_second  | 406      |
| train/loss_actor        | -0.0958  |
| train/loss_critic       | 1.29     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 41000
Best mean reward: -24.03 - Last mean reward per episode: -30.93
Num timesteps: 42000
Best mean reward: -24.03 - Last mean reward per episode: -30.49
Num timesteps: 43000
Best mean reward: -24.03 - Last mean reward per episode: -31.33
Num timesteps: 44000
Best mean reward: -24.03 - Last mean reward per episode: -31.04
Num timesteps: 45000
Best mean reward: -24.03 - Last mean reward per episode: -30.62
Num timesteps: 46000
Best mean reward: -24.03 - Last mean reward per episode: -30.97
Num timesteps: 47000
Best mean reward: -24.03 - Last mean reward per episode: -31.70
Num timesteps: 48000
Best mean reward: -24.03 - Last mean reward per episode: -31.85
Num timesteps: 49000
Best mean reward: -24.03 - Last mean reward per episode: -31.91
Num timesteps: 50000
Best mean reward: -24.03 - Last mean reward per episode: -31.42
--------------------------------------
| reference_Q_mean        | 0.0732   |
| reference_Q_std         | 0.203    |
| reference_action_mean   | 0.0387   |
| reference_action_std    | 0.169    |
| reference_actor_Q_mean  | 0.108    |
| reference_actor_Q_std   | 0.217    |
| rollout/Q_mean          | 0.0289   |
| rollout/actions_mean    | -0.0668  |
| rollout/actions_std     | 0.614    |
| rollout/episode_steps   | 981      |
| rollout/episodes        | 50       |
| rollout/return          | -31.4    |
| rollout/return_history  | -31.4    |
| total/duration          | 124      |
| total/episodes          | 50       |
| total/epochs            | 1        |
| total/steps             | 49998    |
| total/steps_per_second  | 403      |
| train/loss_actor        | -0.164   |
| train/loss_critic       | 0.00462  |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 51000
Best mean reward: -24.03 - Last mean reward per episode: -31.64
Num timesteps: 52000
Best mean reward: -24.03 - Last mean reward per episode: -31.95
Num timesteps: 53000
Best mean reward: -24.03 - Last mean reward per episode: -32.57
Num timesteps: 54000
Best mean reward: -24.03 - Last mean reward per episode: -32.28
Num timesteps: 55000
Best mean reward: -24.03 - Last mean reward per episode: -32.63
Num timesteps: 56000
Best mean reward: -24.03 - Last mean reward per episode: -32.33
Num timesteps: 57000
Best mean reward: -24.03 - Last mean reward per episode: -32.48
Num timesteps: 58000
Best mean reward: -24.03 - Last mean reward per episode: -32.17
Num timesteps: 59000
Best mean reward: -24.03 - Last mean reward per episode: -32.48
Num timesteps: 60000
Best mean reward: -24.03 - Last mean reward per episode: -32.38
--------------------------------------
| reference_Q_mean        | 0.0714   |
| reference_Q_std         | 0.206    |
| reference_action_mean   | 0.0491   |
| reference_action_std    | 0.113    |
| reference_actor_Q_mean  | 0.102    |
| reference_actor_Q_std   | 0.212    |
| rollout/Q_mean          | 0.0574   |
| rollout/actions_mean    | -0.0615  |
| rollout/actions_std     | 0.617    |
| rollout/episode_steps   | 984      |
| rollout/episodes        | 60       |
| rollout/return          | -32.4    |
| rollout/return_history  | -32.4    |
| total/duration          | 150      |
| total/episodes          | 60       |
| total/epochs            | 1        |
| total/steps             | 59998    |
| total/steps_per_second  | 401      |
| train/loss_actor        | -0.209   |
| train/loss_critic       | 0.00719  |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 61000
Best mean reward: -24.03 - Last mean reward per episode: -32.95
Num timesteps: 62000
Best mean reward: -24.03 - Last mean reward per episode: -32.99
Num timesteps: 63000
Best mean reward: -24.03 - Last mean reward per episode: -32.73
Num timesteps: 64000
Best mean reward: -24.03 - Last mean reward per episode: -33.22
Num timesteps: 65000
Best mean reward: -24.03 - Last mean reward per episode: -33.29
Num timesteps: 66000
Best mean reward: -24.03 - Last mean reward per episode: -33.61
Num timesteps: 67000
Best mean reward: -24.03 - Last mean reward per episode: -33.51
Num timesteps: 68000
Best mean reward: -24.03 - Last mean reward per episode: -33.96
Num timesteps: 69000
Best mean reward: -24.03 - Last mean reward per episode: -34.36
Num timesteps: 70000
Best mean reward: -24.03 - Last mean reward per episode: -34.54
--------------------------------------
| reference_Q_mean        | 0.0133   |
| reference_Q_std         | 0.256    |
| reference_action_mean   | 0.0279   |
| reference_action_std    | 0.15     |
| reference_actor_Q_mean  | 0.0504   |
| reference_actor_Q_std   | 0.26     |
| rollout/Q_mean          | 0.0767   |
| rollout/actions_mean    | -0.0254  |
| rollout/actions_std     | 0.628    |
| rollout/episode_steps   | 986      |
| rollout/episodes        | 70       |
| rollout/return          | -34.5    |
| rollout/return_history  | -34.5    |
| total/duration          | 174      |
| total/episodes          | 70       |
| total/epochs            | 1        |
| total/steps             | 69998    |
| total/steps_per_second  | 401      |
| train/loss_actor        | -0.256   |
| train/loss_critic       | 0.0113   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 71000
Best mean reward: -24.03 - Last mean reward per episode: -34.75
Num timesteps: 72000
Best mean reward: -24.03 - Last mean reward per episode: -33.17
Num timesteps: 73000
Best mean reward: -24.03 - Last mean reward per episode: -32.90
Num timesteps: 74000
Best mean reward: -24.03 - Last mean reward per episode: -32.93
Num timesteps: 75000
Best mean reward: -24.03 - Last mean reward per episode: -33.59
Num timesteps: 76000
Best mean reward: -24.03 - Last mean reward per episode: -33.77
Num timesteps: 77000
Best mean reward: -24.03 - Last mean reward per episode: -33.98
Num timesteps: 78000
Best mean reward: -24.03 - Last mean reward per episode: -34.05
Num timesteps: 79000
Best mean reward: -24.03 - Last mean reward per episode: -33.82
Num timesteps: 80000
Best mean reward: -24.03 - Last mean reward per episode: -34.00
--------------------------------------
| reference_Q_mean        | 0.0601   |
| reference_Q_std         | 0.28     |
| reference_action_mean   | -0.1     |
| reference_action_std    | 0.179    |
| reference_actor_Q_mean  | 0.0989   |
| reference_actor_Q_std   | 0.284    |
| rollout/Q_mean          | 0.107    |
| rollout/actions_mean    | -0.0393  |
| rollout/actions_std     | 0.627    |
| rollout/episode_steps   | 988      |
| rollout/episodes        | 81       |
| rollout/return          | -34      |
| rollout/return_history  | -34      |
| total/duration          | 198      |
| total/episodes          | 81       |
| total/epochs            | 1        |
| total/steps             | 79998    |
| total/steps_per_second  | 404      |
| train/loss_actor        | -0.259   |
| train/loss_critic       | 0.00929  |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 81000
Best mean reward: -24.03 - Last mean reward per episode: -32.51
Num timesteps: 82000
Best mean reward: -24.03 - Last mean reward per episode: -31.14
Num timesteps: 83000
Best mean reward: -24.03 - Last mean reward per episode: -31.32
Num timesteps: 84000
Best mean reward: -24.03 - Last mean reward per episode: -30.15
Num timesteps: 85000
Best mean reward: -24.03 - Last mean reward per episode: -30.68
Num timesteps: 86000
Best mean reward: -24.03 - Last mean reward per episode: -29.46
Num timesteps: 87000
Best mean reward: -24.03 - Last mean reward per episode: -28.16
Num timesteps: 88000
Best mean reward: -24.03 - Last mean reward per episode: -26.84
Num timesteps: 89000
Best mean reward: -24.03 - Last mean reward per episode: -26.17
Num timesteps: 90000
Best mean reward: -24.03 - Last mean reward per episode: -26.21
--------------------------------------
| reference_Q_mean        | 0.016    |
| reference_Q_std         | 0.266    |
| reference_action_mean   | -0.168   |
| reference_action_std    | 0.235    |
| reference_actor_Q_mean  | 0.0448   |
| reference_actor_Q_std   | 0.266    |
| rollout/Q_mean          | 0.286    |
| rollout/actions_mean    | -0.0547  |
| rollout/actions_std     | 0.627    |
| rollout/episode_steps   | 957      |
| rollout/episodes        | 94       |
| rollout/return          | -26.2    |
| rollout/return_history  | -26.2    |
| total/duration          | 222      |
| total/episodes          | 94       |
| total/epochs            | 1        |
| total/steps             | 89998    |
| total/steps_per_second  | 405      |
| train/loss_actor        | -0.566   |
| train/loss_critic       | 0.0391   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 91000
Best mean reward: -24.03 - Last mean reward per episode: -26.74
Num timesteps: 92000
Best mean reward: -24.03 - Last mean reward per episode: -25.75
Num timesteps: 93000
Best mean reward: -24.03 - Last mean reward per episode: -25.77
Num timesteps: 94000
Best mean reward: -24.03 - Last mean reward per episode: -24.74
Num timesteps: 95000
Best mean reward: -24.03 - Last mean reward per episode: -21.76
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 96000
Best mean reward: -21.76 - Last mean reward per episode: -20.83
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 97000
Best mean reward: -20.83 - Last mean reward per episode: -20.67
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 98000
Best mean reward: -20.67 - Last mean reward per episode: -18.53
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 99000
Best mean reward: -18.53 - Last mean reward per episode: -17.24
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 100000
Best mean reward: -17.24 - Last mean reward per episode: -14.60
Saving new best model to ./modelos/DDPG/mountain-3.pkl
--------------------------------------
| reference_Q_mean        | 0.0193   |
| reference_Q_std         | 0.229    |
| reference_action_mean   | 0.248    |
| reference_action_std    | 0.0932   |
| reference_actor_Q_mean  | 0.0662   |
| reference_actor_Q_std   | 0.224    |
| rollout/Q_mean          | 0.525    |
| rollout/actions_mean    | -0.0342  |
| rollout/actions_std     | 0.636    |
| rollout/episode_steps   | 914      |
| rollout/episodes        | 109      |
| rollout/return          | -17      |
| rollout/return_history  | -14.6    |
| total/duration          | 247      |
| total/episodes          | 109      |
| total/epochs            | 1        |
| total/steps             | 99998    |
| total/steps_per_second  | 405      |
| train/loss_actor        | -1.09    |
| train/loss_critic       | 0.0861   |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 101000
Best mean reward: -14.60 - Last mean reward per episode: -10.72
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 102000
Best mean reward: -10.72 - Last mean reward per episode: -8.64
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 103000
Best mean reward: -8.64 - Last mean reward per episode: -8.30
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 104000
Best mean reward: -8.30 - Last mean reward per episode: -4.57
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 105000
Best mean reward: -4.57 - Last mean reward per episode: -0.88
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 106000
Best mean reward: -0.88 - Last mean reward per episode: -0.15
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 107000
Best mean reward: -0.15 - Last mean reward per episode: 0.36
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 108000
Best mean reward: 0.36 - Last mean reward per episode: 2.65
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 109000
Best mean reward: 2.65 - Last mean reward per episode: 3.26
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 110000
Best mean reward: 3.26 - Last mean reward per episode: 5.58
Saving new best model to ./modelos/DDPG/mountain-3.pkl
--------------------------------------
| reference_Q_mean        | 0.193    |
| reference_Q_std         | 0.354    |
| reference_action_mean   | -0.444   |
| reference_action_std    | 0.295    |
| reference_actor_Q_mean  | 0.25     |
| reference_actor_Q_std   | 0.336    |
| rollout/Q_mean          | 0.937    |
| rollout/actions_mean    | -0.0486  |
| rollout/actions_std     | 0.644    |
| rollout/episode_steps   | 844      |
| rollout/episodes        | 130      |
| rollout/return          | -3.68    |
| rollout/return_history  | 5.58     |
| total/duration          | 270      |
| total/episodes          | 130      |
| total/epochs            | 1        |
| total/steps             | 109998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -2.51    |
| train/loss_critic       | 0.958    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 111000
Best mean reward: 5.58 - Last mean reward per episode: 10.21
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 112000
Best mean reward: 10.21 - Last mean reward per episode: 12.52
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 113000
Best mean reward: 12.52 - Last mean reward per episode: 14.53
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 114000
Best mean reward: 14.53 - Last mean reward per episode: 19.75
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 115000
Best mean reward: 19.75 - Last mean reward per episode: 24.74
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 116000
Best mean reward: 24.74 - Last mean reward per episode: 28.27
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 117000
Best mean reward: 28.27 - Last mean reward per episode: 33.34
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 118000
Best mean reward: 33.34 - Last mean reward per episode: 38.11
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 119000
Best mean reward: 38.11 - Last mean reward per episode: 43.45
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 120000
Best mean reward: 43.45 - Last mean reward per episode: 48.82
Saving new best model to ./modelos/DDPG/mountain-3.pkl
--------------------------------------
| reference_Q_mean        | 0.0858   |
| reference_Q_std         | 0.414    |
| reference_action_mean   | 0.358    |
| reference_action_std    | 0.511    |
| reference_actor_Q_mean  | 0.148    |
| reference_actor_Q_std   | 0.4      |
| rollout/Q_mean          | 1.59     |
| rollout/actions_mean    | -0.023   |
| rollout/actions_std     | 0.648    |
| rollout/episode_steps   | 722      |
| rollout/episodes        | 166      |
| rollout/return          | 16       |
| rollout/return_history  | 48.8     |
| total/duration          | 294      |
| total/episodes          | 166      |
| total/epochs            | 1        |
| total/steps             | 119998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -4.9     |
| train/loss_critic       | 0.327    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 121000
Best mean reward: 48.82 - Last mean reward per episode: 53.07
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 122000
Best mean reward: 53.07 - Last mean reward per episode: 56.91
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 123000
Best mean reward: 56.91 - Last mean reward per episode: 61.51
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 124000
Best mean reward: 61.51 - Last mean reward per episode: 63.89
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 125000
Best mean reward: 63.89 - Last mean reward per episode: 67.39
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 126000
Best mean reward: 67.39 - Last mean reward per episode: 68.76
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 127000
Best mean reward: 68.76 - Last mean reward per episode: 69.98
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 128000
Best mean reward: 69.98 - Last mean reward per episode: 71.55
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 129000
Best mean reward: 71.55 - Last mean reward per episode: 72.09
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 130000
Best mean reward: 72.09 - Last mean reward per episode: 73.50
Saving new best model to ./modelos/DDPG/mountain-3.pkl
--------------------------------------
| reference_Q_mean        | 0.573    |
| reference_Q_std         | 0.897    |
| reference_action_mean   | 0.921    |
| reference_action_std    | 0.196    |
| reference_actor_Q_mean  | 0.741    |
| reference_actor_Q_std   | 1.11     |
| rollout/Q_mean          | 2.35     |
| rollout/actions_mean    | 0.00543  |
| rollout/actions_std     | 0.65     |
| rollout/episode_steps   | 673      |
| rollout/episodes        | 193      |
| rollout/return          | 25.5     |
| rollout/return_history  | 73.5     |
| total/duration          | 320      |
| total/episodes          | 193      |
| total/epochs            | 1        |
| total/steps             | 129998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -7.93    |
| train/loss_critic       | 0.359    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 131000
Best mean reward: 73.50 - Last mean reward per episode: 76.33
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 132000
Best mean reward: 76.33 - Last mean reward per episode: 77.85
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 133000
Best mean reward: 77.85 - Last mean reward per episode: 79.47
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 134000
Best mean reward: 79.47 - Last mean reward per episode: 81.14
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 135000
Best mean reward: 81.14 - Last mean reward per episode: 81.06
Num timesteps: 136000
Best mean reward: 81.14 - Last mean reward per episode: 82.67
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 137000
Best mean reward: 82.67 - Last mean reward per episode: 82.41
Num timesteps: 138000
Best mean reward: 82.67 - Last mean reward per episode: 84.74
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 139000
Best mean reward: 84.74 - Last mean reward per episode: 85.44
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 140000
Best mean reward: 85.44 - Last mean reward per episode: 85.14
--------------------------------------
| reference_Q_mean        | 1.15     |
| reference_Q_std         | 2.78     |
| reference_action_mean   | 0.899    |
| reference_action_std    | 0.248    |
| reference_actor_Q_mean  | 1.57     |
| reference_actor_Q_std   | 3.22     |
| rollout/Q_mean          | 3.39     |
| rollout/actions_mean    | 0.0479   |
| rollout/actions_std     | 0.66     |
| rollout/episode_steps   | 593      |
| rollout/episodes        | 236      |
| rollout/return          | 36.3     |
| rollout/return_history  | 85.1     |
| total/duration          | 343      |
| total/episodes          | 236      |
| total/epochs            | 1        |
| total/steps             | 139998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -13.1    |
| train/loss_critic       | 0.762    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 141000
Best mean reward: 85.44 - Last mean reward per episode: 85.25
Num timesteps: 142000
Best mean reward: 85.44 - Last mean reward per episode: 84.72
Num timesteps: 143000
Best mean reward: 85.44 - Last mean reward per episode: 84.62
Num timesteps: 144000
Best mean reward: 85.44 - Last mean reward per episode: 84.46
Num timesteps: 145000
Best mean reward: 85.44 - Last mean reward per episode: 82.62
Num timesteps: 146000
Best mean reward: 85.44 - Last mean reward per episode: 82.46
Num timesteps: 147000
Best mean reward: 85.44 - Last mean reward per episode: 81.10
Num timesteps: 148000
Best mean reward: 85.44 - Last mean reward per episode: 79.28
Num timesteps: 149000
Best mean reward: 85.44 - Last mean reward per episode: 78.60
Num timesteps: 150000
Best mean reward: 85.44 - Last mean reward per episode: 76.77
--------------------------------------
| reference_Q_mean        | 1.94     |
| reference_Q_std         | 5.71     |
| reference_action_mean   | 1        |
| reference_action_std    | 0.000376 |
| reference_actor_Q_mean  | 2.53     |
| reference_actor_Q_std   | 6.57     |
| rollout/Q_mean          | 3.64     |
| rollout/actions_mean    | 0.0968   |
| rollout/actions_std     | 0.671    |
| rollout/episode_steps   | 588      |
| rollout/episodes        | 254      |
| rollout/return          | 36.5     |
| rollout/return_history  | 76.8     |
| total/duration          | 369      |
| total/episodes          | 254      |
| total/epochs            | 1        |
| total/steps             | 149998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -15.2    |
| train/loss_critic       | 0.913    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 151000
Best mean reward: 85.44 - Last mean reward per episode: 74.90
Num timesteps: 152000
Best mean reward: 85.44 - Last mean reward per episode: 73.04
Num timesteps: 153000
Best mean reward: 85.44 - Last mean reward per episode: 71.58
Num timesteps: 154000
Best mean reward: 85.44 - Last mean reward per episode: 70.32
Num timesteps: 155000
Best mean reward: 85.44 - Last mean reward per episode: 68.40
Num timesteps: 156000
Best mean reward: 85.44 - Last mean reward per episode: 66.50
Num timesteps: 157000
Best mean reward: 85.44 - Last mean reward per episode: 66.07
Num timesteps: 158000
Best mean reward: 85.44 - Last mean reward per episode: 64.41
Num timesteps: 159000
Best mean reward: 85.44 - Last mean reward per episode: 63.12
Num timesteps: 160000
Best mean reward: 85.44 - Last mean reward per episode: 61.76
--------------------------------------
| reference_Q_mean        | 3.59     |
| reference_Q_std         | 8.27     |
| reference_action_mean   | 1        |
| reference_action_std    | 1.39e-05 |
| reference_actor_Q_mean  | 4.44     |
| reference_actor_Q_std   | 9.22     |
| rollout/Q_mean          | 3.61     |
| rollout/actions_mean    | 0.131    |
| rollout/actions_std     | 0.677    |
| rollout/episode_steps   | 603      |
| rollout/episodes        | 264      |
| rollout/return          | 32.8     |
| rollout/return_history  | 61.8     |
| total/duration          | 393      |
| total/episodes          | 264      |
| total/epochs            | 1        |
| total/steps             | 159998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -14.5    |
| train/loss_critic       | 1.35     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 161000
Best mean reward: 85.44 - Last mean reward per episode: 60.12
Num timesteps: 162000
Best mean reward: 85.44 - Last mean reward per episode: 58.00
Num timesteps: 163000
Best mean reward: 85.44 - Last mean reward per episode: 56.20
Num timesteps: 164000
Best mean reward: 85.44 - Last mean reward per episode: 56.21
Num timesteps: 165000
Best mean reward: 85.44 - Last mean reward per episode: 54.14
Num timesteps: 166000
Best mean reward: 85.44 - Last mean reward per episode: 52.48
Num timesteps: 167000
Best mean reward: 85.44 - Last mean reward per episode: 52.03
Num timesteps: 168000
Best mean reward: 85.44 - Last mean reward per episode: 52.00
Num timesteps: 169000
Best mean reward: 85.44 - Last mean reward per episode: 50.13
Num timesteps: 170000
Best mean reward: 85.44 - Last mean reward per episode: 47.95
--------------------------------------
| reference_Q_mean        | 8.05     |
| reference_Q_std         | 14.6     |
| reference_action_mean   | 0.934    |
| reference_action_std    | 0.335    |
| reference_actor_Q_mean  | 9.15     |
| reference_actor_Q_std   | 15.8     |
| rollout/Q_mean          | 3.68     |
| rollout/actions_mean    | 0.17     |
| rollout/actions_std     | 0.682    |
| rollout/episode_steps   | 610      |
| rollout/episodes        | 278      |
| rollout/return          | 30.7     |
| rollout/return_history  | 47.9     |
| total/duration          | 417      |
| total/episodes          | 278      |
| total/epochs            | 1        |
| total/steps             | 169998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -13.3    |
| train/loss_critic       | 1.76     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 171000
Best mean reward: 85.44 - Last mean reward per episode: 46.11
Num timesteps: 172000
Best mean reward: 85.44 - Last mean reward per episode: 44.29
Num timesteps: 173000
Best mean reward: 85.44 - Last mean reward per episode: 42.37
Num timesteps: 174000
Best mean reward: 85.44 - Last mean reward per episode: 40.52
Num timesteps: 175000
Best mean reward: 85.44 - Last mean reward per episode: 39.39
Num timesteps: 176000
Best mean reward: 85.44 - Last mean reward per episode: 39.38
Num timesteps: 177000
Best mean reward: 85.44 - Last mean reward per episode: 39.26
Num timesteps: 178000
Best mean reward: 85.44 - Last mean reward per episode: 39.63
Num timesteps: 179000
Best mean reward: 85.44 - Last mean reward per episode: 38.10
Num timesteps: 180000
Best mean reward: 85.44 - Last mean reward per episode: 36.52
--------------------------------------
| reference_Q_mean        | 14.5     |
| reference_Q_std         | 23       |
| reference_action_mean   | 0.831    |
| reference_action_std    | 0.495    |
| reference_actor_Q_mean  | 15.9     |
| reference_actor_Q_std   | 24.5     |
| rollout/Q_mean          | 3.7      |
| rollout/actions_mean    | 0.206    |
| rollout/actions_std     | 0.684    |
| rollout/episode_steps   | 616      |
| rollout/episodes        | 291      |
| rollout/return          | 28.9     |
| rollout/return_history  | 36.5     |
| total/duration          | 442      |
| total/episodes          | 291      |
| total/epochs            | 1        |
| total/steps             | 179998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -10.2    |
| train/loss_critic       | 0.575    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 181000
Best mean reward: 85.44 - Last mean reward per episode: 35.98
Num timesteps: 182000
Best mean reward: 85.44 - Last mean reward per episode: 36.75
Num timesteps: 183000
Best mean reward: 85.44 - Last mean reward per episode: 36.95
Num timesteps: 184000
Best mean reward: 85.44 - Last mean reward per episode: 37.49
Num timesteps: 185000
Best mean reward: 85.44 - Last mean reward per episode: 37.80
Num timesteps: 186000
Best mean reward: 85.44 - Last mean reward per episode: 38.73
Num timesteps: 187000
Best mean reward: 85.44 - Last mean reward per episode: 41.55
Num timesteps: 188000
Best mean reward: 85.44 - Last mean reward per episode: 58.91
Num timesteps: 189000
Best mean reward: 85.44 - Last mean reward per episode: 71.23
Num timesteps: 190000
Best mean reward: 85.44 - Last mean reward per episode: 81.33
--------------------------------------
| reference_Q_mean        | 15.4     |
| reference_Q_std         | 22.2     |
| reference_action_mean   | -0.13    |
| reference_action_std    | 0.763    |
| reference_actor_Q_mean  | 16.3     |
| reference_actor_Q_std   | 22.4     |
| rollout/Q_mean          | 5.81     |
| rollout/actions_mean    | 0.218    |
| rollout/actions_std     | 0.689    |
| rollout/episode_steps   | 500      |
| rollout/episodes        | 380      |
| rollout/return          | 43.4     |
| rollout/return_history  | 81.3     |
| total/duration          | 467      |
| total/episodes          | 380      |
| total/epochs            | 1        |
| total/steps             | 189998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -15.5    |
| train/loss_critic       | 0.548    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 191000
Best mean reward: 85.44 - Last mean reward per episode: 87.58
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 192000
Best mean reward: 87.58 - Last mean reward per episode: 92.39
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 193000
Best mean reward: 92.39 - Last mean reward per episode: 92.86
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 194000
Best mean reward: 92.86 - Last mean reward per episode: 93.16
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 195000
Best mean reward: 93.16 - Last mean reward per episode: 93.35
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 196000
Best mean reward: 93.35 - Last mean reward per episode: 93.57
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 197000
Best mean reward: 93.57 - Last mean reward per episode: 93.74
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 198000
Best mean reward: 93.74 - Last mean reward per episode: 93.76
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 199000
Best mean reward: 93.76 - Last mean reward per episode: 93.97
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 200000
Best mean reward: 93.97 - Last mean reward per episode: 93.95
--------------------------------------
| reference_Q_mean        | 16.7     |
| reference_Q_std         | 20.9     |
| reference_action_mean   | -0.401   |
| reference_action_std    | 0.658    |
| reference_actor_Q_mean  | 17.4     |
| reference_actor_Q_std   | 20.8     |
| rollout/Q_mean          | 8.43     |
| rollout/actions_mean    | 0.216    |
| rollout/actions_std     | 0.694    |
| rollout/episode_steps   | 411      |
| rollout/episodes        | 487      |
| rollout/return          | 54.5     |
| rollout/return_history  | 93.9     |
| total/duration          | 490      |
| total/episodes          | 487      |
| total/epochs            | 1        |
| total/steps             | 199998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -27.1    |
| train/loss_critic       | 1.07     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 201000
Best mean reward: 93.97 - Last mean reward per episode: 93.95
Num timesteps: 202000
Best mean reward: 93.97 - Last mean reward per episode: 94.02
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 203000
Best mean reward: 94.02 - Last mean reward per episode: 93.93
Num timesteps: 204000
Best mean reward: 94.02 - Last mean reward per episode: 94.00
Num timesteps: 205000
Best mean reward: 94.02 - Last mean reward per episode: 94.02
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 206000
Best mean reward: 94.02 - Last mean reward per episode: 94.03
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 207000
Best mean reward: 94.03 - Last mean reward per episode: 94.03
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 208000
Best mean reward: 94.03 - Last mean reward per episode: 94.17
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 209000
Best mean reward: 94.17 - Last mean reward per episode: 94.17
Num timesteps: 210000
Best mean reward: 94.17 - Last mean reward per episode: 94.18
Saving new best model to ./modelos/DDPG/mountain-3.pkl
--------------------------------------
| reference_Q_mean        | 22.1     |
| reference_Q_std         | 21.7     |
| reference_action_mean   | -0.289   |
| reference_action_std    | 0.695    |
| reference_actor_Q_mean  | 23.3     |
| reference_actor_Q_std   | 22       |
| rollout/Q_mean          | 11.2     |
| rollout/actions_mean    | 0.211    |
| rollout/actions_std     | 0.702    |
| rollout/episode_steps   | 344      |
| rollout/episodes        | 611      |
| rollout/return          | 62.5     |
| rollout/return_history  | 94.2     |
| total/duration          | 513      |
| total/episodes          | 611      |
| total/epochs            | 1        |
| total/steps             | 209998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -43.1    |
| train/loss_critic       | 1.37     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 211000
Best mean reward: 94.18 - Last mean reward per episode: 94.29
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 212000
Best mean reward: 94.29 - Last mean reward per episode: 94.30
Saving new best model to ./modelos/DDPG/mountain-3.pkl
Num timesteps: 213000
Best mean reward: 94.30 - Last mean reward per episode: 94.07
Num timesteps: 214000
Best mean reward: 94.30 - Last mean reward per episode: 94.03
Num timesteps: 215000
Best mean reward: 94.30 - Last mean reward per episode: 94.07
Num timesteps: 216000
Best mean reward: 94.30 - Last mean reward per episode: 94.09
Num timesteps: 217000
Best mean reward: 94.30 - Last mean reward per episode: 94.17
Num timesteps: 218000
Best mean reward: 94.30 - Last mean reward per episode: 94.17
Num timesteps: 219000
Best mean reward: 94.30 - Last mean reward per episode: 94.01
Num timesteps: 220000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
--------------------------------------
| reference_Q_mean        | 29.2     |
| reference_Q_std         | 22.6     |
| reference_action_mean   | -0.531   |
| reference_action_std    | 0.611    |
| reference_actor_Q_mean  | 30.7     |
| reference_actor_Q_std   | 23.1     |
| rollout/Q_mean          | 13.8     |
| rollout/actions_mean    | 0.202    |
| rollout/actions_std     | 0.71     |
| rollout/episode_steps   | 308      |
| rollout/episodes        | 713      |
| rollout/return          | 66.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 537      |
| total/episodes          | 713      |
| total/epochs            | 1        |
| total/steps             | 219998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -56.6    |
| train/loss_critic       | 1.2      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 221000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 222000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 223000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 224000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
Num timesteps: 225000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 226000
Best mean reward: 94.30 - Last mean reward per episode: 93.15
Num timesteps: 227000
Best mean reward: 94.30 - Last mean reward per episode: 93.03
Num timesteps: 228000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 229000
Best mean reward: 94.30 - Last mean reward per episode: 91.37
Num timesteps: 230000
Best mean reward: 94.30 - Last mean reward per episode: 91.39
--------------------------------------
| reference_Q_mean        | 38.5     |
| reference_Q_std         | 22.4     |
| reference_action_mean   | -0.829   |
| reference_action_std    | 0.462    |
| reference_actor_Q_mean  | 40.1     |
| reference_actor_Q_std   | 22.3     |
| rollout/Q_mean          | 16.2     |
| rollout/actions_mean    | 0.194    |
| rollout/actions_std     | 0.715    |
| rollout/episode_steps   | 285      |
| rollout/episodes        | 807      |
| rollout/return          | 69.9     |
| rollout/return_history  | 91.4     |
| total/duration          | 561      |
| total/episodes          | 807      |
| total/epochs            | 1        |
| total/steps             | 229998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -68.2    |
| train/loss_critic       | 1.35     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 231000
Best mean reward: 94.30 - Last mean reward per episode: 91.95
Num timesteps: 232000
Best mean reward: 94.30 - Last mean reward per episode: 91.77
Num timesteps: 233000
Best mean reward: 94.30 - Last mean reward per episode: 91.65
Num timesteps: 234000
Best mean reward: 94.30 - Last mean reward per episode: 89.66
Num timesteps: 235000
Best mean reward: 94.30 - Last mean reward per episode: 89.51
Num timesteps: 236000
Best mean reward: 94.30 - Last mean reward per episode: 89.44
Num timesteps: 237000
Best mean reward: 94.30 - Last mean reward per episode: 89.64
Num timesteps: 238000
Best mean reward: 94.30 - Last mean reward per episode: 89.61
Num timesteps: 239000
Best mean reward: 94.30 - Last mean reward per episode: 91.30
Num timesteps: 240000
Best mean reward: 94.30 - Last mean reward per episode: 91.19
--------------------------------------
| reference_Q_mean        | 44.2     |
| reference_Q_std         | 17.8     |
| reference_action_mean   | -0.862   |
| reference_action_std    | 0.409    |
| reference_actor_Q_mean  | 46.4     |
| reference_actor_Q_std   | 18.7     |
| rollout/Q_mean          | 18.5     |
| rollout/actions_mean    | 0.179    |
| rollout/actions_std     | 0.727    |
| rollout/episode_steps   | 264      |
| rollout/episodes        | 909      |
| rollout/return          | 72.2     |
| rollout/return_history  | 91.2     |
| total/duration          | 587      |
| total/episodes          | 909      |
| total/epochs            | 1        |
| total/steps             | 239998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 1.12     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 241000
Best mean reward: 94.30 - Last mean reward per episode: 91.08
Num timesteps: 242000
Best mean reward: 94.30 - Last mean reward per episode: 91.27
Num timesteps: 243000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 244000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 245000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 246000
Best mean reward: 94.30 - Last mean reward per episode: 91.51
Num timesteps: 247000
Best mean reward: 94.30 - Last mean reward per episode: 91.49
Num timesteps: 248000
Best mean reward: 94.30 - Last mean reward per episode: 91.55
Num timesteps: 249000
Best mean reward: 94.30 - Last mean reward per episode: 91.70
Num timesteps: 250000
Best mean reward: 94.30 - Last mean reward per episode: 91.71
--------------------------------------
| reference_Q_mean        | 48       |
| reference_Q_std         | 13.9     |
| reference_action_mean   | -0.69    |
| reference_action_std    | 0.662    |
| reference_actor_Q_mean  | 50       |
| reference_actor_Q_std   | 14.7     |
| rollout/Q_mean          | 20.8     |
| rollout/actions_mean    | 0.166    |
| rollout/actions_std     | 0.736    |
| rollout/episode_steps   | 245      |
| rollout/episodes        | 1.02e+03 |
| rollout/return          | 74.4     |
| rollout/return_history  | 91.7     |
| total/duration          | 610      |
| total/episodes          | 1.02e+03 |
| total/epochs            | 1        |
| total/steps             | 249998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -71.8    |
| train/loss_critic       | 1.27     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 251000
Best mean reward: 94.30 - Last mean reward per episode: 91.65
Num timesteps: 252000
Best mean reward: 94.30 - Last mean reward per episode: 91.67
Num timesteps: 253000
Best mean reward: 94.30 - Last mean reward per episode: 91.42
Num timesteps: 254000
Best mean reward: 94.30 - Last mean reward per episode: 91.44
Num timesteps: 255000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 256000
Best mean reward: 94.30 - Last mean reward per episode: 93.06
Num timesteps: 257000
Best mean reward: 94.30 - Last mean reward per episode: 92.52
Num timesteps: 258000
Best mean reward: 94.30 - Last mean reward per episode: 92.34
Num timesteps: 259000
Best mean reward: 94.30 - Last mean reward per episode: 92.35
Num timesteps: 260000
Best mean reward: 94.30 - Last mean reward per episode: 92.35
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 11.9     |
| reference_action_mean   | -0.63    |
| reference_action_std    | 0.693    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 12.1     |
| rollout/Q_mean          | 22.9     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.743    |
| rollout/episode_steps   | 233      |
| rollout/episodes        | 1.12e+03 |
| rollout/return          | 75.9     |
| rollout/return_history  | 92.3     |
| total/duration          | 635      |
| total/episodes          | 1.12e+03 |
| total/epochs            | 1        |
| total/steps             | 259998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -72.2    |
| train/loss_critic       | 0.83     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 261000
Best mean reward: 94.30 - Last mean reward per episode: 92.46
Num timesteps: 262000
Best mean reward: 94.30 - Last mean reward per episode: 92.68
Num timesteps: 263000
Best mean reward: 94.30 - Last mean reward per episode: 92.78
Num timesteps: 264000
Best mean reward: 94.30 - Last mean reward per episode: 92.38
Num timesteps: 265000
Best mean reward: 94.30 - Last mean reward per episode: 92.65
Num timesteps: 266000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 267000
Best mean reward: 94.30 - Last mean reward per episode: 93.30
Num timesteps: 268000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 269000
Best mean reward: 94.30 - Last mean reward per episode: 93.43
Num timesteps: 270000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 10.4     |
| reference_action_mean   | -0.502   |
| reference_action_std    | 0.799    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 10.2     |
| rollout/Q_mean          | 24.8     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.749    |
| rollout/episode_steps   | 220      |
| rollout/episodes        | 1.23e+03 |
| rollout/return          | 77.5     |
| rollout/return_history  | 93.3     |
| total/duration          | 659      |
| total/episodes          | 1.23e+03 |
| total/epochs            | 1        |
| total/steps             | 269998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -72.7    |
| train/loss_critic       | 0.714    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 271000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 272000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 273000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 274000
Best mean reward: 94.30 - Last mean reward per episode: 93.83
Num timesteps: 275000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 276000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 277000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 278000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 279000
Best mean reward: 94.30 - Last mean reward per episode: 93.17
Num timesteps: 280000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 9.86     |
| reference_action_mean   | -0.655   |
| reference_action_std    | 0.66     |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 9.78     |
| rollout/Q_mean          | 26.6     |
| rollout/actions_mean    | 0.141    |
| rollout/actions_std     | 0.753    |
| rollout/episode_steps   | 210      |
| rollout/episodes        | 1.33e+03 |
| rollout/return          | 78.7     |
| rollout/return_history  | 93.2     |
| total/duration          | 683      |
| total/episodes          | 1.33e+03 |
| total/epochs            | 1        |
| total/steps             | 279998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -72.9    |
| train/loss_critic       | 0.441    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 281000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 282000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 283000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 284000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 285000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 286000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
Num timesteps: 287000
Best mean reward: 94.30 - Last mean reward per episode: 93.84
Num timesteps: 288000
Best mean reward: 94.30 - Last mean reward per episode: 93.88
Num timesteps: 289000
Best mean reward: 94.30 - Last mean reward per episode: 93.91
Num timesteps: 290000
Best mean reward: 94.30 - Last mean reward per episode: 93.92
--------------------------------------
| reference_Q_mean        | 53       |
| reference_Q_std         | 7.73     |
| reference_action_mean   | -0.887   |
| reference_action_std    | 0.384    |
| reference_actor_Q_mean  | 54.4     |
| reference_actor_Q_std   | 8.18     |
| rollout/Q_mean          | 28.2     |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.756    |
| rollout/episode_steps   | 199      |
| rollout/episodes        | 1.46e+03 |
| rollout/return          | 80       |
| rollout/return_history  | 93.9     |
| total/duration          | 708      |
| total/episodes          | 1.46e+03 |
| total/epochs            | 1        |
| total/steps             | 289998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -73.1    |
| train/loss_critic       | 0.283    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 291000
Best mean reward: 94.30 - Last mean reward per episode: 93.97
Num timesteps: 292000
Best mean reward: 94.30 - Last mean reward per episode: 94.03
Num timesteps: 293000
Best mean reward: 94.30 - Last mean reward per episode: 94.07
Num timesteps: 294000
Best mean reward: 94.30 - Last mean reward per episode: 94.15
Num timesteps: 295000
Best mean reward: 94.30 - Last mean reward per episode: 94.14
Num timesteps: 296000
Best mean reward: 94.30 - Last mean reward per episode: 94.15
Num timesteps: 297000
Best mean reward: 94.30 - Last mean reward per episode: 93.86
Num timesteps: 298000
Best mean reward: 94.30 - Last mean reward per episode: 93.91
Num timesteps: 299000
Best mean reward: 94.30 - Last mean reward per episode: 93.89
Num timesteps: 300000
Best mean reward: 94.30 - Last mean reward per episode: 93.79
--------------------------------------
| reference_Q_mean        | 55.9     |
| reference_Q_std         | 6.29     |
| reference_action_mean   | -0.707   |
| reference_action_std    | 0.519    |
| reference_actor_Q_mean  | 57.3     |
| reference_actor_Q_std   | 6.53     |
| rollout/Q_mean          | 29.7     |
| rollout/actions_mean    | 0.141    |
| rollout/actions_std     | 0.757    |
| rollout/episode_steps   | 193      |
| rollout/episodes        | 1.55e+03 |
| rollout/return          | 80.9     |
| rollout/return_history  | 93.8     |
| total/duration          | 732      |
| total/episodes          | 1.55e+03 |
| total/epochs            | 1        |
| total/steps             | 299998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -72.9    |
| train/loss_critic       | 0.23     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 301000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 302000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 303000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 304000
Best mean reward: 94.30 - Last mean reward per episode: 93.71
Num timesteps: 305000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 306000
Best mean reward: 94.30 - Last mean reward per episode: 92.51
Num timesteps: 307000
Best mean reward: 94.30 - Last mean reward per episode: 91.31
Num timesteps: 308000
Best mean reward: 94.30 - Last mean reward per episode: 91.57
Num timesteps: 309000
Best mean reward: 94.30 - Last mean reward per episode: 91.60
Num timesteps: 310000
Best mean reward: 94.30 - Last mean reward per episode: 91.68
--------------------------------------
| reference_Q_mean        | 56.9     |
| reference_Q_std         | 5.44     |
| reference_action_mean   | -0.497   |
| reference_action_std    | 0.669    |
| reference_actor_Q_mean  | 58.3     |
| reference_actor_Q_std   | 5.67     |
| rollout/Q_mean          | 31       |
| rollout/actions_mean    | 0.139    |
| rollout/actions_std     | 0.758    |
| rollout/episode_steps   | 188      |
| rollout/episodes        | 1.65e+03 |
| rollout/return          | 81.5     |
| rollout/return_history  | 91.7     |
| total/duration          | 757      |
| total/episodes          | 1.65e+03 |
| total/epochs            | 1        |
| total/steps             | 309998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -72.2    |
| train/loss_critic       | 0.171    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 311000
Best mean reward: 94.30 - Last mean reward per episode: 91.69
Num timesteps: 312000
Best mean reward: 94.30 - Last mean reward per episode: 91.62
Num timesteps: 313000
Best mean reward: 94.30 - Last mean reward per episode: 91.60
Num timesteps: 314000
Best mean reward: 94.30 - Last mean reward per episode: 91.55
Num timesteps: 315000
Best mean reward: 94.30 - Last mean reward per episode: 91.52
Num timesteps: 316000
Best mean reward: 94.30 - Last mean reward per episode: 92.52
Num timesteps: 317000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 318000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 319000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 320000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
--------------------------------------
| reference_Q_mean        | 57.6     |
| reference_Q_std         | 4.78     |
| reference_action_mean   | -0.57    |
| reference_action_std    | 0.677    |
| reference_actor_Q_mean  | 58.3     |
| reference_actor_Q_std   | 4.97     |
| rollout/Q_mean          | 32.3     |
| rollout/actions_mean    | 0.142    |
| rollout/actions_std     | 0.758    |
| rollout/episode_steps   | 183      |
| rollout/episodes        | 1.75e+03 |
| rollout/return          | 82.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 781      |
| total/episodes          | 1.75e+03 |
| total/epochs            | 1        |
| total/steps             | 319998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -71.5    |
| train/loss_critic       | 0.178    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 321000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 322000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 323000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 324000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 325000
Best mean reward: 94.30 - Last mean reward per episode: 92.51
Num timesteps: 326000
Best mean reward: 94.30 - Last mean reward per episode: 92.77
Num timesteps: 327000
Best mean reward: 94.30 - Last mean reward per episode: 92.77
Num timesteps: 328000
Best mean reward: 94.30 - Last mean reward per episode: 91.11
Num timesteps: 329000
Best mean reward: 94.30 - Last mean reward per episode: 91.15
Num timesteps: 330000
Best mean reward: 94.30 - Last mean reward per episode: 91.25
--------------------------------------
| reference_Q_mean        | 56.6     |
| reference_Q_std         | 4.47     |
| reference_action_mean   | -0.0697  |
| reference_action_std    | 0.855    |
| reference_actor_Q_mean  | 57.2     |
| reference_actor_Q_std   | 4.36     |
| rollout/Q_mean          | 33.4     |
| rollout/actions_mean    | 0.142    |
| rollout/actions_std     | 0.76     |
| rollout/episode_steps   | 178      |
| rollout/episodes        | 1.85e+03 |
| rollout/return          | 82.7     |
| rollout/return_history  | 91.3     |
| total/duration          | 804      |
| total/episodes          | 1.85e+03 |
| total/epochs            | 1        |
| total/steps             | 329998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 2.26     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 331000
Best mean reward: 94.30 - Last mean reward per episode: 91.24
Num timesteps: 332000
Best mean reward: 94.30 - Last mean reward per episode: 91.12
Num timesteps: 333000
Best mean reward: 94.30 - Last mean reward per episode: 91.10
Num timesteps: 334000
Best mean reward: 94.30 - Last mean reward per episode: 91.08
Num timesteps: 335000
Best mean reward: 94.30 - Last mean reward per episode: 92.17
Num timesteps: 336000
Best mean reward: 94.30 - Last mean reward per episode: 92.29
Num timesteps: 337000
Best mean reward: 94.30 - Last mean reward per episode: 92.30
Num timesteps: 338000
Best mean reward: 94.30 - Last mean reward per episode: 94.00
Num timesteps: 339000
Best mean reward: 94.30 - Last mean reward per episode: 93.96
Num timesteps: 340000
Best mean reward: 94.30 - Last mean reward per episode: 93.96
--------------------------------------
| reference_Q_mean        | 56       |
| reference_Q_std         | 4.23     |
| reference_action_mean   | -0.0711  |
| reference_action_std    | 0.907    |
| reference_actor_Q_mean  | 56.8     |
| reference_actor_Q_std   | 4.39     |
| rollout/Q_mean          | 34.5     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.76     |
| rollout/episode_steps   | 174      |
| rollout/episodes        | 1.96e+03 |
| rollout/return          | 83.3     |
| rollout/return_history  | 94       |
| total/duration          | 827      |
| total/episodes          | 1.96e+03 |
| total/epochs            | 1        |
| total/steps             | 339998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.958    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 341000
Best mean reward: 94.30 - Last mean reward per episode: 94.05
Num timesteps: 342000
Best mean reward: 94.30 - Last mean reward per episode: 94.06
Num timesteps: 343000
Best mean reward: 94.30 - Last mean reward per episode: 94.03
Num timesteps: 344000
Best mean reward: 94.30 - Last mean reward per episode: 94.10
Num timesteps: 345000
Best mean reward: 94.30 - Last mean reward per episode: 94.02
Num timesteps: 346000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 347000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 348000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 349000
Best mean reward: 94.30 - Last mean reward per episode: 92.99
Num timesteps: 350000
Best mean reward: 94.30 - Last mean reward per episode: 92.70
--------------------------------------
| reference_Q_mean        | 56       |
| reference_Q_std         | 4.18     |
| reference_action_mean   | 0.77     |
| reference_action_std    | 0.593    |
| reference_actor_Q_mean  | 56.5     |
| reference_actor_Q_std   | 4.06     |
| rollout/Q_mean          | 35.5     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.76     |
| rollout/episode_steps   | 171      |
| rollout/episodes        | 2.05e+03 |
| rollout/return          | 83.7     |
| rollout/return_history  | 92.7     |
| total/duration          | 851      |
| total/episodes          | 2.05e+03 |
| total/epochs            | 1        |
| total/steps             | 349998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.144    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 351000
Best mean reward: 94.30 - Last mean reward per episode: 92.46
Num timesteps: 352000
Best mean reward: 94.30 - Last mean reward per episode: 92.23
Num timesteps: 353000
Best mean reward: 94.30 - Last mean reward per episode: 92.07
Num timesteps: 354000
Best mean reward: 94.30 - Last mean reward per episode: 91.08
Num timesteps: 355000
Best mean reward: 94.30 - Last mean reward per episode: 90.89
Num timesteps: 356000
Best mean reward: 94.30 - Last mean reward per episode: 90.76
Num timesteps: 357000
Best mean reward: 94.30 - Last mean reward per episode: 90.39
Num timesteps: 358000
Best mean reward: 94.30 - Last mean reward per episode: 89.98
Num timesteps: 359000
Best mean reward: 94.30 - Last mean reward per episode: 89.64
Num timesteps: 360000
Best mean reward: 94.30 - Last mean reward per episode: 89.48
--------------------------------------
| reference_Q_mean        | 54.9     |
| reference_Q_std         | 4.33     |
| reference_action_mean   | 0.825    |
| reference_action_std    | 0.548    |
| reference_actor_Q_mean  | 55.5     |
| reference_actor_Q_std   | 4.35     |
| rollout/Q_mean          | 36.3     |
| rollout/actions_mean    | 0.166    |
| rollout/actions_std     | 0.759    |
| rollout/episode_steps   | 170      |
| rollout/episodes        | 2.11e+03 |
| rollout/return          | 83.9     |
| rollout/return_history  | 89.5     |
| total/duration          | 877      |
| total/episodes          | 2.11e+03 |
| total/epochs            | 1        |
| total/steps             | 359998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -68      |
| train/loss_critic       | 0.913    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 361000
Best mean reward: 94.30 - Last mean reward per episode: 89.46
Num timesteps: 362000
Best mean reward: 94.30 - Last mean reward per episode: 89.58
Num timesteps: 363000
Best mean reward: 94.30 - Last mean reward per episode: 89.57
Num timesteps: 364000
Best mean reward: 94.30 - Last mean reward per episode: 89.62
Num timesteps: 365000
Best mean reward: 94.30 - Last mean reward per episode: 89.67
Num timesteps: 366000
Best mean reward: 94.30 - Last mean reward per episode: 90.51
Num timesteps: 367000
Best mean reward: 94.30 - Last mean reward per episode: 90.58
Num timesteps: 368000
Best mean reward: 94.30 - Last mean reward per episode: 90.57
Num timesteps: 369000
Best mean reward: 94.30 - Last mean reward per episode: 89.10
Num timesteps: 370000
Best mean reward: 94.30 - Last mean reward per episode: 89.10
--------------------------------------
| reference_Q_mean        | 53.8     |
| reference_Q_std         | 4.63     |
| reference_action_mean   | 0.444    |
| reference_action_std    | 0.873    |
| reference_actor_Q_mean  | 54.1     |
| reference_actor_Q_std   | 4.63     |
| rollout/Q_mean          | 37.1     |
| rollout/actions_mean    | 0.171    |
| rollout/actions_std     | 0.76     |
| rollout/episode_steps   | 169      |
| rollout/episodes        | 2.18e+03 |
| rollout/return          | 84       |
| rollout/return_history  | 89.1     |
| total/duration          | 902      |
| total/episodes          | 2.18e+03 |
| total/epochs            | 1        |
| total/steps             | 369998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -66.7    |
| train/loss_critic       | 0.182    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 371000
Best mean reward: 94.30 - Last mean reward per episode: 87.70
Num timesteps: 372000
Best mean reward: 94.30 - Last mean reward per episode: 87.91
Num timesteps: 373000
Best mean reward: 94.30 - Last mean reward per episode: 87.95
Num timesteps: 374000
Best mean reward: 94.30 - Last mean reward per episode: 88.00
Num timesteps: 375000
Best mean reward: 94.30 - Last mean reward per episode: 87.93
Num timesteps: 376000
Best mean reward: 94.30 - Last mean reward per episode: 87.84
Num timesteps: 377000
Best mean reward: 94.30 - Last mean reward per episode: 87.90
Num timesteps: 378000
Best mean reward: 94.30 - Last mean reward per episode: 87.92
Num timesteps: 379000
Best mean reward: 94.30 - Last mean reward per episode: 87.93
Num timesteps: 380000
Best mean reward: 94.30 - Last mean reward per episode: 88.01
--------------------------------------
| reference_Q_mean        | 53.4     |
| reference_Q_std         | 5.07     |
| reference_action_mean   | 0.162    |
| reference_action_std    | 0.964    |
| reference_actor_Q_mean  | 53.9     |
| reference_actor_Q_std   | 5.14     |
| rollout/Q_mean          | 37.9     |
| rollout/actions_mean    | 0.177    |
| rollout/actions_std     | 0.762    |
| rollout/episode_steps   | 168      |
| rollout/episodes        | 2.27e+03 |
| rollout/return          | 84.2     |
| rollout/return_history  | 88       |
| total/duration          | 926      |
| total/episodes          | 2.27e+03 |
| total/epochs            | 1        |
| total/steps             | 379998   |
| total/steps_per_second  | 411      |
| train/loss_actor        | -64.9    |
| train/loss_critic       | 0.243    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 381000
Best mean reward: 94.30 - Last mean reward per episode: 89.59
Num timesteps: 382000
Best mean reward: 94.30 - Last mean reward per episode: 90.95
Num timesteps: 383000
Best mean reward: 94.30 - Last mean reward per episode: 91.25
Num timesteps: 384000
Best mean reward: 94.30 - Last mean reward per episode: 91.57
Num timesteps: 385000
Best mean reward: 94.30 - Last mean reward per episode: 91.83
Num timesteps: 386000
Best mean reward: 94.30 - Last mean reward per episode: 91.84
Num timesteps: 387000
Best mean reward: 94.30 - Last mean reward per episode: 92.13
Num timesteps: 388000
Best mean reward: 94.30 - Last mean reward per episode: 92.42
Num timesteps: 389000
Best mean reward: 94.30 - Last mean reward per episode: 92.75
Num timesteps: 390000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
--------------------------------------
| reference_Q_mean        | 52.9     |
| reference_Q_std         | 5.41     |
| reference_action_mean   | -0.0416  |
| reference_action_std    | 0.961    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 5.58     |
| rollout/Q_mean          | 38.7     |
| rollout/actions_mean    | 0.174    |
| rollout/actions_std     | 0.766    |
| rollout/episode_steps   | 164      |
| rollout/episodes        | 2.38e+03 |
| rollout/return          | 84.6     |
| rollout/return_history  | 93.1     |
| total/duration          | 952      |
| total/episodes          | 2.38e+03 |
| total/epochs            | 1        |
| total/steps             | 389998   |
| total/steps_per_second  | 410      |
| train/loss_actor        | -64.4    |
| train/loss_critic       | 0.2      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 391000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 392000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 393000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
Num timesteps: 394000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 395000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 396000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 397000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 398000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 399000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 400000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 5.84     |
| reference_action_mean   | -0.239   |
| reference_action_std    | 0.942    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 5.83     |
| rollout/Q_mean          | 39.5     |
| rollout/actions_mean    | 0.171    |
| rollout/actions_std     | 0.769    |
| rollout/episode_steps   | 160      |
| rollout/episodes        | 2.5e+03  |
| rollout/return          | 85.1     |
| rollout/return_history  | 93.5     |
| total/duration          | 977      |
| total/episodes          | 2.5e+03  |
| total/epochs            | 1        |
| total/steps             | 399998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -65.3    |
| train/loss_critic       | 0.21     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 401000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 402000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 403000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 404000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 405000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 406000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 407000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 408000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 409000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 410000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 6.41     |
| reference_action_mean   | -0.107   |
| reference_action_std    | 0.965    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 6.43     |
| rollout/Q_mean          | 40.2     |
| rollout/actions_mean    | 0.17     |
| rollout/actions_std     | 0.772    |
| rollout/episode_steps   | 156      |
| rollout/episodes        | 2.62e+03 |
| rollout/return          | 85.5     |
| rollout/return_history  | 93.6     |
| total/duration          | 1e+03    |
| total/episodes          | 2.62e+03 |
| total/epochs            | 1        |
| total/steps             | 409998   |
| total/steps_per_second  | 409      |
| train/loss_actor        | -66.4    |
| train/loss_critic       | 1.07     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 411000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 412000
Best mean reward: 94.30 - Last mean reward per episode: 93.55
Num timesteps: 413000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 414000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 415000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 416000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 417000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 418000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 419000
Best mean reward: 94.30 - Last mean reward per episode: 93.40
Num timesteps: 420000
Best mean reward: 94.30 - Last mean reward per episode: 93.38
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 7.06     |
| reference_action_mean   | -0.496   |
| reference_action_std    | 0.838    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 6.94     |
| rollout/Q_mean          | 40.9     |
| rollout/actions_mean    | 0.168    |
| rollout/actions_std     | 0.774    |
| rollout/episode_steps   | 154      |
| rollout/episodes        | 2.73e+03 |
| rollout/return          | 85.8     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.03e+03 |
| total/episodes          | 2.73e+03 |
| total/epochs            | 1        |
| total/steps             | 419998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -67.8    |
| train/loss_critic       | 0.574    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 421000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 422000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 423000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 424000
Best mean reward: 94.30 - Last mean reward per episode: 93.38
Num timesteps: 425000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
Num timesteps: 426000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 427000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 428000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 429000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 430000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 7.58     |
| reference_action_mean   | -0.389   |
| reference_action_std    | 0.899    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 7.32     |
| rollout/Q_mean          | 41.5     |
| rollout/actions_mean    | 0.167    |
| rollout/actions_std     | 0.776    |
| rollout/episode_steps   | 151      |
| rollout/episodes        | 2.85e+03 |
| rollout/return          | 86.1     |
| rollout/return_history  | 93.7     |
| total/duration          | 1.06e+03 |
| total/episodes          | 2.85e+03 |
| total/epochs            | 1        |
| total/steps             | 429998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.387    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 431000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 432000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 433000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 434000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 435000
Best mean reward: 94.30 - Last mean reward per episode: 93.88
Num timesteps: 436000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 437000
Best mean reward: 94.30 - Last mean reward per episode: 93.87
Num timesteps: 438000
Best mean reward: 94.30 - Last mean reward per episode: 93.89
Num timesteps: 439000
Best mean reward: 94.30 - Last mean reward per episode: 93.93
Num timesteps: 440000
Best mean reward: 94.30 - Last mean reward per episode: 93.90
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 6.63     |
| reference_action_mean   | 0.0497   |
| reference_action_std    | 0.963    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 6.18     |
| rollout/Q_mean          | 42.2     |
| rollout/actions_mean    | 0.167    |
| rollout/actions_std     | 0.778    |
| rollout/episode_steps   | 148      |
| rollout/episodes        | 2.98e+03 |
| rollout/return          | 86.4     |
| rollout/return_history  | 93.9     |
| total/duration          | 1.08e+03 |
| total/episodes          | 2.98e+03 |
| total/epochs            | 1        |
| total/steps             | 439998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.272    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 441000
Best mean reward: 94.30 - Last mean reward per episode: 93.91
Num timesteps: 442000
Best mean reward: 94.30 - Last mean reward per episode: 93.94
Num timesteps: 443000
Best mean reward: 94.30 - Last mean reward per episode: 94.01
Num timesteps: 444000
Best mean reward: 94.30 - Last mean reward per episode: 94.01
Num timesteps: 445000
Best mean reward: 94.30 - Last mean reward per episode: 94.01
Num timesteps: 446000
Best mean reward: 94.30 - Last mean reward per episode: 94.08
Num timesteps: 447000
Best mean reward: 94.30 - Last mean reward per episode: 94.06
Num timesteps: 448000
Best mean reward: 94.30 - Last mean reward per episode: 94.11
Num timesteps: 449000
Best mean reward: 94.30 - Last mean reward per episode: 94.08
Num timesteps: 450000
Best mean reward: 94.30 - Last mean reward per episode: 94.10
--------------------------------------
| reference_Q_mean        | 50.6     |
| reference_Q_std         | 6.95     |
| reference_action_mean   | 0.0234   |
| reference_action_std    | 0.926    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 6.87     |
| rollout/Q_mean          | 42.8     |
| rollout/actions_mean    | 0.169    |
| rollout/actions_std     | 0.779    |
| rollout/episode_steps   | 145      |
| rollout/episodes        | 3.1e+03  |
| rollout/return          | 86.7     |
| rollout/return_history  | 94.1     |
| total/duration          | 1.1e+03  |
| total/episodes          | 3.1e+03  |
| total/epochs            | 1        |
| total/steps             | 449998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.37     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 451000
Best mean reward: 94.30 - Last mean reward per episode: 94.11
Num timesteps: 452000
Best mean reward: 94.30 - Last mean reward per episode: 94.09
Num timesteps: 453000
Best mean reward: 94.30 - Last mean reward per episode: 94.11
Num timesteps: 454000
Best mean reward: 94.30 - Last mean reward per episode: 94.15
Num timesteps: 455000
Best mean reward: 94.30 - Last mean reward per episode: 94.17
Num timesteps: 456000
Best mean reward: 94.30 - Last mean reward per episode: 94.15
Num timesteps: 457000
Best mean reward: 94.30 - Last mean reward per episode: 94.14
Num timesteps: 458000
Best mean reward: 94.30 - Last mean reward per episode: 94.13
Num timesteps: 459000
Best mean reward: 94.30 - Last mean reward per episode: 94.14
Num timesteps: 460000
Best mean reward: 94.30 - Last mean reward per episode: 94.10
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 7.56     |
| reference_action_mean   | -0.3     |
| reference_action_std    | 0.93     |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 7.56     |
| rollout/Q_mean          | 43.3     |
| rollout/actions_mean    | 0.169    |
| rollout/actions_std     | 0.779    |
| rollout/episode_steps   | 143      |
| rollout/episodes        | 3.21e+03 |
| rollout/return          | 87       |
| rollout/return_history  | 94.1     |
| total/duration          | 1.13e+03 |
| total/episodes          | 3.21e+03 |
| total/epochs            | 1        |
| total/steps             | 459998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.226    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 461000
Best mean reward: 94.30 - Last mean reward per episode: 94.02
Num timesteps: 462000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 463000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 464000
Best mean reward: 94.30 - Last mean reward per episode: 92.93
Num timesteps: 465000
Best mean reward: 94.30 - Last mean reward per episode: 92.66
Num timesteps: 466000
Best mean reward: 94.30 - Last mean reward per episode: 92.51
Num timesteps: 467000
Best mean reward: 94.30 - Last mean reward per episode: 92.31
Num timesteps: 468000
Best mean reward: 94.30 - Last mean reward per episode: 92.17
Num timesteps: 469000
Best mean reward: 94.30 - Last mean reward per episode: 92.15
Num timesteps: 470000
Best mean reward: 94.30 - Last mean reward per episode: 92.15
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 6.82     |
| reference_action_mean   | -0.265   |
| reference_action_std    | 0.898    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 6.92     |
| rollout/Q_mean          | 43.8     |
| rollout/actions_mean    | 0.173    |
| rollout/actions_std     | 0.779    |
| rollout/episode_steps   | 143      |
| rollout/episodes        | 3.3e+03  |
| rollout/return          | 87.1     |
| rollout/return_history  | 92.1     |
| total/duration          | 1.15e+03 |
| total/episodes          | 3.3e+03  |
| total/epochs            | 1        |
| total/steps             | 469998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 0.232    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 471000
Best mean reward: 94.30 - Last mean reward per episode: 92.18
Num timesteps: 472000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 473000
Best mean reward: 94.30 - Last mean reward per episode: 93.15
Num timesteps: 474000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 475000
Best mean reward: 94.30 - Last mean reward per episode: 93.90
Num timesteps: 476000
Best mean reward: 94.30 - Last mean reward per episode: 93.96
Num timesteps: 477000
Best mean reward: 94.30 - Last mean reward per episode: 93.97
Num timesteps: 478000
Best mean reward: 94.30 - Last mean reward per episode: 93.86
Num timesteps: 479000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 480000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 6.31     |
| reference_action_mean   | -0.0882  |
| reference_action_std    | 0.908    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 6.55     |
| rollout/Q_mean          | 44.4     |
| rollout/actions_mean    | 0.175    |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 140      |
| rollout/episodes        | 3.42e+03 |
| rollout/return          | 87.4     |
| rollout/return_history  | 93.8     |
| total/duration          | 1.18e+03 |
| total/episodes          | 3.42e+03 |
| total/epochs            | 1        |
| total/steps             | 479998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.225    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 481000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 482000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 483000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 484000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 485000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 486000
Best mean reward: 94.30 - Last mean reward per episode: 93.76
Num timesteps: 487000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 488000
Best mean reward: 94.30 - Last mean reward per episode: 93.79
Num timesteps: 489000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 490000
Best mean reward: 94.30 - Last mean reward per episode: 93.40
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 6.43     |
| reference_action_mean   | 0.286    |
| reference_action_std    | 0.91     |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 6.71     |
| rollout/Q_mean          | 44.9     |
| rollout/actions_mean    | 0.175    |
| rollout/actions_std     | 0.781    |
| rollout/episode_steps   | 139      |
| rollout/episodes        | 3.52e+03 |
| rollout/return          | 87.5     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.2e+03  |
| total/episodes          | 3.52e+03 |
| total/epochs            | 1        |
| total/steps             | 489998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.209    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 491000
Best mean reward: 94.30 - Last mean reward per episode: 93.30
Num timesteps: 492000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 493000
Best mean reward: 94.30 - Last mean reward per episode: 92.93
Num timesteps: 494000
Best mean reward: 94.30 - Last mean reward per episode: 92.55
Num timesteps: 495000
Best mean reward: 94.30 - Last mean reward per episode: 92.27
Num timesteps: 496000
Best mean reward: 94.30 - Last mean reward per episode: 92.00
Num timesteps: 497000
Best mean reward: 94.30 - Last mean reward per episode: 91.84
Num timesteps: 498000
Best mean reward: 94.30 - Last mean reward per episode: 91.66
Num timesteps: 499000
Best mean reward: 94.30 - Last mean reward per episode: 91.55
Num timesteps: 500000
Best mean reward: 94.30 - Last mean reward per episode: 91.61
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 6.18     |
| reference_action_mean   | 0.379    |
| reference_action_std    | 0.887    |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 6.16     |
| rollout/Q_mean          | 45.3     |
| rollout/actions_mean    | 0.181    |
| rollout/actions_std     | 0.781    |
| rollout/episode_steps   | 138      |
| rollout/episodes        | 3.61e+03 |
| rollout/return          | 87.6     |
| rollout/return_history  | 91.6     |
| total/duration          | 1.23e+03 |
| total/episodes          | 3.61e+03 |
| total/epochs            | 1        |
| total/steps             | 499998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -68.5    |
| train/loss_critic       | 0.273    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 501000
Best mean reward: 94.30 - Last mean reward per episode: 91.51
Num timesteps: 502000
Best mean reward: 94.30 - Last mean reward per episode: 91.60
Num timesteps: 503000
Best mean reward: 94.30 - Last mean reward per episode: 91.58
Num timesteps: 504000
Best mean reward: 94.30 - Last mean reward per episode: 92.09
Num timesteps: 505000
Best mean reward: 94.30 - Last mean reward per episode: 92.35
Num timesteps: 506000
Best mean reward: 94.30 - Last mean reward per episode: 92.41
Num timesteps: 507000
Best mean reward: 94.30 - Last mean reward per episode: 92.49
Num timesteps: 508000
Best mean reward: 94.30 - Last mean reward per episode: 92.50
Num timesteps: 509000
Best mean reward: 94.30 - Last mean reward per episode: 90.95
Num timesteps: 510000
Best mean reward: 94.30 - Last mean reward per episode: 90.98
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 5.77     |
| reference_action_mean   | 0.368    |
| reference_action_std    | 0.888    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 5.9      |
| rollout/Q_mean          | 45.8     |
| rollout/actions_mean    | 0.183    |
| rollout/actions_std     | 0.782    |
| rollout/episode_steps   | 138      |
| rollout/episodes        | 3.71e+03 |
| rollout/return          | 87.7     |
| rollout/return_history  | 91       |
| total/duration          | 1.25e+03 |
| total/episodes          | 3.71e+03 |
| total/epochs            | 1        |
| total/steps             | 509998   |
| total/steps_per_second  | 408      |
| train/loss_actor        | -68      |
| train/loss_critic       | 0.519    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 511000
Best mean reward: 94.30 - Last mean reward per episode: 91.20
Num timesteps: 512000
Best mean reward: 94.30 - Last mean reward per episode: 91.37
Num timesteps: 513000
Best mean reward: 94.30 - Last mean reward per episode: 91.47
Num timesteps: 514000
Best mean reward: 94.30 - Last mean reward per episode: 91.36
Num timesteps: 515000
Best mean reward: 94.30 - Last mean reward per episode: 91.29
Num timesteps: 516000
Best mean reward: 94.30 - Last mean reward per episode: 91.39
Num timesteps: 517000
Best mean reward: 94.30 - Last mean reward per episode: 91.35
Num timesteps: 518000
Best mean reward: 94.30 - Last mean reward per episode: 92.94
Num timesteps: 519000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
Num timesteps: 520000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 6.3      |
| reference_action_mean   | -0.34    |
| reference_action_std    | 0.865    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 6.3      |
| rollout/Q_mean          | 46.2     |
| rollout/actions_mean    | 0.185    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 136      |
| rollout/episodes        | 3.82e+03 |
| rollout/return          | 87.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.28e+03 |
| total/episodes          | 3.82e+03 |
| total/epochs            | 1        |
| total/steps             | 519998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -68      |
| train/loss_critic       | 1.24     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 521000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 522000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
Num timesteps: 523000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 524000
Best mean reward: 94.30 - Last mean reward per episode: 93.97
Num timesteps: 525000
Best mean reward: 94.30 - Last mean reward per episode: 93.87
Num timesteps: 526000
Best mean reward: 94.30 - Last mean reward per episode: 94.06
Num timesteps: 527000
Best mean reward: 94.30 - Last mean reward per episode: 94.01
Num timesteps: 528000
Best mean reward: 94.30 - Last mean reward per episode: 93.98
Num timesteps: 529000
Best mean reward: 94.30 - Last mean reward per episode: 94.00
Num timesteps: 530000
Best mean reward: 94.30 - Last mean reward per episode: 94.03
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 6.66     |
| reference_action_mean   | -0.486   |
| reference_action_std    | 0.769    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 6.6      |
| rollout/Q_mean          | 46.6     |
| rollout/actions_mean    | 0.186    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 134      |
| rollout/episodes        | 3.94e+03 |
| rollout/return          | 88.1     |
| rollout/return_history  | 94       |
| total/duration          | 1.3e+03  |
| total/episodes          | 3.94e+03 |
| total/epochs            | 1        |
| total/steps             | 529998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -67.2    |
| train/loss_critic       | 0.44     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 531000
Best mean reward: 94.30 - Last mean reward per episode: 94.07
Num timesteps: 532000
Best mean reward: 94.30 - Last mean reward per episode: 94.08
Num timesteps: 533000
Best mean reward: 94.30 - Last mean reward per episode: 94.17
Num timesteps: 534000
Best mean reward: 94.30 - Last mean reward per episode: 94.15
Num timesteps: 535000
Best mean reward: 94.30 - Last mean reward per episode: 93.95
Num timesteps: 536000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 537000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 538000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 539000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 540000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 6.97     |
| reference_action_mean   | -0.628   |
| reference_action_std    | 0.668    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 7        |
| rollout/Q_mean          | 47       |
| rollout/actions_mean    | 0.186    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 133      |
| rollout/episodes        | 4.05e+03 |
| rollout/return          | 88.2     |
| rollout/return_history  | 93.7     |
| total/duration          | 1.33e+03 |
| total/episodes          | 4.05e+03 |
| total/epochs            | 1        |
| total/steps             | 539998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -66.6    |
| train/loss_critic       | 0.539    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 541000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 542000
Best mean reward: 94.30 - Last mean reward per episode: 92.22
Num timesteps: 543000
Best mean reward: 94.30 - Last mean reward per episode: 91.02
Num timesteps: 544000
Best mean reward: 94.30 - Last mean reward per episode: 90.74
Num timesteps: 545000
Best mean reward: 94.30 - Last mean reward per episode: 90.52
Num timesteps: 546000
Best mean reward: 94.30 - Last mean reward per episode: 90.24
Num timesteps: 547000
Best mean reward: 94.30 - Last mean reward per episode: 90.20
Num timesteps: 548000
Best mean reward: 94.30 - Last mean reward per episode: 90.15
Num timesteps: 549000
Best mean reward: 94.30 - Last mean reward per episode: 90.32
Num timesteps: 550000
Best mean reward: 94.30 - Last mean reward per episode: 90.24
--------------------------------------
| reference_Q_mean        | 51.4     |
| reference_Q_std         | 7.2      |
| reference_action_mean   | -0.652   |
| reference_action_std    | 0.598    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 7.16     |
| rollout/Q_mean          | 47.3     |
| rollout/actions_mean    | 0.181    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 134      |
| rollout/episodes        | 4.10e+03 |
| rollout/return          | 88.2     |
| rollout/return_history  | 90.2     |
| total/duration          | 1.35e+03 |
| total/episodes          | 4.10e+03 |
| total/epochs            | 1        |
| total/steps             | 549998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -66      |
| train/loss_critic       | 0.679    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 551000
Best mean reward: 94.30 - Last mean reward per episode: 90.06
Num timesteps: 552000
Best mean reward: 94.30 - Last mean reward per episode: 90.20
Num timesteps: 553000
Best mean reward: 94.30 - Last mean reward per episode: 90.37
Num timesteps: 554000
Best mean reward: 94.30 - Last mean reward per episode: 90.34
Num timesteps: 555000
Best mean reward: 94.30 - Last mean reward per episode: 90.28
Num timesteps: 556000
Best mean reward: 94.30 - Last mean reward per episode: 90.15
Num timesteps: 557000
Best mean reward: 94.30 - Last mean reward per episode: 92.85
Num timesteps: 558000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 559000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 560000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 6.47     |
| reference_action_mean   | -0.49    |
| reference_action_std    | 0.763    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 6.48     |
| rollout/Q_mean          | 47.6     |
| rollout/actions_mean    | 0.181    |
| rollout/actions_std     | 0.782    |
| rollout/episode_steps   | 134      |
| rollout/episodes        | 4.19e+03 |
| rollout/return          | 88.3     |
| rollout/return_history  | 93.6     |
| total/duration          | 1.38e+03 |
| total/episodes          | 4.19e+03 |
| total/epochs            | 1        |
| total/steps             | 559998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -66      |
| train/loss_critic       | 0.5      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 561000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 562000
Best mean reward: 94.30 - Last mean reward per episode: 93.96
Num timesteps: 563000
Best mean reward: 94.30 - Last mean reward per episode: 93.98
Num timesteps: 564000
Best mean reward: 94.30 - Last mean reward per episode: 93.90
Num timesteps: 565000
Best mean reward: 94.30 - Last mean reward per episode: 93.97
Num timesteps: 566000
Best mean reward: 94.30 - Last mean reward per episode: 94.22
Num timesteps: 567000
Best mean reward: 94.30 - Last mean reward per episode: 94.23
Num timesteps: 568000
Best mean reward: 94.30 - Last mean reward per episode: 94.14
Num timesteps: 569000
Best mean reward: 94.30 - Last mean reward per episode: 94.14
Num timesteps: 570000
Best mean reward: 94.30 - Last mean reward per episode: 94.13
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 6.4      |
| reference_action_mean   | -0.638   |
| reference_action_std    | 0.735    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 6.35     |
| rollout/Q_mean          | 48       |
| rollout/actions_mean    | 0.179    |
| rollout/actions_std     | 0.782    |
| rollout/episode_steps   | 133      |
| rollout/episodes        | 4.29e+03 |
| rollout/return          | 88.5     |
| rollout/return_history  | 94.1     |
| total/duration          | 1.4e+03  |
| total/episodes          | 4.29e+03 |
| total/epochs            | 1        |
| total/steps             | 569998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -66.9    |
| train/loss_critic       | 0.988    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 571000
Best mean reward: 94.30 - Last mean reward per episode: 94.03
Num timesteps: 572000
Best mean reward: 94.30 - Last mean reward per episode: 93.86
Num timesteps: 573000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
Num timesteps: 574000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
Num timesteps: 575000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 576000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 577000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 578000
Best mean reward: 94.30 - Last mean reward per episode: 93.15
Num timesteps: 579000
Best mean reward: 94.30 - Last mean reward per episode: 93.14
Num timesteps: 580000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 6.19     |
| reference_action_mean   | -0.261   |
| reference_action_std    | 0.944    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 6.03     |
| rollout/Q_mean          | 48.3     |
| rollout/actions_mean    | 0.177    |
| rollout/actions_std     | 0.782    |
| rollout/episode_steps   | 133      |
| rollout/episodes        | 4.38e+03 |
| rollout/return          | 88.6     |
| rollout/return_history  | 93.2     |
| total/duration          | 1.43e+03 |
| total/episodes          | 4.38e+03 |
| total/epochs            | 1        |
| total/steps             | 579998   |
| total/steps_per_second  | 407      |
| train/loss_actor        | -67.2    |
| train/loss_critic       | 2.4      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 581000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 582000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 583000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 584000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
Num timesteps: 585000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 586000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
Num timesteps: 587000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 588000
Best mean reward: 94.30 - Last mean reward per episode: 92.58
Num timesteps: 589000
Best mean reward: 94.30 - Last mean reward per episode: 92.51
Num timesteps: 590000
Best mean reward: 94.30 - Last mean reward per episode: 92.55
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 6.08     |
| reference_action_mean   | -0.418   |
| reference_action_std    | 0.876    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 6.15     |
| rollout/Q_mean          | 48.7     |
| rollout/actions_mean    | 0.177    |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 132      |
| rollout/episodes        | 4.45e+03 |
| rollout/return          | 88.6     |
| rollout/return_history  | 92.6     |
| total/duration          | 1.45e+03 |
| total/episodes          | 4.45e+03 |
| total/epochs            | 1        |
| total/steps             | 589998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -66.9    |
| train/loss_critic       | 0.386    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 591000
Best mean reward: 94.30 - Last mean reward per episode: 92.53
Num timesteps: 592000
Best mean reward: 94.30 - Last mean reward per episode: 92.47
Num timesteps: 593000
Best mean reward: 94.30 - Last mean reward per episode: 92.47
Num timesteps: 594000
Best mean reward: 94.30 - Last mean reward per episode: 92.48
Num timesteps: 595000
Best mean reward: 94.30 - Last mean reward per episode: 92.77
Num timesteps: 596000
Best mean reward: 94.30 - Last mean reward per episode: 92.75
Num timesteps: 597000
Best mean reward: 94.30 - Last mean reward per episode: 92.72
Num timesteps: 598000
Best mean reward: 94.30 - Last mean reward per episode: 91.24
Num timesteps: 599000
Best mean reward: 94.30 - Last mean reward per episode: 91.14
Num timesteps: 600000
Best mean reward: 94.30 - Last mean reward per episode: 92.40
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 5.8      |
| reference_action_mean   | -0.133   |
| reference_action_std    | 0.956    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 5.94     |
| rollout/Q_mean          | 49       |
| rollout/actions_mean    | 0.176    |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 132      |
| rollout/episodes        | 4.54e+03 |
| rollout/return          | 88.7     |
| rollout/return_history  | 92.4     |
| total/duration          | 1.48e+03 |
| total/episodes          | 4.54e+03 |
| total/epochs            | 1        |
| total/steps             | 599998   |
| total/steps_per_second  | 406      |
| train/loss_actor        | -68.2    |
| train/loss_critic       | 1.18     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 601000
Best mean reward: 94.30 - Last mean reward per episode: 92.38
Num timesteps: 602000
Best mean reward: 94.30 - Last mean reward per episode: 92.43
Num timesteps: 603000
Best mean reward: 94.30 - Last mean reward per episode: 92.40
Num timesteps: 604000
Best mean reward: 94.30 - Last mean reward per episode: 92.34
Num timesteps: 605000
Best mean reward: 94.30 - Last mean reward per episode: 92.29
Num timesteps: 606000
Best mean reward: 94.30 - Last mean reward per episode: 92.06
Num timesteps: 607000
Best mean reward: 94.30 - Last mean reward per episode: 91.74
Num timesteps: 608000
Best mean reward: 94.30 - Last mean reward per episode: 92.92
Num timesteps: 609000
Best mean reward: 94.30 - Last mean reward per episode: 92.73
Num timesteps: 610000
Best mean reward: 94.30 - Last mean reward per episode: 92.47
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 5.94     |
| reference_action_mean   | 0.551    |
| reference_action_std    | 0.82     |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 6.06     |
| rollout/Q_mean          | 49.3     |
| rollout/actions_mean    | 0.18     |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 131      |
| rollout/episodes        | 4.64e+03 |
| rollout/return          | 88.8     |
| rollout/return_history  | 92.5     |
| total/duration          | 1.51e+03 |
| total/episodes          | 4.64e+03 |
| total/epochs            | 1        |
| total/steps             | 609998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -68.3    |
| train/loss_critic       | 0.427    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 611000
Best mean reward: 94.30 - Last mean reward per episode: 92.24
Num timesteps: 612000
Best mean reward: 94.30 - Last mean reward per episode: 92.00
Num timesteps: 613000
Best mean reward: 94.30 - Last mean reward per episode: 91.66
Num timesteps: 614000
Best mean reward: 94.30 - Last mean reward per episode: 91.47
Num timesteps: 615000
Best mean reward: 94.30 - Last mean reward per episode: 91.33
Num timesteps: 616000
Best mean reward: 94.30 - Last mean reward per episode: 91.14
Num timesteps: 617000
Best mean reward: 94.30 - Last mean reward per episode: 90.84
Num timesteps: 618000
Best mean reward: 94.30 - Last mean reward per episode: 90.72
Num timesteps: 619000
Best mean reward: 94.30 - Last mean reward per episode: 90.97
Num timesteps: 620000
Best mean reward: 94.30 - Last mean reward per episode: 91.24
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 6.5      |
| reference_action_mean   | 0.199    |
| reference_action_std    | 0.954    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 6.52     |
| rollout/Q_mean          | 49.6     |
| rollout/actions_mean    | 0.185    |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 131      |
| rollout/episodes        | 4.73e+03 |
| rollout/return          | 88.8     |
| rollout/return_history  | 91.2     |
| total/duration          | 1.53e+03 |
| total/episodes          | 4.73e+03 |
| total/epochs            | 1        |
| total/steps             | 619998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -66.7    |
| train/loss_critic       | 1.04     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 621000
Best mean reward: 94.30 - Last mean reward per episode: 91.38
Num timesteps: 622000
Best mean reward: 94.30 - Last mean reward per episode: 91.80
Num timesteps: 623000
Best mean reward: 94.30 - Last mean reward per episode: 92.05
Num timesteps: 624000
Best mean reward: 94.30 - Last mean reward per episode: 92.26
Num timesteps: 625000
Best mean reward: 94.30 - Last mean reward per episode: 92.26
Num timesteps: 626000
Best mean reward: 94.30 - Last mean reward per episode: 92.43
Num timesteps: 627000
Best mean reward: 94.30 - Last mean reward per episode: 92.24
Num timesteps: 628000
Best mean reward: 94.30 - Last mean reward per episode: 92.38
Num timesteps: 629000
Best mean reward: 94.30 - Last mean reward per episode: 92.62
Num timesteps: 630000
Best mean reward: 94.30 - Last mean reward per episode: 92.61
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 6.55     |
| reference_action_mean   | -0.0232  |
| reference_action_std    | 0.971    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 6.47     |
| rollout/Q_mean          | 49.8     |
| rollout/actions_mean    | 0.186    |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 131      |
| rollout/episodes        | 4.82e+03 |
| rollout/return          | 88.9     |
| rollout/return_history  | 92.6     |
| total/duration          | 1.56e+03 |
| total/episodes          | 4.82e+03 |
| total/epochs            | 1        |
| total/steps             | 629998   |
| total/steps_per_second  | 405      |
| train/loss_actor        | -66.2    |
| train/loss_critic       | 0.495    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 631000
Best mean reward: 94.30 - Last mean reward per episode: 92.62
Num timesteps: 632000
Best mean reward: 94.30 - Last mean reward per episode: 92.69
Num timesteps: 633000
Best mean reward: 94.30 - Last mean reward per episode: 92.69
Num timesteps: 634000
Best mean reward: 94.30 - Last mean reward per episode: 92.59
Num timesteps: 635000
Best mean reward: 94.30 - Last mean reward per episode: 92.50
Num timesteps: 636000
Best mean reward: 94.30 - Last mean reward per episode: 92.72
Num timesteps: 637000
Best mean reward: 94.30 - Last mean reward per episode: 93.05
Num timesteps: 638000
Best mean reward: 94.30 - Last mean reward per episode: 92.90
Num timesteps: 639000
Best mean reward: 94.30 - Last mean reward per episode: 93.01
Num timesteps: 640000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 6.7      |
| reference_action_mean   | -0.453   |
| reference_action_std    | 0.878    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 6.63     |
| rollout/Q_mean          | 50.1     |
| rollout/actions_mean    | 0.184    |
| rollout/actions_std     | 0.781    |
| rollout/episode_steps   | 130      |
| rollout/episodes        | 4.92e+03 |
| rollout/return          | 89       |
| rollout/return_history  | 93.1     |
| total/duration          | 1.58e+03 |
| total/episodes          | 4.92e+03 |
| total/epochs            | 1        |
| total/steps             | 639998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -67      |
| train/loss_critic       | 0.639    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 641000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 642000
Best mean reward: 94.30 - Last mean reward per episode: 92.71
Num timesteps: 643000
Best mean reward: 94.30 - Last mean reward per episode: 92.59
Num timesteps: 644000
Best mean reward: 94.30 - Last mean reward per episode: 92.84
Num timesteps: 645000
Best mean reward: 94.30 - Last mean reward per episode: 92.93
Num timesteps: 646000
Best mean reward: 94.30 - Last mean reward per episode: 92.87
Num timesteps: 647000
Best mean reward: 94.30 - Last mean reward per episode: 92.88
Num timesteps: 648000
Best mean reward: 94.30 - Last mean reward per episode: 91.43
Num timesteps: 649000
Best mean reward: 94.30 - Last mean reward per episode: 91.56
Num timesteps: 650000
Best mean reward: 94.30 - Last mean reward per episode: 91.54
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 7.07     |
| reference_action_mean   | -0.108   |
| reference_action_std    | 0.982    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 6.94     |
| rollout/Q_mean          | 50.4     |
| rollout/actions_mean    | 0.181    |
| rollout/actions_std     | 0.782    |
| rollout/episode_steps   | 130      |
| rollout/episodes        | 5.01e+03 |
| rollout/return          | 89       |
| rollout/return_history  | 91.5     |
| total/duration          | 1.61e+03 |
| total/episodes          | 5.01e+03 |
| total/epochs            | 1        |
| total/steps             | 649998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -66.7    |
| train/loss_critic       | 0.623    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 651000
Best mean reward: 94.30 - Last mean reward per episode: 91.55
Num timesteps: 652000
Best mean reward: 94.30 - Last mean reward per episode: 92.14
Num timesteps: 653000
Best mean reward: 94.30 - Last mean reward per episode: 92.18
Num timesteps: 654000
Best mean reward: 94.30 - Last mean reward per episode: 92.00
Num timesteps: 655000
Best mean reward: 94.30 - Last mean reward per episode: 91.88
Num timesteps: 656000
Best mean reward: 94.30 - Last mean reward per episode: 91.78
Num timesteps: 657000
Best mean reward: 94.30 - Last mean reward per episode: 91.80
Num timesteps: 658000
Best mean reward: 94.30 - Last mean reward per episode: 92.71
Num timesteps: 659000
Best mean reward: 94.30 - Last mean reward per episode: 92.82
Num timesteps: 660000
Best mean reward: 94.30 - Last mean reward per episode: 92.76
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 5.89     |
| reference_action_mean   | -0.329   |
| reference_action_std    | 0.902    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 5.89     |
| rollout/Q_mean          | 50.7     |
| rollout/actions_mean    | 0.181    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 129      |
| rollout/episodes        | 5.1e+03  |
| rollout/return          | 89.1     |
| rollout/return_history  | 92.8     |
| total/duration          | 1.63e+03 |
| total/episodes          | 5.1e+03  |
| total/epochs            | 1        |
| total/steps             | 659998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -66.9    |
| train/loss_critic       | 0.644    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 661000
Best mean reward: 94.30 - Last mean reward per episode: 92.66
Num timesteps: 662000
Best mean reward: 94.30 - Last mean reward per episode: 92.68
Num timesteps: 663000
Best mean reward: 94.30 - Last mean reward per episode: 92.63
Num timesteps: 664000
Best mean reward: 94.30 - Last mean reward per episode: 92.63
Num timesteps: 665000
Best mean reward: 94.30 - Last mean reward per episode: 91.38
Num timesteps: 666000
Best mean reward: 94.30 - Last mean reward per episode: 91.15
Num timesteps: 667000
Best mean reward: 94.30 - Last mean reward per episode: 91.21
Num timesteps: 668000
Best mean reward: 94.30 - Last mean reward per episode: 91.17
Num timesteps: 669000
Best mean reward: 94.30 - Last mean reward per episode: 91.79
Num timesteps: 670000
Best mean reward: 94.30 - Last mean reward per episode: 91.74
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 5.54     |
| reference_action_mean   | -0.536   |
| reference_action_std    | 0.782    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 5.47     |
| rollout/Q_mean          | 50.9     |
| rollout/actions_mean    | 0.178    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 129      |
| rollout/episodes        | 5.19e+03 |
| rollout/return          | 89.1     |
| rollout/return_history  | 91.7     |
| total/duration          | 1.66e+03 |
| total/episodes          | 5.19e+03 |
| total/epochs            | 1        |
| total/steps             | 669998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -68.2    |
| train/loss_critic       | 0.447    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 671000
Best mean reward: 94.30 - Last mean reward per episode: 91.95
Num timesteps: 672000
Best mean reward: 94.30 - Last mean reward per episode: 91.91
Num timesteps: 673000
Best mean reward: 94.30 - Last mean reward per episode: 91.94
Num timesteps: 674000
Best mean reward: 94.30 - Last mean reward per episode: 91.50
Num timesteps: 675000
Best mean reward: 94.30 - Last mean reward per episode: 91.52
Num timesteps: 676000
Best mean reward: 94.30 - Last mean reward per episode: 93.11
Num timesteps: 677000
Best mean reward: 94.30 - Last mean reward per episode: 93.29
Num timesteps: 678000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 679000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 680000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 5.61     |
| reference_action_mean   | -0.772   |
| reference_action_std    | 0.567    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 5.73     |
| rollout/Q_mean          | 51.2     |
| rollout/actions_mean    | 0.178    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 128      |
| rollout/episodes        | 5.3e+03  |
| rollout/return          | 89.2     |
| rollout/return_history  | 93.2     |
| total/duration          | 1.69e+03 |
| total/episodes          | 5.3e+03  |
| total/epochs            | 1        |
| total/steps             | 679998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | -68.6    |
| train/loss_critic       | 1.48     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 681000
Best mean reward: 94.30 - Last mean reward per episode: 93.21
Num timesteps: 682000
Best mean reward: 94.30 - Last mean reward per episode: 93.14
Num timesteps: 683000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 684000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 685000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 686000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
Num timesteps: 687000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 688000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 689000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 690000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 6.02     |
| reference_action_mean   | -0.706   |
| reference_action_std    | 0.608    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 6.19     |
| rollout/Q_mean          | 51.5     |
| rollout/actions_mean    | 0.178    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 128      |
| rollout/episodes        | 5.41e+03 |
| rollout/return          | 89.3     |
| rollout/return_history  | 93.6     |
| total/duration          | 1.71e+03 |
| total/episodes          | 5.41e+03 |
| total/epochs            | 1        |
| total/steps             | 689998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.58     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 691000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 692000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 693000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 694000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 695000
Best mean reward: 94.30 - Last mean reward per episode: 94.00
Num timesteps: 696000
Best mean reward: 94.30 - Last mean reward per episode: 93.90
Num timesteps: 697000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 698000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 699000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
Num timesteps: 700000
Best mean reward: 94.30 - Last mean reward per episode: 93.79
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 7.07     |
| reference_action_mean   | -0.394   |
| reference_action_std    | 0.822    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 7.23     |
| rollout/Q_mean          | 51.7     |
| rollout/actions_mean    | 0.179    |
| rollout/actions_std     | 0.785    |
| rollout/episode_steps   | 127      |
| rollout/episodes        | 5.52e+03 |
| rollout/return          | 89.4     |
| rollout/return_history  | 93.8     |
| total/duration          | 1.74e+03 |
| total/episodes          | 5.52e+03 |
| total/epochs            | 1        |
| total/steps             | 699998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 0.742    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 701000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 702000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 703000
Best mean reward: 94.30 - Last mean reward per episode: 93.71
Num timesteps: 704000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 705000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
Num timesteps: 706000
Best mean reward: 94.30 - Last mean reward per episode: 93.92
Num timesteps: 707000
Best mean reward: 94.30 - Last mean reward per episode: 93.95
Num timesteps: 708000
Best mean reward: 94.30 - Last mean reward per episode: 93.87
Num timesteps: 709000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 710000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
--------------------------------------
| reference_Q_mean        | 49.1     |
| reference_Q_std         | 7.21     |
| reference_action_mean   | -0.395   |
| reference_action_std    | 0.832    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 7.4      |
| rollout/Q_mean          | 51.9     |
| rollout/actions_mean    | 0.181    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 126      |
| rollout/episodes        | 5.63e+03 |
| rollout/return          | 89.5     |
| rollout/return_history  | 93.7     |
| total/duration          | 1.76e+03 |
| total/episodes          | 5.63e+03 |
| total/epochs            | 1        |
| total/steps             | 709998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.898    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 711000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 712000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 713000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 714000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 715000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 716000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
Num timesteps: 717000
Best mean reward: 94.30 - Last mean reward per episode: 92.86
Num timesteps: 718000
Best mean reward: 94.30 - Last mean reward per episode: 92.72
Num timesteps: 719000
Best mean reward: 94.30 - Last mean reward per episode: 92.77
Num timesteps: 720000
Best mean reward: 94.30 - Last mean reward per episode: 92.87
--------------------------------------
| reference_Q_mean        | 48.3     |
| reference_Q_std         | 7.24     |
| reference_action_mean   | -0.478   |
| reference_action_std    | 0.741    |
| reference_actor_Q_mean  | 49.6     |
| reference_actor_Q_std   | 7.44     |
| rollout/Q_mean          | 52.2     |
| rollout/actions_mean    | 0.18     |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 126      |
| rollout/episodes        | 5.71e+03 |
| rollout/return          | 89.5     |
| rollout/return_history  | 92.9     |
| total/duration          | 1.79e+03 |
| total/episodes          | 5.71e+03 |
| total/epochs            | 1        |
| total/steps             | 719998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | -68.5    |
| train/loss_critic       | 0.619    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 721000
Best mean reward: 94.30 - Last mean reward per episode: 92.82
Num timesteps: 722000
Best mean reward: 94.30 - Last mean reward per episode: 92.86
Num timesteps: 723000
Best mean reward: 94.30 - Last mean reward per episode: 93.11
Num timesteps: 724000
Best mean reward: 94.30 - Last mean reward per episode: 91.40
Num timesteps: 725000
Best mean reward: 94.30 - Last mean reward per episode: 91.42
Num timesteps: 726000
Best mean reward: 94.30 - Last mean reward per episode: 91.26
Num timesteps: 727000
Best mean reward: 94.30 - Last mean reward per episode: 91.23
Num timesteps: 728000
Best mean reward: 94.30 - Last mean reward per episode: 91.06
Num timesteps: 729000
Best mean reward: 94.30 - Last mean reward per episode: 91.64
Num timesteps: 730000
Best mean reward: 94.30 - Last mean reward per episode: 91.65
--------------------------------------
| reference_Q_mean        | 48.5     |
| reference_Q_std         | 7.22     |
| reference_action_mean   | -0.439   |
| reference_action_std    | 0.775    |
| reference_actor_Q_mean  | 49.6     |
| reference_actor_Q_std   | 7.55     |
| rollout/Q_mean          | 52.4     |
| rollout/actions_mean    | 0.18     |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 126      |
| rollout/episodes        | 5.8e+03  |
| rollout/return          | 89.6     |
| rollout/return_history  | 91.7     |
| total/duration          | 1.81e+03 |
| total/episodes          | 5.8e+03  |
| total/epochs            | 1        |
| total/steps             | 729998   |
| total/steps_per_second  | 402      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.666    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 731000
Best mean reward: 94.30 - Last mean reward per episode: 91.65
Num timesteps: 732000
Best mean reward: 94.30 - Last mean reward per episode: 91.70
Num timesteps: 733000
Best mean reward: 94.30 - Last mean reward per episode: 91.59
Num timesteps: 734000
Best mean reward: 94.30 - Last mean reward per episode: 91.52
Num timesteps: 735000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 736000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
Num timesteps: 737000
Best mean reward: 94.30 - Last mean reward per episode: 91.92
Num timesteps: 738000
Best mean reward: 94.30 - Last mean reward per episode: 92.12
Num timesteps: 739000
Best mean reward: 94.30 - Last mean reward per episode: 92.13
Num timesteps: 740000
Best mean reward: 94.30 - Last mean reward per episode: 92.15
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 6.94     |
| reference_action_mean   | -0.377   |
| reference_action_std    | 0.835    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 7.19     |
| rollout/Q_mean          | 52.6     |
| rollout/actions_mean    | 0.18     |
| rollout/actions_std     | 0.785    |
| rollout/episode_steps   | 125      |
| rollout/episodes        | 5.9e+03  |
| rollout/return          | 89.6     |
| rollout/return_history  | 92.1     |
| total/duration          | 1.84e+03 |
| total/episodes          | 5.9e+03  |
| total/epochs            | 1        |
| total/steps             | 739998   |
| total/steps_per_second  | 402      |
| train/loss_actor        | -68.3    |
| train/loss_critic       | 0.732    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 741000
Best mean reward: 94.30 - Last mean reward per episode: 92.08
Num timesteps: 742000
Best mean reward: 94.30 - Last mean reward per episode: 92.03
Num timesteps: 743000
Best mean reward: 94.30 - Last mean reward per episode: 92.17
Num timesteps: 744000
Best mean reward: 94.30 - Last mean reward per episode: 92.25
Num timesteps: 745000
Best mean reward: 94.30 - Last mean reward per episode: 93.84
Num timesteps: 746000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 747000
Best mean reward: 94.30 - Last mean reward per episode: 93.83
Num timesteps: 748000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 749000
Best mean reward: 94.30 - Last mean reward per episode: 93.90
Num timesteps: 750000
Best mean reward: 94.30 - Last mean reward per episode: 93.86
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 6.61     |
| reference_action_mean   | -0.277   |
| reference_action_std    | 0.9      |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 6.56     |
| rollout/Q_mean          | 52.8     |
| rollout/actions_mean    | 0.182    |
| rollout/actions_std     | 0.785    |
| rollout/episode_steps   | 125      |
| rollout/episodes        | 6.02e+03 |
| rollout/return          | 89.7     |
| rollout/return_history  | 93.9     |
| total/duration          | 1.87e+03 |
| total/episodes          | 6.02e+03 |
| total/epochs            | 1        |
| total/steps             | 749998   |
| total/steps_per_second  | 402      |
| train/loss_actor        | -68.1    |
| train/loss_critic       | 0.624    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 751000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 752000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 753000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 754000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 755000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 756000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 757000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 758000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
Num timesteps: 759000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 760000
Best mean reward: 94.30 - Last mean reward per episode: 93.38
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 5.95     |
| reference_action_mean   | 0.172    |
| reference_action_std    | 0.924    |
| reference_actor_Q_mean  | 51.2     |
| reference_actor_Q_std   | 5.96     |
| rollout/Q_mean          | 53       |
| rollout/actions_mean    | 0.183    |
| rollout/actions_std     | 0.785    |
| rollout/episode_steps   | 124      |
| rollout/episodes        | 6.12e+03 |
| rollout/return          | 89.7     |
| rollout/return_history  | 93.4     |
| total/duration          | 1.89e+03 |
| total/episodes          | 6.12e+03 |
| total/epochs            | 1        |
| total/steps             | 759998   |
| total/steps_per_second  | 402      |
| train/loss_actor        | -68.3    |
| train/loss_critic       | 0.629    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 761000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
Num timesteps: 762000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 763000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 764000
Best mean reward: 94.30 - Last mean reward per episode: 93.38
Num timesteps: 765000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 766000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 767000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 768000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 769000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 770000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 5.79     |
| reference_action_mean   | 0.0945   |
| reference_action_std    | 0.963    |
| reference_actor_Q_mean  | 51.2     |
| reference_actor_Q_std   | 5.77     |
| rollout/Q_mean          | 53.2     |
| rollout/actions_mean    | 0.185    |
| rollout/actions_std     | 0.785    |
| rollout/episode_steps   | 124      |
| rollout/episodes        | 6.22e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 93.5     |
| total/duration          | 1.92e+03 |
| total/episodes          | 6.22e+03 |
| total/epochs            | 1        |
| total/steps             | 769998   |
| total/steps_per_second  | 402      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.728    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 771000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 772000
Best mean reward: 94.30 - Last mean reward per episode: 93.41
Num timesteps: 773000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 774000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 775000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 776000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 777000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 778000
Best mean reward: 94.30 - Last mean reward per episode: 92.18
Num timesteps: 779000
Best mean reward: 94.30 - Last mean reward per episode: 92.21
Num timesteps: 780000
Best mean reward: 94.30 - Last mean reward per episode: 92.13
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 5.95     |
| reference_action_mean   | 0.0951   |
| reference_action_std    | 0.96     |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 5.92     |
| rollout/Q_mean          | 53.4     |
| rollout/actions_mean    | 0.186    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 124      |
| rollout/episodes        | 6.31e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 92.1     |
| total/duration          | 1.94e+03 |
| total/episodes          | 6.31e+03 |
| total/epochs            | 1        |
| total/steps             | 779998   |
| total/steps_per_second  | 401      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 0.723    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 781000
Best mean reward: 94.30 - Last mean reward per episode: 92.31
Num timesteps: 782000
Best mean reward: 94.30 - Last mean reward per episode: 92.28
Num timesteps: 783000
Best mean reward: 94.30 - Last mean reward per episode: 92.14
Num timesteps: 784000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 785000
Best mean reward: 94.30 - Last mean reward per episode: 91.95
Num timesteps: 786000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 787000
Best mean reward: 94.30 - Last mean reward per episode: 91.92
Num timesteps: 788000
Best mean reward: 94.30 - Last mean reward per episode: 91.88
Num timesteps: 789000
Best mean reward: 94.30 - Last mean reward per episode: 91.91
Num timesteps: 790000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 5.9      |
| reference_action_mean   | -0.137   |
| reference_action_std    | 0.941    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 5.9      |
| rollout/Q_mean          | 53.6     |
| rollout/actions_mean    | 0.187    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 6.4e+03  |
| rollout/return          | 89.9     |
| rollout/return_history  | 93.2     |
| total/duration          | 1.97e+03 |
| total/episodes          | 6.4e+03  |
| total/epochs            | 1        |
| total/steps             | 789998   |
| total/steps_per_second  | 401      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 1.14     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 791000
Best mean reward: 94.30 - Last mean reward per episode: 93.29
Num timesteps: 792000
Best mean reward: 94.30 - Last mean reward per episode: 93.17
Num timesteps: 793000
Best mean reward: 94.30 - Last mean reward per episode: 93.15
Num timesteps: 794000
Best mean reward: 94.30 - Last mean reward per episode: 92.98
Num timesteps: 795000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 796000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 797000
Best mean reward: 94.30 - Last mean reward per episode: 93.31
Num timesteps: 798000
Best mean reward: 94.30 - Last mean reward per episode: 93.22
Num timesteps: 799000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 800000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 5.61     |
| reference_action_mean   | -0.212   |
| reference_action_std    | 0.908    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 5.71     |
| rollout/Q_mean          | 53.8     |
| rollout/actions_mean    | 0.189    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 6.5e+03  |
| rollout/return          | 89.9     |
| rollout/return_history  | 93.1     |
| total/duration          | 2e+03    |
| total/episodes          | 6.5e+03  |
| total/epochs            | 1        |
| total/steps             | 799998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | -68.3    |
| train/loss_critic       | 0.597    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 801000
Best mean reward: 94.30 - Last mean reward per episode: 92.99
Num timesteps: 802000
Best mean reward: 94.30 - Last mean reward per episode: 92.95
Num timesteps: 803000
Best mean reward: 94.30 - Last mean reward per episode: 91.64
Num timesteps: 804000
Best mean reward: 94.30 - Last mean reward per episode: 90.63
Num timesteps: 805000
Best mean reward: 94.30 - Last mean reward per episode: 89.62
Num timesteps: 806000
Best mean reward: 94.30 - Last mean reward per episode: 88.53
Num timesteps: 807000
Best mean reward: 94.30 - Last mean reward per episode: 88.38
Num timesteps: 808000
Best mean reward: 94.30 - Last mean reward per episode: 87.41
Num timesteps: 809000
Best mean reward: 94.30 - Last mean reward per episode: 86.44
Num timesteps: 810000
Best mean reward: 94.30 - Last mean reward per episode: 85.43
--------------------------------------
| reference_Q_mean        | 53       |
| reference_Q_std         | 5.07     |
| reference_action_mean   | 0.603    |
| reference_action_std    | 0.738    |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 5.04     |
| rollout/Q_mean          | 53.8     |
| rollout/actions_mean    | 0.187    |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 124      |
| rollout/episodes        | 6.52e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 85.4     |
| total/duration          | 2.02e+03 |
| total/episodes          | 6.52e+03 |
| total/epochs            | 1        |
| total/steps             | 809998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | -65.4    |
| train/loss_critic       | 0.781    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 811000
Best mean reward: 94.30 - Last mean reward per episode: 84.42
Num timesteps: 812000
Best mean reward: 94.30 - Last mean reward per episode: 83.42
Num timesteps: 813000
Best mean reward: 94.30 - Last mean reward per episode: 82.31
Num timesteps: 814000
Best mean reward: 94.30 - Last mean reward per episode: 80.94
Num timesteps: 815000
Best mean reward: 94.30 - Last mean reward per episode: 79.79
Num timesteps: 816000
Best mean reward: 94.30 - Last mean reward per episode: 78.48
Num timesteps: 817000
Best mean reward: 94.30 - Last mean reward per episode: 77.39
Num timesteps: 818000
Best mean reward: 94.30 - Last mean reward per episode: 76.11
Num timesteps: 819000
Best mean reward: 94.30 - Last mean reward per episode: 74.79
Num timesteps: 820000
Best mean reward: 94.30 - Last mean reward per episode: 73.47
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 5.3      |
| reference_action_mean   | -0.0155  |
| reference_action_std    | 0.893    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 5.37     |
| rollout/Q_mean          | 53.7     |
| rollout/actions_mean    | 0.188    |
| rollout/actions_std     | 0.777    |
| rollout/episode_steps   | 125      |
| rollout/episodes        | 6.53e+03 |
| rollout/return          | 89.6     |
| rollout/return_history  | 73.5     |
| total/duration          | 2.05e+03 |
| total/episodes          | 6.53e+03 |
| total/epochs            | 1        |
| total/steps             | 819998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | -60.6    |
| train/loss_critic       | 1.5      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 821000
Best mean reward: 94.30 - Last mean reward per episode: 71.98
Num timesteps: 822000
Best mean reward: 94.30 - Last mean reward per episode: 70.37
Num timesteps: 823000
Best mean reward: 94.30 - Last mean reward per episode: 68.72
Num timesteps: 824000
Best mean reward: 94.30 - Last mean reward per episode: 67.36
Num timesteps: 825000
Best mean reward: 94.30 - Last mean reward per episode: 66.01
Num timesteps: 826000
Best mean reward: 94.30 - Last mean reward per episode: 64.79
Num timesteps: 827000
Best mean reward: 94.30 - Last mean reward per episode: 63.69
Num timesteps: 828000
Best mean reward: 94.30 - Last mean reward per episode: 63.28
Num timesteps: 829000
Best mean reward: 94.30 - Last mean reward per episode: 61.83
Num timesteps: 830000
Best mean reward: 94.30 - Last mean reward per episode: 60.26
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 5.5      |
| reference_action_mean   | -0.262   |
| reference_action_std    | 0.839    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 5.64     |
| rollout/Q_mean          | 53.7     |
| rollout/actions_mean    | 0.18     |
| rollout/actions_std     | 0.778    |
| rollout/episode_steps   | 127      |
| rollout/episodes        | 6.54e+03 |
| rollout/return          | 89.4     |
| rollout/return_history  | 60.3     |
| total/duration          | 2.08e+03 |
| total/episodes          | 6.54e+03 |
| total/epochs            | 1        |
| total/steps             | 829998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | -55.9    |
| train/loss_critic       | 1.06     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 831000
Best mean reward: 94.30 - Last mean reward per episode: 59.15
Num timesteps: 832000
Best mean reward: 94.30 - Last mean reward per episode: 57.24
Num timesteps: 833000
Best mean reward: 94.30 - Last mean reward per episode: 56.59
Num timesteps: 834000
Best mean reward: 94.30 - Last mean reward per episode: 56.11
Num timesteps: 835000
Best mean reward: 94.30 - Last mean reward per episode: 54.73
Num timesteps: 836000
Best mean reward: 94.30 - Last mean reward per episode: 52.95
Num timesteps: 837000
Best mean reward: 94.30 - Last mean reward per episode: 51.43
Num timesteps: 838000
Best mean reward: 94.30 - Last mean reward per episode: 49.87
Num timesteps: 839000
Best mean reward: 94.30 - Last mean reward per episode: 48.05
Num timesteps: 840000
Best mean reward: 94.30 - Last mean reward per episode: 47.99
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 5.96     |
| reference_action_mean   | -0.303   |
| reference_action_std    | 0.842    |
| reference_actor_Q_mean  | 49.7     |
| reference_actor_Q_std   | 6.11     |
| rollout/Q_mean          | 53.7     |
| rollout/actions_mean    | 0.173    |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 128      |
| rollout/episodes        | 6.56e+03 |
| rollout/return          | 89.3     |
| rollout/return_history  | 48       |
| total/duration          | 2.1e+03  |
| total/episodes          | 6.56e+03 |
| total/epochs            | 1        |
| total/steps             | 839998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | -51.8    |
| train/loss_critic       | 0.911    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 841000
Best mean reward: 94.30 - Last mean reward per episode: 47.85
Num timesteps: 842000
Best mean reward: 94.30 - Last mean reward per episode: 47.77
Num timesteps: 843000
Best mean reward: 94.30 - Last mean reward per episode: 47.79
Num timesteps: 844000
Best mean reward: 94.30 - Last mean reward per episode: 47.86
Num timesteps: 845000
Best mean reward: 94.30 - Last mean reward per episode: 47.87
Num timesteps: 846000
Best mean reward: 94.30 - Last mean reward per episode: 46.44
Num timesteps: 847000
Best mean reward: 94.30 - Last mean reward per episode: 46.38
Num timesteps: 848000
Best mean reward: 94.30 - Last mean reward per episode: 49.80
Num timesteps: 849000
Best mean reward: 94.30 - Last mean reward per episode: 54.84
Num timesteps: 850000
Best mean reward: 94.30 - Last mean reward per episode: 69.06
--------------------------------------
| reference_Q_mean        | 48.4     |
| reference_Q_std         | 6.19     |
| reference_action_mean   | -0.517   |
| reference_action_std    | 0.806    |
| reference_actor_Q_mean  | 48.7     |
| reference_actor_Q_std   | 6.32     |
| rollout/Q_mean          | 53.8     |
| rollout/actions_mean    | 0.174    |
| rollout/actions_std     | 0.779    |
| rollout/episode_steps   | 128      |
| rollout/episodes        | 6.63e+03 |
| rollout/return          | 89.3     |
| rollout/return_history  | 69.1     |
| total/duration          | 2.13e+03 |
| total/episodes          | 6.63e+03 |
| total/epochs            | 1        |
| total/steps             | 849998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | -50.3    |
| train/loss_critic       | 2.2      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 851000
Best mean reward: 94.30 - Last mean reward per episode: 82.24
Num timesteps: 852000
Best mean reward: 94.30 - Last mean reward per episode: 91.30
Num timesteps: 853000
Best mean reward: 94.30 - Last mean reward per episode: 91.64
Num timesteps: 854000
Best mean reward: 94.30 - Last mean reward per episode: 91.81
Num timesteps: 855000
Best mean reward: 94.30 - Last mean reward per episode: 91.75
Num timesteps: 856000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 857000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 858000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 859000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 860000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
--------------------------------------
| reference_Q_mean        | 47.5     |
| reference_Q_std         | 6.45     |
| reference_action_mean   | -0.363   |
| reference_action_std    | 0.866    |
| reference_actor_Q_mean  | 47.9     |
| reference_actor_Q_std   | 6.49     |
| rollout/Q_mean          | 54       |
| rollout/actions_mean    | 0.176    |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 127      |
| rollout/episodes        | 6.75e+03 |
| rollout/return          | 89.4     |
| rollout/return_history  | 93.8     |
| total/duration          | 2.15e+03 |
| total/episodes          | 6.75e+03 |
| total/epochs            | 1        |
| total/steps             | 859998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | -53.4    |
| train/loss_critic       | 1.96     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 861000
Best mean reward: 94.30 - Last mean reward per episode: 93.83
Num timesteps: 862000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 863000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 864000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 865000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 866000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 867000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
Num timesteps: 868000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 869000
Best mean reward: 94.30 - Last mean reward per episode: 93.55
Num timesteps: 870000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
--------------------------------------
| reference_Q_mean        | 47.6     |
| reference_Q_std         | 6.68     |
| reference_action_mean   | -0.223   |
| reference_action_std    | 0.931    |
| reference_actor_Q_mean  | 48.1     |
| reference_actor_Q_std   | 6.63     |
| rollout/Q_mean          | 54.1     |
| rollout/actions_mean    | 0.176    |
| rollout/actions_std     | 0.78     |
| rollout/episode_steps   | 126      |
| rollout/episodes        | 6.88e+03 |
| rollout/return          | 89.4     |
| rollout/return_history  | 93.7     |
| total/duration          | 2.18e+03 |
| total/episodes          | 6.88e+03 |
| total/epochs            | 1        |
| total/steps             | 869998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | -58.6    |
| train/loss_critic       | 0.921    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 871000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 872000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 873000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 874000
Best mean reward: 94.30 - Last mean reward per episode: 93.77
Num timesteps: 875000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 876000
Best mean reward: 94.30 - Last mean reward per episode: 92.35
Num timesteps: 877000
Best mean reward: 94.30 - Last mean reward per episode: 92.39
Num timesteps: 878000
Best mean reward: 94.30 - Last mean reward per episode: 92.39
Num timesteps: 879000
Best mean reward: 94.30 - Last mean reward per episode: 92.41
Num timesteps: 880000
Best mean reward: 94.30 - Last mean reward per episode: 92.46
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 6.57     |
| reference_action_mean   | -0.184   |
| reference_action_std    | 0.946    |
| reference_actor_Q_mean  | 49.4     |
| reference_actor_Q_std   | 6.69     |
| rollout/Q_mean          | 54.3     |
| rollout/actions_mean    | 0.176    |
| rollout/actions_std     | 0.781    |
| rollout/episode_steps   | 126      |
| rollout/episodes        | 7e+03    |
| rollout/return          | 89.5     |
| rollout/return_history  | 92.5     |
| total/duration          | 2.2e+03  |
| total/episodes          | 7e+03    |
| total/epochs            | 1        |
| total/steps             | 879998   |
| total/steps_per_second  | 399      |
| train/loss_actor        | -64.2    |
| train/loss_critic       | 1.71     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 881000
Best mean reward: 94.30 - Last mean reward per episode: 92.43
Num timesteps: 882000
Best mean reward: 94.30 - Last mean reward per episode: 92.39
Num timesteps: 883000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
Num timesteps: 884000
Best mean reward: 94.30 - Last mean reward per episode: 93.91
Num timesteps: 885000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
Num timesteps: 886000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 887000
Best mean reward: 94.30 - Last mean reward per episode: 93.77
Num timesteps: 888000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 889000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 890000
Best mean reward: 94.30 - Last mean reward per episode: 93.21
--------------------------------------
| reference_Q_mean        | 48.8     |
| reference_Q_std         | 7.41     |
| reference_action_mean   | -0.543   |
| reference_action_std    | 0.742    |
| reference_actor_Q_mean  | 49.5     |
| reference_actor_Q_std   | 7.02     |
| rollout/Q_mean          | 54.5     |
| rollout/actions_mean    | 0.176    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 125      |
| rollout/episodes        | 7.12e+03 |
| rollout/return          | 89.6     |
| rollout/return_history  | 93.2     |
| total/duration          | 2.23e+03 |
| total/episodes          | 7.12e+03 |
| total/epochs            | 1        |
| total/steps             | 889998   |
| total/steps_per_second  | 399      |
| train/loss_actor        | -68.1    |
| train/loss_critic       | 1.8      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 891000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
Num timesteps: 892000
Best mean reward: 94.30 - Last mean reward per episode: 93.29
Num timesteps: 893000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 894000
Best mean reward: 94.30 - Last mean reward per episode: 93.30
Num timesteps: 895000
Best mean reward: 94.30 - Last mean reward per episode: 91.99
Num timesteps: 896000
Best mean reward: 94.30 - Last mean reward per episode: 91.99
Num timesteps: 897000
Best mean reward: 94.30 - Last mean reward per episode: 90.56
Num timesteps: 898000
Best mean reward: 94.30 - Last mean reward per episode: 90.81
Num timesteps: 899000
Best mean reward: 94.30 - Last mean reward per episode: 90.83
Num timesteps: 900000
Best mean reward: 94.30 - Last mean reward per episode: 91.01
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 6.57     |
| reference_action_mean   | -0.665   |
| reference_action_std    | 0.642    |
| reference_actor_Q_mean  | 49.6     |
| reference_actor_Q_std   | 6.31     |
| rollout/Q_mean          | 54.6     |
| rollout/actions_mean    | 0.174    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 125      |
| rollout/episodes        | 7.21e+03 |
| rollout/return          | 89.6     |
| rollout/return_history  | 91       |
| total/duration          | 2.26e+03 |
| total/episodes          | 7.21e+03 |
| total/epochs            | 1        |
| total/steps             | 899998   |
| total/steps_per_second  | 399      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 1.42     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 901000
Best mean reward: 94.30 - Last mean reward per episode: 90.98
Num timesteps: 902000
Best mean reward: 94.30 - Last mean reward per episode: 90.85
Num timesteps: 903000
Best mean reward: 94.30 - Last mean reward per episode: 89.03
Num timesteps: 904000
Best mean reward: 94.30 - Last mean reward per episode: 89.07
Num timesteps: 905000
Best mean reward: 94.30 - Last mean reward per episode: 89.09
Num timesteps: 906000
Best mean reward: 94.30 - Last mean reward per episode: 88.76
Num timesteps: 907000
Best mean reward: 94.30 - Last mean reward per episode: 88.54
Num timesteps: 908000
Best mean reward: 94.30 - Last mean reward per episode: 88.47
Num timesteps: 909000
Best mean reward: 94.30 - Last mean reward per episode: 89.87
Num timesteps: 910000
Best mean reward: 94.30 - Last mean reward per episode: 91.25
--------------------------------------
| reference_Q_mean        | 49.8     |
| reference_Q_std         | 6.42     |
| reference_action_mean   | -0.286   |
| reference_action_std    | 0.779    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 6.23     |
| rollout/Q_mean          | 54.7     |
| rollout/actions_mean    | 0.171    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 125      |
| rollout/episodes        | 7.28e+03 |
| rollout/return          | 89.6     |
| rollout/return_history  | 91.3     |
| total/duration          | 2.28e+03 |
| total/episodes          | 7.28e+03 |
| total/epochs            | 1        |
| total/steps             | 909998   |
| total/steps_per_second  | 399      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.551    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 911000
Best mean reward: 94.30 - Last mean reward per episode: 91.23
Num timesteps: 912000
Best mean reward: 94.30 - Last mean reward per episode: 91.31
Num timesteps: 913000
Best mean reward: 94.30 - Last mean reward per episode: 91.34
Num timesteps: 914000
Best mean reward: 94.30 - Last mean reward per episode: 91.42
Num timesteps: 915000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 916000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 917000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 918000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 919000
Best mean reward: 94.30 - Last mean reward per episode: 93.84
Num timesteps: 920000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 6.4      |
| reference_action_mean   | 0.0101   |
| reference_action_std    | 0.916    |
| reference_actor_Q_mean  | 51.1     |
| reference_actor_Q_std   | 6.49     |
| rollout/Q_mean          | 54.9     |
| rollout/actions_mean    | 0.17     |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 125      |
| rollout/episodes        | 7.39e+03 |
| rollout/return          | 89.6     |
| rollout/return_history  | 93.8     |
| total/duration          | 2.31e+03 |
| total/episodes          | 7.39e+03 |
| total/epochs            | 1        |
| total/steps             | 919998   |
| total/steps_per_second  | 399      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.775    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 921000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 922000
Best mean reward: 94.30 - Last mean reward per episode: 93.84
Num timesteps: 923000
Best mean reward: 94.30 - Last mean reward per episode: 93.94
Num timesteps: 924000
Best mean reward: 94.30 - Last mean reward per episode: 93.98
Num timesteps: 925000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 926000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 927000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 928000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 929000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 930000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 6.54     |
| reference_action_mean   | -0.00435 |
| reference_action_std    | 0.958    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 6.45     |
| rollout/Q_mean          | 55       |
| rollout/actions_mean    | 0.17     |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 124      |
| rollout/episodes        | 7.49e+03 |
| rollout/return          | 89.7     |
| rollout/return_history  | 93.3     |
| total/duration          | 2.33e+03 |
| total/episodes          | 7.49e+03 |
| total/epochs            | 1        |
| total/steps             | 929998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.544    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 931000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 932000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 933000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 934000
Best mean reward: 94.30 - Last mean reward per episode: 93.41
Num timesteps: 935000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 936000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 937000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 938000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 939000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
Num timesteps: 940000
Best mean reward: 94.30 - Last mean reward per episode: 93.31
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 6.18     |
| reference_action_mean   | -0.0713  |
| reference_action_std    | 0.944    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 5.96     |
| rollout/Q_mean          | 55.2     |
| rollout/actions_mean    | 0.17     |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 124      |
| rollout/episodes        | 7.58e+03 |
| rollout/return          | 89.7     |
| rollout/return_history  | 93.3     |
| total/duration          | 2.36e+03 |
| total/episodes          | 7.58e+03 |
| total/epochs            | 1        |
| total/steps             | 939998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.69     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 941000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 942000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 943000
Best mean reward: 94.30 - Last mean reward per episode: 93.30
Num timesteps: 944000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
Num timesteps: 945000
Best mean reward: 94.30 - Last mean reward per episode: 92.00
Num timesteps: 946000
Best mean reward: 94.30 - Last mean reward per episode: 91.88
Num timesteps: 947000
Best mean reward: 94.30 - Last mean reward per episode: 91.82
Num timesteps: 948000
Best mean reward: 94.30 - Last mean reward per episode: 92.08
Num timesteps: 949000
Best mean reward: 94.30 - Last mean reward per episode: 92.18
Num timesteps: 950000
Best mean reward: 94.30 - Last mean reward per episode: 92.23
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 5.74     |
| reference_action_mean   | -0.0875  |
| reference_action_std    | 0.968    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 5.68     |
| rollout/Q_mean          | 55.3     |
| rollout/actions_mean    | 0.171    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 124      |
| rollout/episodes        | 7.68e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 92.2     |
| total/duration          | 2.38e+03 |
| total/episodes          | 7.68e+03 |
| total/epochs            | 1        |
| total/steps             | 949998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | -68.5    |
| train/loss_critic       | 0.502    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 951000
Best mean reward: 94.30 - Last mean reward per episode: 92.06
Num timesteps: 952000
Best mean reward: 94.30 - Last mean reward per episode: 91.87
Num timesteps: 953000
Best mean reward: 94.30 - Last mean reward per episode: 91.74
Num timesteps: 954000
Best mean reward: 94.30 - Last mean reward per episode: 91.68
Num timesteps: 955000
Best mean reward: 94.30 - Last mean reward per episode: 91.78
Num timesteps: 956000
Best mean reward: 94.30 - Last mean reward per episode: 93.17
Num timesteps: 957000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 958000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 959000
Best mean reward: 94.30 - Last mean reward per episode: 93.05
Num timesteps: 960000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 6        |
| reference_action_mean   | -0.196   |
| reference_action_std    | 0.941    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 6.02     |
| rollout/Q_mean          | 55.5     |
| rollout/actions_mean    | 0.172    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 7.77e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 93.1     |
| total/duration          | 2.41e+03 |
| total/episodes          | 7.77e+03 |
| total/epochs            | 1        |
| total/steps             | 959998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 0.576    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 961000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 962000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 963000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 964000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 965000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 966000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 967000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
Num timesteps: 968000
Best mean reward: 94.30 - Last mean reward per episode: 93.17
Num timesteps: 969000
Best mean reward: 94.30 - Last mean reward per episode: 93.04
Num timesteps: 970000
Best mean reward: 94.30 - Last mean reward per episode: 92.97
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 5.49     |
| reference_action_mean   | 0.207    |
| reference_action_std    | 0.928    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 5.24     |
| rollout/Q_mean          | 55.6     |
| rollout/actions_mean    | 0.174    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 7.87e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 93       |
| total/duration          | 2.43e+03 |
| total/episodes          | 7.87e+03 |
| total/epochs            | 1        |
| total/steps             | 969998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | -68.4    |
| train/loss_critic       | 0.78     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 971000
Best mean reward: 94.30 - Last mean reward per episode: 92.84
Num timesteps: 972000
Best mean reward: 94.30 - Last mean reward per episode: 92.68
Num timesteps: 973000
Best mean reward: 94.30 - Last mean reward per episode: 92.60
Num timesteps: 974000
Best mean reward: 94.30 - Last mean reward per episode: 92.56
Num timesteps: 975000
Best mean reward: 94.30 - Last mean reward per episode: 92.69
Num timesteps: 976000
Best mean reward: 94.30 - Last mean reward per episode: 92.58
Num timesteps: 977000
Best mean reward: 94.30 - Last mean reward per episode: 92.69
Num timesteps: 978000
Best mean reward: 94.30 - Last mean reward per episode: 92.65
Num timesteps: 979000
Best mean reward: 94.30 - Last mean reward per episode: 91.36
Num timesteps: 980000
Best mean reward: 94.30 - Last mean reward per episode: 91.40
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 4.81     |
| reference_action_mean   | 0.215    |
| reference_action_std    | 0.947    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 4.63     |
| rollout/Q_mean          | 55.7     |
| rollout/actions_mean    | 0.174    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 7.96e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 91.4     |
| total/duration          | 2.46e+03 |
| total/episodes          | 7.96e+03 |
| total/epochs            | 1        |
| total/steps             | 979998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | -68.3    |
| train/loss_critic       | 0.858    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 981000
Best mean reward: 94.30 - Last mean reward per episode: 91.43
Num timesteps: 982000
Best mean reward: 94.30 - Last mean reward per episode: 91.45
Num timesteps: 983000
Best mean reward: 94.30 - Last mean reward per episode: 91.21
Num timesteps: 984000
Best mean reward: 94.30 - Last mean reward per episode: 91.45
Num timesteps: 985000
Best mean reward: 94.30 - Last mean reward per episode: 91.42
Num timesteps: 986000
Best mean reward: 94.30 - Last mean reward per episode: 91.48
Num timesteps: 987000
Best mean reward: 94.30 - Last mean reward per episode: 91.47
Num timesteps: 988000
Best mean reward: 94.30 - Last mean reward per episode: 91.45
Num timesteps: 989000
Best mean reward: 94.30 - Last mean reward per episode: 92.67
Num timesteps: 990000
Best mean reward: 94.30 - Last mean reward per episode: 92.54
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 4.54     |
| reference_action_mean   | 0.307    |
| reference_action_std    | 0.927    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 4.47     |
| rollout/Q_mean          | 55.9     |
| rollout/actions_mean    | 0.176    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 8.06e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 92.5     |
| total/duration          | 2.49e+03 |
| total/episodes          | 8.06e+03 |
| total/epochs            | 1        |
| total/steps             | 989998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | -68.1    |
| train/loss_critic       | 1.54     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 991000
Best mean reward: 94.30 - Last mean reward per episode: 92.48
Num timesteps: 992000
Best mean reward: 94.30 - Last mean reward per episode: 92.09
Num timesteps: 993000
Best mean reward: 94.30 - Last mean reward per episode: 92.14
Num timesteps: 994000
Best mean reward: 94.30 - Last mean reward per episode: 92.10
Num timesteps: 995000
Best mean reward: 94.30 - Last mean reward per episode: 91.90
Num timesteps: 996000
Best mean reward: 94.30 - Last mean reward per episode: 91.79
Num timesteps: 997000
Best mean reward: 94.30 - Last mean reward per episode: 91.67
Num timesteps: 998000
Best mean reward: 94.30 - Last mean reward per episode: 91.54
Num timesteps: 999000
Best mean reward: 94.30 - Last mean reward per episode: 91.55
Num timesteps: 1000000
Best mean reward: 94.30 - Last mean reward per episode: 91.50
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 4.76     |
| reference_action_mean   | 0.776    |
| reference_action_std    | 0.598    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 4.97     |
| rollout/Q_mean          | 56       |
| rollout/actions_mean    | 0.178    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 8.15e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 91.5     |
| total/duration          | 2.51e+03 |
| total/episodes          | 8.15e+03 |
| total/epochs            | 1        |
| total/steps             | 999998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | -67.2    |
| train/loss_critic       | 0.619    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1001000
Best mean reward: 94.30 - Last mean reward per episode: 91.37
Num timesteps: 1002000
Best mean reward: 94.30 - Last mean reward per episode: 91.12
Num timesteps: 1003000
Best mean reward: 94.30 - Last mean reward per episode: 90.80
Num timesteps: 1004000
Best mean reward: 94.30 - Last mean reward per episode: 90.74
Num timesteps: 1005000
Best mean reward: 94.30 - Last mean reward per episode: 89.32
Num timesteps: 1006000
Best mean reward: 94.30 - Last mean reward per episode: 89.16
Num timesteps: 1007000
Best mean reward: 94.30 - Last mean reward per episode: 87.62
Num timesteps: 1008000
Best mean reward: 94.30 - Last mean reward per episode: 87.53
Num timesteps: 1009000
Best mean reward: 94.30 - Last mean reward per episode: 86.27
Num timesteps: 1010000
Best mean reward: 94.30 - Last mean reward per episode: 86.03
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 5.27     |
| reference_action_mean   | 0.695    |
| reference_action_std    | 0.685    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 5.44     |
| rollout/Q_mean          | 56       |
| rollout/actions_mean    | 0.181    |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 8.18e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 86       |
| total/duration          | 2.54e+03 |
| total/episodes          | 8.18e+03 |
| total/epochs            | 1        |
| total/steps             | 1009998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -64      |
| train/loss_critic       | 0.873    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1011000
Best mean reward: 94.30 - Last mean reward per episode: 84.80
Num timesteps: 1012000
Best mean reward: 94.30 - Last mean reward per episode: 83.80
Num timesteps: 1013000
Best mean reward: 94.30 - Last mean reward per episode: 82.10
Num timesteps: 1014000
Best mean reward: 94.30 - Last mean reward per episode: 82.46
Num timesteps: 1015000
Best mean reward: 94.30 - Last mean reward per episode: 82.65
Num timesteps: 1016000
Best mean reward: 94.30 - Last mean reward per episode: 82.79
Num timesteps: 1017000
Best mean reward: 94.30 - Last mean reward per episode: 83.01
Num timesteps: 1018000
Best mean reward: 94.30 - Last mean reward per episode: 83.30
Num timesteps: 1019000
Best mean reward: 94.30 - Last mean reward per episode: 83.45
Num timesteps: 1020000
Best mean reward: 94.30 - Last mean reward per episode: 82.01
--------------------------------------
| reference_Q_mean        | 50.9     |
| reference_Q_std         | 5.73     |
| reference_action_mean   | 0.134    |
| reference_action_std    | 0.908    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 5.96     |
| rollout/Q_mean          | 56       |
| rollout/actions_mean    | 0.18     |
| rollout/actions_std     | 0.783    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 8.26e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 82       |
| total/duration          | 2.56e+03 |
| total/episodes          | 8.26e+03 |
| total/epochs            | 1        |
| total/steps             | 1019998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -62.5    |
| train/loss_critic       | 1.24     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1021000
Best mean reward: 94.30 - Last mean reward per episode: 81.75
Num timesteps: 1022000
Best mean reward: 94.30 - Last mean reward per episode: 85.59
Num timesteps: 1023000
Best mean reward: 94.30 - Last mean reward per episode: 91.00
Num timesteps: 1024000
Best mean reward: 94.30 - Last mean reward per episode: 91.18
Num timesteps: 1025000
Best mean reward: 94.30 - Last mean reward per episode: 91.08
Num timesteps: 1026000
Best mean reward: 94.30 - Last mean reward per episode: 91.16
Num timesteps: 1027000
Best mean reward: 94.30 - Last mean reward per episode: 91.33
Num timesteps: 1028000
Best mean reward: 94.30 - Last mean reward per episode: 91.35
Num timesteps: 1029000
Best mean reward: 94.30 - Last mean reward per episode: 91.38
Num timesteps: 1030000
Best mean reward: 94.30 - Last mean reward per episode: 89.79
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 6.05     |
| reference_action_mean   | -0.0354  |
| reference_action_std    | 0.833    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 6.16     |
| rollout/Q_mean          | 56.1     |
| rollout/actions_mean    | 0.179    |
| rollout/actions_std     | 0.784    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 8.35e+03 |
| rollout/return          | 89.8     |
| rollout/return_history  | 89.8     |
| total/duration          | 2.59e+03 |
| total/episodes          | 8.35e+03 |
| total/epochs            | 1        |
| total/steps             | 1029998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -62.5    |
| train/loss_critic       | 0.649    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1031000
Best mean reward: 94.30 - Last mean reward per episode: 91.99
Num timesteps: 1032000
Best mean reward: 94.30 - Last mean reward per episode: 91.99
Num timesteps: 1033000
Best mean reward: 94.30 - Last mean reward per episode: 91.95
Num timesteps: 1034000
Best mean reward: 94.30 - Last mean reward per episode: 90.40
Num timesteps: 1035000
Best mean reward: 94.30 - Last mean reward per episode: 90.40
Num timesteps: 1036000
Best mean reward: 94.30 - Last mean reward per episode: 90.53
Num timesteps: 1037000
Best mean reward: 94.30 - Last mean reward per episode: 90.41
Num timesteps: 1038000
Best mean reward: 94.30 - Last mean reward per episode: 90.36
Num timesteps: 1039000
Best mean reward: 94.30 - Last mean reward per episode: 90.36
Num timesteps: 1040000
Best mean reward: 94.30 - Last mean reward per episode: 92.13
--------------------------------------
| reference_Q_mean        | 49.8     |
| reference_Q_std         | 6.08     |
| reference_action_mean   | -0.184   |
| reference_action_std    | 0.833    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 6.11     |
| rollout/Q_mean          | 56.2     |
| rollout/actions_mean    | 0.178    |
| rollout/actions_std     | 0.785    |
| rollout/episode_steps   | 123      |
| rollout/episodes        | 8.46e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 92.1     |
| total/duration          | 2.61e+03 |
| total/episodes          | 8.46e+03 |
| total/epochs            | 1        |
| total/steps             | 1039998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -62.5    |
| train/loss_critic       | 0.601    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1041000
Best mean reward: 94.30 - Last mean reward per episode: 92.15
Num timesteps: 1042000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
Num timesteps: 1043000
Best mean reward: 94.30 - Last mean reward per episode: 93.88
Num timesteps: 1044000
Best mean reward: 94.30 - Last mean reward per episode: 93.86
Num timesteps: 1045000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 1046000
Best mean reward: 94.30 - Last mean reward per episode: 93.86
Num timesteps: 1047000
Best mean reward: 94.30 - Last mean reward per episode: 93.91
Num timesteps: 1048000
Best mean reward: 94.30 - Last mean reward per episode: 93.79
Num timesteps: 1049000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 1050000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 5.93     |
| reference_action_mean   | -0.566   |
| reference_action_std    | 0.709    |
| reference_actor_Q_mean  | 49.8     |
| reference_actor_Q_std   | 6        |
| rollout/Q_mean          | 56.4     |
| rollout/actions_mean    | 0.177    |
| rollout/actions_std     | 0.785    |
| rollout/episode_steps   | 122      |
| rollout/episodes        | 8.58e+03 |
| rollout/return          | 89.9     |
| rollout/return_history  | 93.7     |
| total/duration          | 2.64e+03 |
| total/episodes          | 8.58e+03 |
| total/epochs            | 1        |
| total/steps             | 1049998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -63.6    |
| train/loss_critic       | 0.848    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1051000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 1052000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1053000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 1054000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 1055000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 1056000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 1057000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1058000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 1059000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 1060000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
--------------------------------------
| reference_Q_mean        | 49.2     |
| reference_Q_std         | 5.69     |
| reference_action_mean   | -0.413   |
| reference_action_std    | 0.818    |
| reference_actor_Q_mean  | 49.9     |
| reference_actor_Q_std   | 5.74     |
| rollout/Q_mean          | 56.5     |
| rollout/actions_mean    | 0.176    |
| rollout/actions_std     | 0.786    |
| rollout/episode_steps   | 122      |
| rollout/episodes        | 8.7e+03  |
| rollout/return          | 90       |
| rollout/return_history  | 93.7     |
| total/duration          | 2.66e+03 |
| total/episodes          | 8.7e+03  |
| total/epochs            | 1        |
| total/steps             | 1059998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -67.1    |
| train/loss_critic       | 0.475    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1061000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 1062000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 1063000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 1064000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1065000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 1066000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 1067000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 1068000
Best mean reward: 94.30 - Last mean reward per episode: 93.76
Num timesteps: 1069000
Best mean reward: 94.30 - Last mean reward per episode: 93.76
Num timesteps: 1070000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 5.54     |
| reference_action_mean   | -0.67    |
| reference_action_std    | 0.666    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 5.52     |
| rollout/Q_mean          | 56.6     |
| rollout/actions_mean    | 0.175    |
| rollout/actions_std     | 0.787    |
| rollout/episode_steps   | 121      |
| rollout/episodes        | 8.82e+03 |
| rollout/return          | 90       |
| rollout/return_history  | 93.7     |
| total/duration          | 2.69e+03 |
| total/episodes          | 8.82e+03 |
| total/epochs            | 1        |
| total/steps             | 1069998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.867    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1071000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 1072000
Best mean reward: 94.30 - Last mean reward per episode: 93.83
Num timesteps: 1073000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 1074000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 1075000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 1076000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 1077000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 1078000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
Num timesteps: 1079000
Best mean reward: 94.30 - Last mean reward per episode: 93.09
Num timesteps: 1080000
Best mean reward: 94.30 - Last mean reward per episode: 93.06
--------------------------------------
| reference_Q_mean        | 48.7     |
| reference_Q_std         | 5.55     |
| reference_action_mean   | -0.559   |
| reference_action_std    | 0.72     |
| reference_actor_Q_mean  | 49.3     |
| reference_actor_Q_std   | 5.63     |
| rollout/Q_mean          | 56.7     |
| rollout/actions_mean    | 0.173    |
| rollout/actions_std     | 0.788    |
| rollout/episode_steps   | 121      |
| rollout/episodes        | 8.92e+03 |
| rollout/return          | 90.1     |
| rollout/return_history  | 93.1     |
| total/duration          | 2.71e+03 |
| total/episodes          | 8.92e+03 |
| total/epochs            | 1        |
| total/steps             | 1079998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.606    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1081000
Best mean reward: 94.30 - Last mean reward per episode: 92.88
Num timesteps: 1082000
Best mean reward: 94.30 - Last mean reward per episode: 92.77
Num timesteps: 1083000
Best mean reward: 94.30 - Last mean reward per episode: 92.84
Num timesteps: 1084000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 1085000
Best mean reward: 94.30 - Last mean reward per episode: 92.92
Num timesteps: 1086000
Best mean reward: 94.30 - Last mean reward per episode: 92.89
Num timesteps: 1087000
Best mean reward: 94.30 - Last mean reward per episode: 92.13
Num timesteps: 1088000
Best mean reward: 94.30 - Last mean reward per episode: 91.87
Num timesteps: 1089000
Best mean reward: 94.30 - Last mean reward per episode: 91.95
Num timesteps: 1090000
Best mean reward: 94.30 - Last mean reward per episode: 91.93
--------------------------------------
| reference_Q_mean        | 48.3     |
| reference_Q_std         | 5.76     |
| reference_action_mean   | -0.33    |
| reference_action_std    | 0.874    |
| reference_actor_Q_mean  | 49.1     |
| reference_actor_Q_std   | 5.89     |
| rollout/Q_mean          | 56.8     |
| rollout/actions_mean    | 0.17     |
| rollout/actions_std     | 0.79     |
| rollout/episode_steps   | 121      |
| rollout/episodes        | 9.01e+03 |
| rollout/return          | 90.1     |
| rollout/return_history  | 91.9     |
| total/duration          | 2.74e+03 |
| total/episodes          | 9.01e+03 |
| total/epochs            | 1        |
| total/steps             | 1089998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.623    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1091000
Best mean reward: 94.30 - Last mean reward per episode: 90.57
Num timesteps: 1092000
Best mean reward: 94.30 - Last mean reward per episode: 90.62
Num timesteps: 1093000
Best mean reward: 94.30 - Last mean reward per episode: 90.79
Num timesteps: 1094000
Best mean reward: 94.30 - Last mean reward per episode: 90.89
Num timesteps: 1095000
Best mean reward: 94.30 - Last mean reward per episode: 90.90
Num timesteps: 1096000
Best mean reward: 94.30 - Last mean reward per episode: 91.94
Num timesteps: 1097000
Best mean reward: 94.30 - Last mean reward per episode: 92.13
Num timesteps: 1098000
Best mean reward: 94.30 - Last mean reward per episode: 92.14
Num timesteps: 1099000
Best mean reward: 94.30 - Last mean reward per episode: 92.35
Num timesteps: 1100000
Best mean reward: 94.30 - Last mean reward per episode: 92.38
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 5.51     |
| reference_action_mean   | 0.0924   |
| reference_action_std    | 0.967    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 5.6      |
| rollout/Q_mean          | 56.9     |
| rollout/actions_mean    | 0.169    |
| rollout/actions_std     | 0.79     |
| rollout/episode_steps   | 121      |
| rollout/episodes        | 9.12e+03 |
| rollout/return          | 90.1     |
| rollout/return_history  | 92.4     |
| total/duration          | 2.77e+03 |
| total/episodes          | 9.12e+03 |
| total/epochs            | 1        |
| total/steps             | 1099998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 0.775    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1101000
Best mean reward: 94.30 - Last mean reward per episode: 90.69
Num timesteps: 1102000
Best mean reward: 94.30 - Last mean reward per episode: 90.54
Num timesteps: 1103000
Best mean reward: 94.30 - Last mean reward per episode: 90.42
Num timesteps: 1104000
Best mean reward: 94.30 - Last mean reward per episode: 90.42
Num timesteps: 1105000
Best mean reward: 94.30 - Last mean reward per episode: 90.33
Num timesteps: 1106000
Best mean reward: 94.30 - Last mean reward per episode: 90.19
Num timesteps: 1107000
Best mean reward: 94.30 - Last mean reward per episode: 90.18
Num timesteps: 1108000
Best mean reward: 94.30 - Last mean reward per episode: 90.16
Num timesteps: 1109000
Best mean reward: 94.30 - Last mean reward per episode: 91.59
Num timesteps: 1110000
Best mean reward: 94.30 - Last mean reward per episode: 91.56
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 5.61     |
| reference_action_mean   | -0.114   |
| reference_action_std    | 0.961    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 5.77     |
| rollout/Q_mean          | 57       |
| rollout/actions_mean    | 0.169    |
| rollout/actions_std     | 0.791    |
| rollout/episode_steps   | 120      |
| rollout/episodes        | 9.22e+03 |
| rollout/return          | 90.1     |
| rollout/return_history  | 91.6     |
| total/duration          | 2.79e+03 |
| total/episodes          | 9.22e+03 |
| total/epochs            | 1        |
| total/steps             | 1109998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -68.6    |
| train/loss_critic       | 0.978    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1111000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
Num timesteps: 1112000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1113000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 1114000
Best mean reward: 94.30 - Last mean reward per episode: 93.79
Num timesteps: 1115000
Best mean reward: 94.30 - Last mean reward per episode: 92.40
Num timesteps: 1116000
Best mean reward: 94.30 - Last mean reward per episode: 92.47
Num timesteps: 1117000
Best mean reward: 94.30 - Last mean reward per episode: 92.56
Num timesteps: 1118000
Best mean reward: 94.30 - Last mean reward per episode: 92.59
Num timesteps: 1119000
Best mean reward: 94.30 - Last mean reward per episode: 92.51
Num timesteps: 1120000
Best mean reward: 94.30 - Last mean reward per episode: 92.54
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 6.05     |
| reference_action_mean   | -0.183   |
| reference_action_std    | 0.94     |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 6.18     |
| rollout/Q_mean          | 57.2     |
| rollout/actions_mean    | 0.168    |
| rollout/actions_std     | 0.791    |
| rollout/episode_steps   | 120      |
| rollout/episodes        | 9.34e+03 |
| rollout/return          | 90.1     |
| rollout/return_history  | 92.5     |
| total/duration          | 2.82e+03 |
| total/episodes          | 9.34e+03 |
| total/epochs            | 1        |
| total/steps             | 1119998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -68.8    |
| train/loss_critic       | 2.06     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1121000
Best mean reward: 94.30 - Last mean reward per episode: 92.39
Num timesteps: 1122000
Best mean reward: 94.30 - Last mean reward per episode: 92.39
Num timesteps: 1123000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 1124000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 1125000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
Num timesteps: 1126000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
Num timesteps: 1127000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 1128000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 1129000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 1130000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
--------------------------------------
| reference_Q_mean        | 49.7     |
| reference_Q_std         | 6.37     |
| reference_action_mean   | -0.212   |
| reference_action_std    | 0.95     |
| reference_actor_Q_mean  | 50.2     |
| reference_actor_Q_std   | 6.49     |
| rollout/Q_mean          | 57.3     |
| rollout/actions_mean    | 0.168    |
| rollout/actions_std     | 0.792    |
| rollout/episode_steps   | 119      |
| rollout/episodes        | 9.46e+03 |
| rollout/return          | 90.2     |
| rollout/return_history  | 93.7     |
| total/duration          | 2.84e+03 |
| total/episodes          | 9.46e+03 |
| total/epochs            | 1        |
| total/steps             | 1129998  |
| total/steps_per_second  | 398      |
| train/loss_actor        | -69      |
| train/loss_critic       | 1.19     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1131000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 1132000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 1133000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
Num timesteps: 1134000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 1135000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 1136000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1137000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 1138000
Best mean reward: 94.30 - Last mean reward per episode: 92.02
Num timesteps: 1139000
Best mean reward: 94.30 - Last mean reward per episode: 92.07
Num timesteps: 1140000
Best mean reward: 94.30 - Last mean reward per episode: 92.05
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 6.36     |
| reference_action_mean   | -0.216   |
| reference_action_std    | 0.952    |
| reference_actor_Q_mean  | 50.6     |
| reference_actor_Q_std   | 6.35     |
| rollout/Q_mean          | 57.4     |
| rollout/actions_mean    | 0.167    |
| rollout/actions_std     | 0.793    |
| rollout/episode_steps   | 119      |
| rollout/episodes        | 9.57e+03 |
| rollout/return          | 90.2     |
| rollout/return_history  | 92.1     |
| total/duration          | 2.87e+03 |
| total/episodes          | 9.57e+03 |
| total/epochs            | 1        |
| total/steps             | 1139998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 1.22     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1141000
Best mean reward: 94.30 - Last mean reward per episode: 91.99
Num timesteps: 1142000
Best mean reward: 94.30 - Last mean reward per episode: 91.97
Num timesteps: 1143000
Best mean reward: 94.30 - Last mean reward per episode: 91.81
Num timesteps: 1144000
Best mean reward: 94.30 - Last mean reward per episode: 90.66
Num timesteps: 1145000
Best mean reward: 94.30 - Last mean reward per episode: 89.61
Num timesteps: 1146000
Best mean reward: 94.30 - Last mean reward per episode: 89.60
Num timesteps: 1147000
Best mean reward: 94.30 - Last mean reward per episode: 89.61
Num timesteps: 1148000
Best mean reward: 94.30 - Last mean reward per episode: 89.66
Num timesteps: 1149000
Best mean reward: 94.30 - Last mean reward per episode: 91.32
Num timesteps: 1150000
Best mean reward: 94.30 - Last mean reward per episode: 91.14
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 6.57     |
| reference_action_mean   | -0.615   |
| reference_action_std    | 0.758    |
| reference_actor_Q_mean  | 49.9     |
| reference_actor_Q_std   | 6.48     |
| rollout/Q_mean          | 57.4     |
| rollout/actions_mean    | 0.166    |
| rollout/actions_std     | 0.793    |
| rollout/episode_steps   | 119      |
| rollout/episodes        | 9.66e+03 |
| rollout/return          | 90.2     |
| rollout/return_history  | 91.1     |
| total/duration          | 2.9e+03  |
| total/episodes          | 9.66e+03 |
| total/epochs            | 1        |
| total/steps             | 1149998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 1.63     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1151000
Best mean reward: 94.30 - Last mean reward per episode: 91.13
Num timesteps: 1152000
Best mean reward: 94.30 - Last mean reward per episode: 91.24
Num timesteps: 1153000
Best mean reward: 94.30 - Last mean reward per episode: 92.49
Num timesteps: 1154000
Best mean reward: 94.30 - Last mean reward per episode: 92.99
Num timesteps: 1155000
Best mean reward: 94.30 - Last mean reward per episode: 92.57
Num timesteps: 1156000
Best mean reward: 94.30 - Last mean reward per episode: 92.56
Num timesteps: 1157000
Best mean reward: 94.30 - Last mean reward per episode: 92.62
Num timesteps: 1158000
Best mean reward: 94.30 - Last mean reward per episode: 92.64
Num timesteps: 1159000
Best mean reward: 94.30 - Last mean reward per episode: 92.24
Num timesteps: 1160000
Best mean reward: 94.30 - Last mean reward per episode: 92.43
--------------------------------------
| reference_Q_mean        | 49.1     |
| reference_Q_std         | 6.35     |
| reference_action_mean   | -0.522   |
| reference_action_std    | 0.816    |
| reference_actor_Q_mean  | 49.6     |
| reference_actor_Q_std   | 6.4      |
| rollout/Q_mean          | 57.5     |
| rollout/actions_mean    | 0.164    |
| rollout/actions_std     | 0.794    |
| rollout/episode_steps   | 119      |
| rollout/episodes        | 9.76e+03 |
| rollout/return          | 90.2     |
| rollout/return_history  | 92.4     |
| total/duration          | 2.92e+03 |
| total/episodes          | 9.76e+03 |
| total/epochs            | 1        |
| total/steps             | 1159998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 1.45     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1161000
Best mean reward: 94.30 - Last mean reward per episode: 92.13
Num timesteps: 1162000
Best mean reward: 94.30 - Last mean reward per episode: 92.14
Num timesteps: 1163000
Best mean reward: 94.30 - Last mean reward per episode: 92.14
Num timesteps: 1164000
Best mean reward: 94.30 - Last mean reward per episode: 91.87
Num timesteps: 1165000
Best mean reward: 94.30 - Last mean reward per episode: 92.52
Num timesteps: 1166000
Best mean reward: 94.30 - Last mean reward per episode: 92.88
Num timesteps: 1167000
Best mean reward: 94.30 - Last mean reward per episode: 92.62
Num timesteps: 1168000
Best mean reward: 94.30 - Last mean reward per episode: 92.62
Num timesteps: 1169000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 1170000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
--------------------------------------
| reference_Q_mean        | 49.6     |
| reference_Q_std         | 6.62     |
| reference_action_mean   | -0.236   |
| reference_action_std    | 0.933    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 6.72     |
| rollout/Q_mean          | 57.6     |
| rollout/actions_mean    | 0.163    |
| rollout/actions_std     | 0.794    |
| rollout/episode_steps   | 119      |
| rollout/episodes        | 9.86e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 93.2     |
| total/duration          | 2.95e+03 |
| total/episodes          | 9.86e+03 |
| total/epochs            | 1        |
| total/steps             | 1169998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -69      |
| train/loss_critic       | 1.59     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1171000
Best mean reward: 94.30 - Last mean reward per episode: 93.21
Num timesteps: 1172000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 1173000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 1174000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 1175000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 1176000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 1177000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 1178000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1179000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 1180000
Best mean reward: 94.30 - Last mean reward per episode: 93.55
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 7.06     |
| reference_action_mean   | -0.65    |
| reference_action_std    | 0.697    |
| reference_actor_Q_mean  | 50.4     |
| reference_actor_Q_std   | 7.06     |
| rollout/Q_mean          | 57.7     |
| rollout/actions_mean    | 0.163    |
| rollout/actions_std     | 0.795    |
| rollout/episode_steps   | 118      |
| rollout/episodes        | 9.97e+03 |
| rollout/return          | 90.3     |
| rollout/return_history  | 93.5     |
| total/duration          | 2.97e+03 |
| total/episodes          | 9.97e+03 |
| total/epochs            | 1        |
| total/steps             | 1179998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 1.69     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1181000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 1182000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 1183000
Best mean reward: 94.30 - Last mean reward per episode: 93.31
Num timesteps: 1184000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 1185000
Best mean reward: 94.30 - Last mean reward per episode: 93.40
Num timesteps: 1186000
Best mean reward: 94.30 - Last mean reward per episode: 93.43
Num timesteps: 1187000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 1188000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1189000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 1190000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
--------------------------------------
| reference_Q_mean        | 48.9     |
| reference_Q_std         | 7.89     |
| reference_action_mean   | -0.602   |
| reference_action_std    | 0.767    |
| reference_actor_Q_mean  | 49.3     |
| reference_actor_Q_std   | 7.94     |
| rollout/Q_mean          | 57.9     |
| rollout/actions_mean    | 0.162    |
| rollout/actions_std     | 0.796    |
| rollout/episode_steps   | 118      |
| rollout/episodes        | 1.01e+04 |
| rollout/return          | 90.3     |
| rollout/return_history  | 93.5     |
| total/duration          | 3e+03    |
| total/episodes          | 1.01e+04 |
| total/epochs            | 1        |
| total/steps             | 1189998  |
| total/steps_per_second  | 397      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 1.52     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1191000
Best mean reward: 94.30 - Last mean reward per episode: 93.95
Num timesteps: 1192000
Best mean reward: 94.30 - Last mean reward per episode: 93.96
Num timesteps: 1193000
Best mean reward: 94.30 - Last mean reward per episode: 94.03
Num timesteps: 1194000
Best mean reward: 94.30 - Last mean reward per episode: 94.01
Num timesteps: 1195000
Best mean reward: 94.30 - Last mean reward per episode: 93.98
Num timesteps: 1196000
Best mean reward: 94.30 - Last mean reward per episode: 93.94
Num timesteps: 1197000
Best mean reward: 94.30 - Last mean reward per episode: 93.84
Num timesteps: 1198000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 1199000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 1200000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
--------------------------------------
| reference_Q_mean        | 48.4     |
| reference_Q_std         | 8.2      |
| reference_action_mean   | -0.408   |
| reference_action_std    | 0.882    |
| reference_actor_Q_mean  | 49.3     |
| reference_actor_Q_std   | 8.47     |
| rollout/Q_mean          | 58       |
| rollout/actions_mean    | 0.162    |
| rollout/actions_std     | 0.796    |
| rollout/episode_steps   | 118      |
| rollout/episodes        | 1.02e+04 |
| rollout/return          | 90.4     |
| rollout/return_history  | 93.8     |
| total/duration          | 3.03e+03 |
| total/episodes          | 1.02e+04 |
| total/epochs            | 1        |
| total/steps             | 1199998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 1.13     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1201000
Best mean reward: 94.30 - Last mean reward per episode: 93.88
Num timesteps: 1202000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 1203000
Best mean reward: 94.30 - Last mean reward per episode: 93.79
Num timesteps: 1204000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 1205000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 1206000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
Num timesteps: 1207000
Best mean reward: 94.30 - Last mean reward per episode: 93.76
Num timesteps: 1208000
Best mean reward: 94.30 - Last mean reward per episode: 93.71
Num timesteps: 1209000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 1210000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 8.43     |
| reference_action_mean   | -0.273   |
| reference_action_std    | 0.948    |
| reference_actor_Q_mean  | 50.2     |
| reference_actor_Q_std   | 8.87     |
| rollout/Q_mean          | 58.1     |
| rollout/actions_mean    | 0.163    |
| rollout/actions_std     | 0.796    |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 1.03e+04 |
| rollout/return          | 90.4     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.05e+03 |
| total/episodes          | 1.03e+04 |
| total/epochs            | 1        |
| total/steps             | 1209998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -71.3    |
| train/loss_critic       | 0.797    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1211000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 1212000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
Num timesteps: 1213000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
Num timesteps: 1214000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 1215000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1216000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 1217000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 1218000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 1219000
Best mean reward: 94.30 - Last mean reward per episode: 91.28
Num timesteps: 1220000
Best mean reward: 94.30 - Last mean reward per episode: 89.81
--------------------------------------
| reference_Q_mean        | 49.1     |
| reference_Q_std         | 8.78     |
| reference_action_mean   | -0.789   |
| reference_action_std    | 0.574    |
| reference_actor_Q_mean  | 49.8     |
| reference_actor_Q_std   | 8.91     |
| rollout/Q_mean          | 58.1     |
| rollout/actions_mean    | 0.161    |
| rollout/actions_std     | 0.797    |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 1.04e+04 |
| rollout/return          | 90.4     |
| rollout/return_history  | 89.8     |
| total/duration          | 3.08e+03 |
| total/episodes          | 1.04e+04 |
| total/epochs            | 1        |
| total/steps             | 1219998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.992    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1221000
Best mean reward: 94.30 - Last mean reward per episode: 87.99
Num timesteps: 1222000
Best mean reward: 94.30 - Last mean reward per episode: 86.27
Num timesteps: 1223000
Best mean reward: 94.30 - Last mean reward per episode: 85.76
Num timesteps: 1224000
Best mean reward: 94.30 - Last mean reward per episode: 83.84
Num timesteps: 1225000
Best mean reward: 94.30 - Last mean reward per episode: 83.81
Num timesteps: 1226000
Best mean reward: 94.30 - Last mean reward per episode: 83.89
Num timesteps: 1227000
Best mean reward: 94.30 - Last mean reward per episode: 83.94
Num timesteps: 1228000
Best mean reward: 94.30 - Last mean reward per episode: 84.08
Num timesteps: 1229000
Best mean reward: 94.30 - Last mean reward per episode: 84.14
Num timesteps: 1230000
Best mean reward: 94.30 - Last mean reward per episode: 84.10
--------------------------------------
| reference_Q_mean        | 49       |
| reference_Q_std         | 8.69     |
| reference_action_mean   | -0.766   |
| reference_action_std    | 0.592    |
| reference_actor_Q_mean  | 49.6     |
| reference_actor_Q_std   | 8.9      |
| rollout/Q_mean          | 58.2     |
| rollout/actions_mean    | 0.159    |
| rollout/actions_std     | 0.798    |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 1.05e+04 |
| rollout/return          | 90.4     |
| rollout/return_history  | 84.1     |
| total/duration          | 3.1e+03  |
| total/episodes          | 1.05e+04 |
| total/epochs            | 1        |
| total/steps             | 1229998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -68.2    |
| train/loss_critic       | 0.66     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1231000
Best mean reward: 94.30 - Last mean reward per episode: 84.01
Num timesteps: 1232000
Best mean reward: 94.30 - Last mean reward per episode: 91.82
Num timesteps: 1233000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
Num timesteps: 1234000
Best mean reward: 94.30 - Last mean reward per episode: 93.90
Num timesteps: 1235000
Best mean reward: 94.30 - Last mean reward per episode: 94.00
Num timesteps: 1236000
Best mean reward: 94.30 - Last mean reward per episode: 93.90
Num timesteps: 1237000
Best mean reward: 94.30 - Last mean reward per episode: 93.83
Num timesteps: 1238000
Best mean reward: 94.30 - Last mean reward per episode: 93.91
Num timesteps: 1239000
Best mean reward: 94.30 - Last mean reward per episode: 94.02
Num timesteps: 1240000
Best mean reward: 94.30 - Last mean reward per episode: 94.04
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 8.71     |
| reference_action_mean   | -0.489   |
| reference_action_std    | 0.846    |
| reference_actor_Q_mean  | 49.9     |
| reference_actor_Q_std   | 8.89     |
| rollout/Q_mean          | 58.3     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.799    |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 1.06e+04 |
| rollout/return          | 90.4     |
| rollout/return_history  | 94       |
| total/duration          | 3.13e+03 |
| total/episodes          | 1.06e+04 |
| total/epochs            | 1        |
| total/steps             | 1239998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -68      |
| train/loss_critic       | 0.85     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1241000
Best mean reward: 94.30 - Last mean reward per episode: 92.36
Num timesteps: 1242000
Best mean reward: 94.30 - Last mean reward per episode: 92.32
Num timesteps: 1243000
Best mean reward: 94.30 - Last mean reward per episode: 92.28
Num timesteps: 1244000
Best mean reward: 94.30 - Last mean reward per episode: 92.23
Num timesteps: 1245000
Best mean reward: 94.30 - Last mean reward per episode: 92.10
Num timesteps: 1246000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 1247000
Best mean reward: 94.30 - Last mean reward per episode: 91.69
Num timesteps: 1248000
Best mean reward: 94.30 - Last mean reward per episode: 91.57
Num timesteps: 1249000
Best mean reward: 94.30 - Last mean reward per episode: 91.55
Num timesteps: 1250000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
--------------------------------------
| reference_Q_mean        | 49.3     |
| reference_Q_std         | 8.48     |
| reference_action_mean   | -0.323   |
| reference_action_std    | 0.912    |
| reference_actor_Q_mean  | 50.1     |
| reference_actor_Q_std   | 8.47     |
| rollout/Q_mean          | 58.4     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.799    |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 1.07e+04 |
| rollout/return          | 90.4     |
| rollout/return_history  | 93.2     |
| total/duration          | 3.15e+03 |
| total/episodes          | 1.07e+04 |
| total/epochs            | 1        |
| total/steps             | 1249998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -68.2    |
| train/loss_critic       | 1.13     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1251000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 1252000
Best mean reward: 94.30 - Last mean reward per episode: 91.48
Num timesteps: 1253000
Best mean reward: 94.30 - Last mean reward per episode: 91.35
Num timesteps: 1254000
Best mean reward: 94.30 - Last mean reward per episode: 91.26
Num timesteps: 1255000
Best mean reward: 94.30 - Last mean reward per episode: 91.24
Num timesteps: 1256000
Best mean reward: 94.30 - Last mean reward per episode: 91.92
Num timesteps: 1257000
Best mean reward: 94.30 - Last mean reward per episode: 91.66
Num timesteps: 1258000
Best mean reward: 94.30 - Last mean reward per episode: 91.53
Num timesteps: 1259000
Best mean reward: 94.30 - Last mean reward per episode: 91.55
Num timesteps: 1260000
Best mean reward: 94.30 - Last mean reward per episode: 91.46
--------------------------------------
| reference_Q_mean        | 48.8     |
| reference_Q_std         | 8.4      |
| reference_action_mean   | -0.246   |
| reference_action_std    | 0.955    |
| reference_actor_Q_mean  | 49.9     |
| reference_actor_Q_std   | 8.26     |
| rollout/Q_mean          | 58.5     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.8      |
| rollout/episode_steps   | 117      |
| rollout/episodes        | 1.08e+04 |
| rollout/return          | 90.4     |
| rollout/return_history  | 91.5     |
| total/duration          | 3.18e+03 |
| total/episodes          | 1.08e+04 |
| total/epochs            | 1        |
| total/steps             | 1259998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -68.4    |
| train/loss_critic       | 1.75     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1261000
Best mean reward: 94.30 - Last mean reward per episode: 91.38
Num timesteps: 1262000
Best mean reward: 94.30 - Last mean reward per episode: 93.22
Num timesteps: 1263000
Best mean reward: 94.30 - Last mean reward per episode: 93.34
Num timesteps: 1264000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 1265000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 1266000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 1267000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 1268000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
Num timesteps: 1269000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 1270000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
--------------------------------------
| reference_Q_mean        | 49.5     |
| reference_Q_std         | 8.5      |
| reference_action_mean   | -0.255   |
| reference_action_std    | 0.957    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 8.5      |
| rollout/Q_mean          | 58.6     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.801    |
| rollout/episode_steps   | 116      |
| rollout/episodes        | 1.09e+04 |
| rollout/return          | 90.5     |
| rollout/return_history  | 93.3     |
| total/duration          | 3.21e+03 |
| total/episodes          | 1.09e+04 |
| total/epochs            | 1        |
| total/steps             | 1269998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 1.21     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1271000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 1272000
Best mean reward: 94.30 - Last mean reward per episode: 91.75
Num timesteps: 1273000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 1274000
Best mean reward: 94.30 - Last mean reward per episode: 92.14
Num timesteps: 1275000
Best mean reward: 94.30 - Last mean reward per episode: 92.24
Num timesteps: 1276000
Best mean reward: 94.30 - Last mean reward per episode: 92.10
Num timesteps: 1277000
Best mean reward: 94.30 - Last mean reward per episode: 91.92
Num timesteps: 1278000
Best mean reward: 94.30 - Last mean reward per episode: 91.71
Num timesteps: 1279000
Best mean reward: 94.30 - Last mean reward per episode: 91.75
Num timesteps: 1280000
Best mean reward: 94.30 - Last mean reward per episode: 90.30
--------------------------------------
| reference_Q_mean        | 49.1     |
| reference_Q_std         | 9.15     |
| reference_action_mean   | -0.209   |
| reference_action_std    | 0.969    |
| reference_actor_Q_mean  | 50.3     |
| reference_actor_Q_std   | 8.87     |
| rollout/Q_mean          | 58.7     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.801    |
| rollout/episode_steps   | 116      |
| rollout/episodes        | 1.1e+04  |
| rollout/return          | 90.5     |
| rollout/return_history  | 90.3     |
| total/duration          | 3.23e+03 |
| total/episodes          | 1.1e+04  |
| total/epochs            | 1        |
| total/steps             | 1279998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -72.1    |
| train/loss_critic       | 1.77     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1281000
Best mean reward: 94.30 - Last mean reward per episode: 90.16
Num timesteps: 1282000
Best mean reward: 94.30 - Last mean reward per episode: 90.12
Num timesteps: 1283000
Best mean reward: 94.30 - Last mean reward per episode: 91.63
Num timesteps: 1284000
Best mean reward: 94.30 - Last mean reward per episode: 91.59
Num timesteps: 1285000
Best mean reward: 94.30 - Last mean reward per episode: 91.62
Num timesteps: 1286000
Best mean reward: 94.30 - Last mean reward per episode: 91.68
Num timesteps: 1287000
Best mean reward: 94.30 - Last mean reward per episode: 91.97
Num timesteps: 1288000
Best mean reward: 94.30 - Last mean reward per episode: 91.94
Num timesteps: 1289000
Best mean reward: 94.30 - Last mean reward per episode: 93.34
Num timesteps: 1290000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
--------------------------------------
| reference_Q_mean        | 48.7     |
| reference_Q_std         | 9.28     |
| reference_action_mean   | -0.308   |
| reference_action_std    | 0.941    |
| reference_actor_Q_mean  | 50       |
| reference_actor_Q_std   | 9.19     |
| rollout/Q_mean          | 58.8     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.802    |
| rollout/episode_steps   | 116      |
| rollout/episodes        | 1.11e+04 |
| rollout/return          | 90.5     |
| rollout/return_history  | 93.5     |
| total/duration          | 3.26e+03 |
| total/episodes          | 1.11e+04 |
| total/epochs            | 1        |
| total/steps             | 1289998  |
| total/steps_per_second  | 396      |
| train/loss_actor        | -72.2    |
| train/loss_critic       | 0.709    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1291000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 1292000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 1293000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 1294000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 1295000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
Num timesteps: 1296000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 1297000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
Num timesteps: 1298000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 1299000
Best mean reward: 94.30 - Last mean reward per episode: 93.22
Num timesteps: 1300000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
--------------------------------------
| reference_Q_mean        | 50.5     |
| reference_Q_std         | 8.48     |
| reference_action_mean   | -0.417   |
| reference_action_std    | 0.875    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 8.31     |
| rollout/Q_mean          | 58.9     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.802    |
| rollout/episode_steps   | 116      |
| rollout/episodes        | 1.12e+04 |
| rollout/return          | 90.5     |
| rollout/return_history  | 93.2     |
| total/duration          | 3.29e+03 |
| total/episodes          | 1.12e+04 |
| total/epochs            | 1        |
| total/steps             | 1299998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -71.9    |
| train/loss_critic       | 0.717    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1301000
Best mean reward: 94.30 - Last mean reward per episode: 92.95
Num timesteps: 1302000
Best mean reward: 94.30 - Last mean reward per episode: 92.95
Num timesteps: 1303000
Best mean reward: 94.30 - Last mean reward per episode: 92.87
Num timesteps: 1304000
Best mean reward: 94.30 - Last mean reward per episode: 92.98
Num timesteps: 1305000
Best mean reward: 94.30 - Last mean reward per episode: 93.17
Num timesteps: 1306000
Best mean reward: 94.30 - Last mean reward per episode: 93.17
Num timesteps: 1307000
Best mean reward: 94.30 - Last mean reward per episode: 93.03
Num timesteps: 1308000
Best mean reward: 94.30 - Last mean reward per episode: 93.34
Num timesteps: 1309000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 1310000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
--------------------------------------
| reference_Q_mean        | 51.2     |
| reference_Q_std         | 7.92     |
| reference_action_mean   | -0.493   |
| reference_action_std    | 0.848    |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 7.85     |
| rollout/Q_mean          | 59       |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.803    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 1.14e+04 |
| rollout/return          | 90.5     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.31e+03 |
| total/episodes          | 1.14e+04 |
| total/epochs            | 1        |
| total/steps             | 1309998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -72.4    |
| train/loss_critic       | 1.84     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1311000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 1312000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 1313000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 1314000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 1315000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 1316000
Best mean reward: 94.30 - Last mean reward per episode: 93.77
Num timesteps: 1317000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 1318000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 1319000
Best mean reward: 94.30 - Last mean reward per episode: 93.76
Num timesteps: 1320000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 7.87     |
| reference_action_mean   | -0.571   |
| reference_action_std    | 0.796    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.83     |
| rollout/Q_mean          | 59.1     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.803    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 1.15e+04 |
| rollout/return          | 90.6     |
| rollout/return_history  | 93.7     |
| total/duration          | 3.34e+03 |
| total/episodes          | 1.15e+04 |
| total/epochs            | 1        |
| total/steps             | 1319998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -72.4    |
| train/loss_critic       | 1.77     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1321000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
Num timesteps: 1322000
Best mean reward: 94.30 - Last mean reward per episode: 93.77
Num timesteps: 1323000
Best mean reward: 94.30 - Last mean reward per episode: 93.95
Num timesteps: 1324000
Best mean reward: 94.30 - Last mean reward per episode: 93.87
Num timesteps: 1325000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
Num timesteps: 1326000
Best mean reward: 94.30 - Last mean reward per episode: 93.99
Num timesteps: 1327000
Best mean reward: 94.30 - Last mean reward per episode: 93.96
Num timesteps: 1328000
Best mean reward: 94.30 - Last mean reward per episode: 93.97
Num timesteps: 1329000
Best mean reward: 94.30 - Last mean reward per episode: 94.07
Num timesteps: 1330000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
--------------------------------------
| reference_Q_mean        | 53.3     |
| reference_Q_std         | 7.94     |
| reference_action_mean   | -0.335   |
| reference_action_std    | 0.913    |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 7.91     |
| rollout/Q_mean          | 59.2     |
| rollout/actions_mean    | 0.148    |
| rollout/actions_std     | 0.804    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 1.16e+04 |
| rollout/return          | 90.6     |
| rollout/return_history  | 93.5     |
| total/duration          | 3.37e+03 |
| total/episodes          | 1.16e+04 |
| total/epochs            | 1        |
| total/steps             | 1329998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -72.4    |
| train/loss_critic       | 0.84     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1331000
Best mean reward: 94.30 - Last mean reward per episode: 93.40
Num timesteps: 1332000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 1333000
Best mean reward: 94.30 - Last mean reward per episode: 91.73
Num timesteps: 1334000
Best mean reward: 94.30 - Last mean reward per episode: 91.57
Num timesteps: 1335000
Best mean reward: 94.30 - Last mean reward per episode: 91.50
Num timesteps: 1336000
Best mean reward: 94.30 - Last mean reward per episode: 91.51
Num timesteps: 1337000
Best mean reward: 94.30 - Last mean reward per episode: 91.02
Num timesteps: 1338000
Best mean reward: 94.30 - Last mean reward per episode: 90.99
Num timesteps: 1339000
Best mean reward: 94.30 - Last mean reward per episode: 90.95
Num timesteps: 1340000
Best mean reward: 94.30 - Last mean reward per episode: 91.46
--------------------------------------
| reference_Q_mean        | 54.8     |
| reference_Q_std         | 6.82     |
| reference_action_mean   | 0.104    |
| reference_action_std    | 0.979    |
| reference_actor_Q_mean  | 55.3     |
| reference_actor_Q_std   | 6.81     |
| rollout/Q_mean          | 59.3     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.805    |
| rollout/episode_steps   | 115      |
| rollout/episodes        | 1.17e+04 |
| rollout/return          | 90.6     |
| rollout/return_history  | 91.5     |
| total/duration          | 3.39e+03 |
| total/episodes          | 1.17e+04 |
| total/epochs            | 1        |
| total/steps             | 1339998  |
| total/steps_per_second  | 395      |
| train/loss_actor        | -72.3    |
| train/loss_critic       | 0.617    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1341000
Best mean reward: 94.30 - Last mean reward per episode: 91.52
Num timesteps: 1342000
Best mean reward: 94.30 - Last mean reward per episode: 93.22
Num timesteps: 1343000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 1344000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 1345000
Best mean reward: 94.30 - Last mean reward per episode: 93.14
Num timesteps: 1346000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 1347000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
Num timesteps: 1348000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 1349000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 1350000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
--------------------------------------
| reference_Q_mean        | 55.4     |
| reference_Q_std         | 5.45     |
| reference_action_mean   | -0.419   |
| reference_action_std    | 0.886    |
| reference_actor_Q_mean  | 55.8     |
| reference_actor_Q_std   | 5.44     |
| rollout/Q_mean          | 59.4     |
| rollout/actions_mean    | 0.146    |
| rollout/actions_std     | 0.805    |
| rollout/episode_steps   | 114      |
| rollout/episodes        | 1.18e+04 |
| rollout/return          | 90.6     |
| rollout/return_history  | 93.5     |
| total/duration          | 3.42e+03 |
| total/episodes          | 1.18e+04 |
| total/epochs            | 1        |
| total/steps             | 1349998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -72.2    |
| train/loss_critic       | 1.62     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1351000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 1352000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 1353000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 1354000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 1355000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
Num timesteps: 1356000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1357000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1358000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1359000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 1360000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
--------------------------------------
| reference_Q_mean        | 55.9     |
| reference_Q_std         | 4.79     |
| reference_action_mean   | -0.605   |
| reference_action_std    | 0.786    |
| reference_actor_Q_mean  | 56.2     |
| reference_actor_Q_std   | 5.05     |
| rollout/Q_mean          | 59.5     |
| rollout/actions_mean    | 0.146    |
| rollout/actions_std     | 0.806    |
| rollout/episode_steps   | 114      |
| rollout/episodes        | 1.19e+04 |
| rollout/return          | 90.7     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.45e+03 |
| total/episodes          | 1.19e+04 |
| total/epochs            | 1        |
| total/steps             | 1359998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -72.2    |
| train/loss_critic       | 1.55     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1361000
Best mean reward: 94.30 - Last mean reward per episode: 93.55
Num timesteps: 1362000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 1363000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 1364000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 1365000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 1366000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 1367000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 1368000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 1369000
Best mean reward: 94.30 - Last mean reward per episode: 93.79
Num timesteps: 1370000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
--------------------------------------
| reference_Q_mean        | 55.5     |
| reference_Q_std         | 4.88     |
| reference_action_mean   | -0.432   |
| reference_action_std    | 0.877    |
| reference_actor_Q_mean  | 55.9     |
| reference_actor_Q_std   | 5.15     |
| rollout/Q_mean          | 59.6     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.806    |
| rollout/episode_steps   | 114      |
| rollout/episodes        | 1.21e+04 |
| rollout/return          | 90.7     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.47e+03 |
| total/episodes          | 1.21e+04 |
| total/epochs            | 1        |
| total/steps             | 1369998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -72.3    |
| train/loss_critic       | 0.717    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1371000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 1372000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 1373000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 1374000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1375000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
Num timesteps: 1376000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 1377000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 1378000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 1379000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 1380000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
--------------------------------------
| reference_Q_mean        | 55.8     |
| reference_Q_std         | 4.94     |
| reference_action_mean   | -0.257   |
| reference_action_std    | 0.948    |
| reference_actor_Q_mean  | 56.6     |
| reference_actor_Q_std   | 5.47     |
| rollout/Q_mean          | 59.7     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.807    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.22e+04 |
| rollout/return          | 90.7     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.5e+03  |
| total/episodes          | 1.22e+04 |
| total/epochs            | 1        |
| total/steps             | 1379998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -72.1    |
| train/loss_critic       | 0.507    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1381000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
Num timesteps: 1382000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 1383000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 1384000
Best mean reward: 94.30 - Last mean reward per episode: 93.29
Num timesteps: 1385000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 1386000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 1387000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 1388000
Best mean reward: 94.30 - Last mean reward per episode: 92.86
Num timesteps: 1389000
Best mean reward: 94.30 - Last mean reward per episode: 92.73
Num timesteps: 1390000
Best mean reward: 94.30 - Last mean reward per episode: 92.91
--------------------------------------
| reference_Q_mean        | 56.8     |
| reference_Q_std         | 5.34     |
| reference_action_mean   | 0.212    |
| reference_action_std    | 0.96     |
| reference_actor_Q_mean  | 57.5     |
| reference_actor_Q_std   | 5.45     |
| rollout/Q_mean          | 59.8     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.807    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.23e+04 |
| rollout/return          | 90.8     |
| rollout/return_history  | 92.9     |
| total/duration          | 3.53e+03 |
| total/episodes          | 1.23e+04 |
| total/epochs            | 1        |
| total/steps             | 1389998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -71.9    |
| train/loss_critic       | 0.393    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1391000
Best mean reward: 94.30 - Last mean reward per episode: 92.82
Num timesteps: 1392000
Best mean reward: 94.30 - Last mean reward per episode: 92.71
Num timesteps: 1393000
Best mean reward: 94.30 - Last mean reward per episode: 92.76
Num timesteps: 1394000
Best mean reward: 94.30 - Last mean reward per episode: 92.76
Num timesteps: 1395000
Best mean reward: 94.30 - Last mean reward per episode: 92.89
Num timesteps: 1396000
Best mean reward: 94.30 - Last mean reward per episode: 92.90
Num timesteps: 1397000
Best mean reward: 94.30 - Last mean reward per episode: 92.99
Num timesteps: 1398000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 1399000
Best mean reward: 94.30 - Last mean reward per episode: 93.01
Num timesteps: 1400000
Best mean reward: 94.30 - Last mean reward per episode: 93.05
--------------------------------------
| reference_Q_mean        | 55.9     |
| reference_Q_std         | 5.86     |
| reference_action_mean   | 0.216    |
| reference_action_std    | 0.973    |
| reference_actor_Q_mean  | 56.3     |
| reference_actor_Q_std   | 5.86     |
| rollout/Q_mean          | 59.9     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.808    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.24e+04 |
| rollout/return          | 90.8     |
| rollout/return_history  | 93.1     |
| total/duration          | 3.55e+03 |
| total/episodes          | 1.24e+04 |
| total/epochs            | 1        |
| total/steps             | 1399998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -71.2    |
| train/loss_critic       | 0.48     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1401000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 1402000
Best mean reward: 94.30 - Last mean reward per episode: 93.14
Num timesteps: 1403000
Best mean reward: 94.30 - Last mean reward per episode: 93.09
Num timesteps: 1404000
Best mean reward: 94.30 - Last mean reward per episode: 92.97
Num timesteps: 1405000
Best mean reward: 94.30 - Last mean reward per episode: 92.88
Num timesteps: 1406000
Best mean reward: 94.30 - Last mean reward per episode: 92.75
Num timesteps: 1407000
Best mean reward: 94.30 - Last mean reward per episode: 92.73
Num timesteps: 1408000
Best mean reward: 94.30 - Last mean reward per episode: 92.69
Num timesteps: 1409000
Best mean reward: 94.30 - Last mean reward per episode: 92.52
Num timesteps: 1410000
Best mean reward: 94.30 - Last mean reward per episode: 92.32
--------------------------------------
| reference_Q_mean        | 55       |
| reference_Q_std         | 6.29     |
| reference_action_mean   | 0.268    |
| reference_action_std    | 0.948    |
| reference_actor_Q_mean  | 55.2     |
| reference_actor_Q_std   | 6.29     |
| rollout/Q_mean          | 59.9     |
| rollout/actions_mean    | 0.146    |
| rollout/actions_std     | 0.808    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.25e+04 |
| rollout/return          | 90.8     |
| rollout/return_history  | 92.3     |
| total/duration          | 3.58e+03 |
| total/episodes          | 1.25e+04 |
| total/epochs            | 1        |
| total/steps             | 1409998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.319    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1411000
Best mean reward: 94.30 - Last mean reward per episode: 92.27
Num timesteps: 1412000
Best mean reward: 94.30 - Last mean reward per episode: 92.27
Num timesteps: 1413000
Best mean reward: 94.30 - Last mean reward per episode: 92.25
Num timesteps: 1414000
Best mean reward: 94.30 - Last mean reward per episode: 91.93
Num timesteps: 1415000
Best mean reward: 94.30 - Last mean reward per episode: 90.39
Num timesteps: 1416000
Best mean reward: 94.30 - Last mean reward per episode: 90.49
Num timesteps: 1417000
Best mean reward: 94.30 - Last mean reward per episode: 90.61
Num timesteps: 1418000
Best mean reward: 94.30 - Last mean reward per episode: 90.43
Num timesteps: 1419000
Best mean reward: 94.30 - Last mean reward per episode: 90.53
Num timesteps: 1420000
Best mean reward: 94.30 - Last mean reward per episode: 90.82
--------------------------------------
| reference_Q_mean        | 53.8     |
| reference_Q_std         | 6.71     |
| reference_action_mean   | -0.185   |
| reference_action_std    | 0.961    |
| reference_actor_Q_mean  | 54.3     |
| reference_actor_Q_std   | 6.53     |
| rollout/Q_mean          | 60       |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.26e+04 |
| rollout/return          | 90.8     |
| rollout/return_history  | 90.8     |
| total/duration          | 3.61e+03 |
| total/episodes          | 1.26e+04 |
| total/epochs            | 1        |
| total/steps             | 1419998  |
| total/steps_per_second  | 394      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 0.357    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1421000
Best mean reward: 94.30 - Last mean reward per episode: 90.98
Num timesteps: 1422000
Best mean reward: 94.30 - Last mean reward per episode: 91.11
Num timesteps: 1423000
Best mean reward: 94.30 - Last mean reward per episode: 91.06
Num timesteps: 1424000
Best mean reward: 94.30 - Last mean reward per episode: 91.37
Num timesteps: 1425000
Best mean reward: 94.30 - Last mean reward per episode: 92.92
Num timesteps: 1426000
Best mean reward: 94.30 - Last mean reward per episode: 92.84
Num timesteps: 1427000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 1428000
Best mean reward: 94.30 - Last mean reward per episode: 92.69
Num timesteps: 1429000
Best mean reward: 94.30 - Last mean reward per episode: 92.40
Num timesteps: 1430000
Best mean reward: 94.30 - Last mean reward per episode: 92.35
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 6.2      |
| reference_action_mean   | -0.321   |
| reference_action_std    | 0.94     |
| reference_actor_Q_mean  | 53.6     |
| reference_actor_Q_std   | 6.26     |
| rollout/Q_mean          | 60       |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.27e+04 |
| rollout/return          | 90.8     |
| rollout/return_history  | 92.4     |
| total/duration          | 3.64e+03 |
| total/episodes          | 1.27e+04 |
| total/epochs            | 1        |
| total/steps             | 1429998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -68.5    |
| train/loss_critic       | 0.378    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1431000
Best mean reward: 94.30 - Last mean reward per episode: 92.34
Num timesteps: 1432000
Best mean reward: 94.30 - Last mean reward per episode: 92.41
Num timesteps: 1433000
Best mean reward: 94.30 - Last mean reward per episode: 92.42
Num timesteps: 1434000
Best mean reward: 94.30 - Last mean reward per episode: 92.34
Num timesteps: 1435000
Best mean reward: 94.30 - Last mean reward per episode: 92.41
Num timesteps: 1436000
Best mean reward: 94.30 - Last mean reward per episode: 92.66
Num timesteps: 1437000
Best mean reward: 94.30 - Last mean reward per episode: 92.71
Num timesteps: 1438000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 1439000
Best mean reward: 94.30 - Last mean reward per episode: 93.34
Num timesteps: 1440000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 5.46     |
| reference_action_mean   | -0.259   |
| reference_action_std    | 0.955    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 5.59     |
| rollout/Q_mean          | 60.1     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.28e+04 |
| rollout/return          | 90.8     |
| rollout/return_history  | 93.4     |
| total/duration          | 3.66e+03 |
| total/episodes          | 1.28e+04 |
| total/epochs            | 1        |
| total/steps             | 1439998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -68.7    |
| train/loss_critic       | 0.389    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1441000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 1442000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 1443000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 1444000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 1445000
Best mean reward: 94.30 - Last mean reward per episode: 93.40
Num timesteps: 1446000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
Num timesteps: 1447000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
Num timesteps: 1448000
Best mean reward: 94.30 - Last mean reward per episode: 93.11
Num timesteps: 1449000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
Num timesteps: 1450000
Best mean reward: 94.30 - Last mean reward per episode: 92.88
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 5.63     |
| reference_action_mean   | -0.237   |
| reference_action_std    | 0.968    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 5.52     |
| rollout/Q_mean          | 60.2     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.29e+04 |
| rollout/return          | 90.8     |
| rollout/return_history  | 92.9     |
| total/duration          | 3.69e+03 |
| total/episodes          | 1.29e+04 |
| total/epochs            | 1        |
| total/steps             | 1449998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -68.6    |
| train/loss_critic       | 0.384    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1451000
Best mean reward: 94.30 - Last mean reward per episode: 92.86
Num timesteps: 1452000
Best mean reward: 94.30 - Last mean reward per episode: 92.77
Num timesteps: 1453000
Best mean reward: 94.30 - Last mean reward per episode: 92.73
Num timesteps: 1454000
Best mean reward: 94.30 - Last mean reward per episode: 92.77
Num timesteps: 1455000
Best mean reward: 94.30 - Last mean reward per episode: 92.84
Num timesteps: 1456000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 1457000
Best mean reward: 94.30 - Last mean reward per episode: 92.82
Num timesteps: 1458000
Best mean reward: 94.30 - Last mean reward per episode: 92.45
Num timesteps: 1459000
Best mean reward: 94.30 - Last mean reward per episode: 92.39
Num timesteps: 1460000
Best mean reward: 94.30 - Last mean reward per episode: 92.60
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 5.27     |
| reference_action_mean   | -0.293   |
| reference_action_std    | 0.944    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 5.7      |
| rollout/Q_mean          | 60.2     |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.3e+04  |
| rollout/return          | 90.9     |
| rollout/return_history  | 92.6     |
| total/duration          | 3.72e+03 |
| total/episodes          | 1.3e+04  |
| total/epochs            | 1        |
| total/steps             | 1459998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.416    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1461000
Best mean reward: 94.30 - Last mean reward per episode: 92.68
Num timesteps: 1462000
Best mean reward: 94.30 - Last mean reward per episode: 92.49
Num timesteps: 1463000
Best mean reward: 94.30 - Last mean reward per episode: 92.55
Num timesteps: 1464000
Best mean reward: 94.30 - Last mean reward per episode: 92.46
Num timesteps: 1465000
Best mean reward: 94.30 - Last mean reward per episode: 92.41
Num timesteps: 1466000
Best mean reward: 94.30 - Last mean reward per episode: 92.43
Num timesteps: 1467000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
Num timesteps: 1468000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 1469000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 1470000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 5.35     |
| reference_action_mean   | -0.253   |
| reference_action_std    | 0.958    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 5.67     |
| rollout/Q_mean          | 60.3     |
| rollout/actions_mean    | 0.144    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.31e+04 |
| rollout/return          | 90.9     |
| rollout/return_history  | 93.2     |
| total/duration          | 3.74e+03 |
| total/episodes          | 1.31e+04 |
| total/epochs            | 1        |
| total/steps             | 1469998  |
| total/steps_per_second  | 393      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.4      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1471000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 1472000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 1473000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 1474000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 1475000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 1476000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
Num timesteps: 1477000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
Num timesteps: 1478000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 1479000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 1480000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 5.69     |
| reference_action_mean   | -0.253   |
| reference_action_std    | 0.957    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 5.84     |
| rollout/Q_mean          | 60.4     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.32e+04 |
| rollout/return          | 90.9     |
| rollout/return_history  | 93.6     |
| total/duration          | 3.77e+03 |
| total/episodes          | 1.32e+04 |
| total/epochs            | 1        |
| total/steps             | 1479998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.376    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1481000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 1482000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 1483000
Best mean reward: 94.30 - Last mean reward per episode: 92.02
Num timesteps: 1484000
Best mean reward: 94.30 - Last mean reward per episode: 91.96
Num timesteps: 1485000
Best mean reward: 94.30 - Last mean reward per episode: 91.94
Num timesteps: 1486000
Best mean reward: 94.30 - Last mean reward per episode: 91.85
Num timesteps: 1487000
Best mean reward: 94.30 - Last mean reward per episode: 91.74
Num timesteps: 1488000
Best mean reward: 94.30 - Last mean reward per episode: 91.74
Num timesteps: 1489000
Best mean reward: 94.30 - Last mean reward per episode: 91.81
Num timesteps: 1490000
Best mean reward: 94.30 - Last mean reward per episode: 91.74
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 5.86     |
| reference_action_mean   | -0.224   |
| reference_action_std    | 0.967    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 6.05     |
| rollout/Q_mean          | 60.4     |
| rollout/actions_mean    | 0.145    |
| rollout/actions_std     | 0.811    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.33e+04 |
| rollout/return          | 90.9     |
| rollout/return_history  | 91.7     |
| total/duration          | 3.8e+03  |
| total/episodes          | 1.33e+04 |
| total/epochs            | 1        |
| total/steps             | 1489998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.333    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1491000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 1492000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 1493000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 1494000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 1495000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 1496000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 1497000
Best mean reward: 94.30 - Last mean reward per episode: 93.43
Num timesteps: 1498000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 1499000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 1500000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 5.91     |
| reference_action_mean   | -0.218   |
| reference_action_std    | 0.967    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 6.18     |
| rollout/Q_mean          | 60.5     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.811    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.35e+04 |
| rollout/return          | 90.9     |
| rollout/return_history  | 93.4     |
| total/duration          | 3.83e+03 |
| total/episodes          | 1.35e+04 |
| total/epochs            | 1        |
| total/steps             | 1499998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.499    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1501000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 1502000
Best mean reward: 94.30 - Last mean reward per episode: 93.34
Num timesteps: 1503000
Best mean reward: 94.30 - Last mean reward per episode: 93.43
Num timesteps: 1504000
Best mean reward: 94.30 - Last mean reward per episode: 93.40
Num timesteps: 1505000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 1506000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 1507000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 1508000
Best mean reward: 94.30 - Last mean reward per episode: 93.34
Num timesteps: 1509000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 1510000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 6.36     |
| reference_action_mean   | -0.181   |
| reference_action_std    | 0.974    |
| reference_actor_Q_mean  | 50.8     |
| reference_actor_Q_std   | 6.72     |
| rollout/Q_mean          | 60.5     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.811    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.36e+04 |
| rollout/return          | 90.9     |
| rollout/return_history  | 93.1     |
| total/duration          | 3.85e+03 |
| total/episodes          | 1.36e+04 |
| total/epochs            | 1        |
| total/steps             | 1509998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 0.38     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1511000
Best mean reward: 94.30 - Last mean reward per episode: 93.03
Num timesteps: 1512000
Best mean reward: 94.30 - Last mean reward per episode: 92.67
Num timesteps: 1513000
Best mean reward: 94.30 - Last mean reward per episode: 92.62
Num timesteps: 1514000
Best mean reward: 94.30 - Last mean reward per episode: 92.52
Num timesteps: 1515000
Best mean reward: 94.30 - Last mean reward per episode: 92.32
Num timesteps: 1516000
Best mean reward: 94.30 - Last mean reward per episode: 92.38
Num timesteps: 1517000
Best mean reward: 94.30 - Last mean reward per episode: 92.40
Num timesteps: 1518000
Best mean reward: 94.30 - Last mean reward per episode: 92.22
Num timesteps: 1519000
Best mean reward: 94.30 - Last mean reward per episode: 92.50
Num timesteps: 1520000
Best mean reward: 94.30 - Last mean reward per episode: 92.27
--------------------------------------
| reference_Q_mean        | 49.2     |
| reference_Q_std         | 6.86     |
| reference_action_mean   | -0.19    |
| reference_action_std    | 0.973    |
| reference_actor_Q_mean  | 51.2     |
| reference_actor_Q_std   | 7.19     |
| rollout/Q_mean          | 60.6     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.37e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 92.3     |
| total/duration          | 3.88e+03 |
| total/episodes          | 1.37e+04 |
| total/epochs            | 1        |
| total/steps             | 1519998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 0.411    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1521000
Best mean reward: 94.30 - Last mean reward per episode: 92.21
Num timesteps: 1522000
Best mean reward: 94.30 - Last mean reward per episode: 92.52
Num timesteps: 1523000
Best mean reward: 94.30 - Last mean reward per episode: 92.58
Num timesteps: 1524000
Best mean reward: 94.30 - Last mean reward per episode: 92.71
Num timesteps: 1525000
Best mean reward: 94.30 - Last mean reward per episode: 92.58
Num timesteps: 1526000
Best mean reward: 94.30 - Last mean reward per episode: 92.53
Num timesteps: 1527000
Best mean reward: 94.30 - Last mean reward per episode: 92.72
Num timesteps: 1528000
Best mean reward: 94.30 - Last mean reward per episode: 92.59
Num timesteps: 1529000
Best mean reward: 94.30 - Last mean reward per episode: 92.78
Num timesteps: 1530000
Best mean reward: 94.30 - Last mean reward per episode: 91.38
--------------------------------------
| reference_Q_mean        | 48.9     |
| reference_Q_std         | 7.41     |
| reference_action_mean   | -0.238   |
| reference_action_std    | 0.967    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 7.61     |
| rollout/Q_mean          | 60.7     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.38e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 91.4     |
| total/duration          | 3.91e+03 |
| total/episodes          | 1.38e+04 |
| total/epochs            | 1        |
| total/steps             | 1529998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 0.316    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1531000
Best mean reward: 94.30 - Last mean reward per episode: 91.41
Num timesteps: 1532000
Best mean reward: 94.30 - Last mean reward per episode: 91.27
Num timesteps: 1533000
Best mean reward: 94.30 - Last mean reward per episode: 90.93
Num timesteps: 1534000
Best mean reward: 94.30 - Last mean reward per episode: 91.04
Num timesteps: 1535000
Best mean reward: 94.30 - Last mean reward per episode: 91.09
Num timesteps: 1536000
Best mean reward: 94.30 - Last mean reward per episode: 91.28
Num timesteps: 1537000
Best mean reward: 94.30 - Last mean reward per episode: 91.37
Num timesteps: 1538000
Best mean reward: 94.30 - Last mean reward per episode: 91.41
Num timesteps: 1539000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 1540000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 7.73     |
| reference_action_mean   | -0.202   |
| reference_action_std    | 0.971    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 7.91     |
| rollout/Q_mean          | 60.7     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.39e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 93.1     |
| total/duration          | 3.93e+03 |
| total/episodes          | 1.39e+04 |
| total/epochs            | 1        |
| total/steps             | 1539998  |
| total/steps_per_second  | 392      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 0.5      |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1541000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 1542000
Best mean reward: 94.30 - Last mean reward per episode: 93.55
Num timesteps: 1543000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1544000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 1545000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 1546000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 1547000
Best mean reward: 94.30 - Last mean reward per episode: 93.05
Num timesteps: 1548000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 1549000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 1550000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
--------------------------------------
| reference_Q_mean        | 50.2     |
| reference_Q_std         | 7.79     |
| reference_action_mean   | -0.23    |
| reference_action_std    | 0.959    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 8.04     |
| rollout/Q_mean          | 60.8     |
| rollout/actions_mean    | 0.148    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.4e+04  |
| rollout/return          | 91       |
| rollout/return_history  | 93.1     |
| total/duration          | 3.96e+03 |
| total/episodes          | 1.4e+04  |
| total/epochs            | 1        |
| total/steps             | 1549998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.423    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1551000
Best mean reward: 94.30 - Last mean reward per episode: 93.15
Num timesteps: 1552000
Best mean reward: 94.30 - Last mean reward per episode: 93.22
Num timesteps: 1553000
Best mean reward: 94.30 - Last mean reward per episode: 93.29
Num timesteps: 1554000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 1555000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
Num timesteps: 1556000
Best mean reward: 94.30 - Last mean reward per episode: 93.07
Num timesteps: 1557000
Best mean reward: 94.30 - Last mean reward per episode: 92.99
Num timesteps: 1558000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 1559000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
Num timesteps: 1560000
Best mean reward: 94.30 - Last mean reward per episode: 93.06
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 7.87     |
| reference_action_mean   | -0.256   |
| reference_action_std    | 0.96     |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 8.09     |
| rollout/Q_mean          | 60.8     |
| rollout/actions_mean    | 0.148    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.41e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 93.1     |
| total/duration          | 3.99e+03 |
| total/episodes          | 1.41e+04 |
| total/epochs            | 1        |
| total/steps             | 1559998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.399    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1561000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 1562000
Best mean reward: 94.30 - Last mean reward per episode: 92.93
Num timesteps: 1563000
Best mean reward: 94.30 - Last mean reward per episode: 92.82
Num timesteps: 1564000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 1565000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 1566000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 1567000
Best mean reward: 94.30 - Last mean reward per episode: 93.04
Num timesteps: 1568000
Best mean reward: 94.30 - Last mean reward per episode: 92.96
Num timesteps: 1569000
Best mean reward: 94.30 - Last mean reward per episode: 92.87
Num timesteps: 1570000
Best mean reward: 94.30 - Last mean reward per episode: 92.85
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 8.05     |
| reference_action_mean   | -0.254   |
| reference_action_std    | 0.96     |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 8.01     |
| rollout/Q_mean          | 60.9     |
| rollout/actions_mean    | 0.148    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.42e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 92.9     |
| total/duration          | 4.02e+03 |
| total/episodes          | 1.42e+04 |
| total/epochs            | 1        |
| total/steps             | 1569998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 1.26     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1571000
Best mean reward: 94.30 - Last mean reward per episode: 91.42
Num timesteps: 1572000
Best mean reward: 94.30 - Last mean reward per episode: 91.50
Num timesteps: 1573000
Best mean reward: 94.30 - Last mean reward per episode: 91.66
Num timesteps: 1574000
Best mean reward: 94.30 - Last mean reward per episode: 91.73
Num timesteps: 1575000
Best mean reward: 94.30 - Last mean reward per episode: 91.25
Num timesteps: 1576000
Best mean reward: 94.30 - Last mean reward per episode: 91.21
Num timesteps: 1577000
Best mean reward: 94.30 - Last mean reward per episode: 90.91
Num timesteps: 1578000
Best mean reward: 94.30 - Last mean reward per episode: 91.18
Num timesteps: 1579000
Best mean reward: 94.30 - Last mean reward per episode: 91.21
Num timesteps: 1580000
Best mean reward: 94.30 - Last mean reward per episode: 91.31
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 8.01     |
| reference_action_mean   | -0.254   |
| reference_action_std    | 0.955    |
| reference_actor_Q_mean  | 52.7     |
| reference_actor_Q_std   | 7.99     |
| rollout/Q_mean          | 60.9     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.43e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 91.3     |
| total/duration          | 4.04e+03 |
| total/episodes          | 1.43e+04 |
| total/epochs            | 1        |
| total/steps             | 1579998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.502    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1581000
Best mean reward: 94.30 - Last mean reward per episode: 91.23
Num timesteps: 1582000
Best mean reward: 94.30 - Last mean reward per episode: 92.65
Num timesteps: 1583000
Best mean reward: 94.30 - Last mean reward per episode: 92.69
Num timesteps: 1584000
Best mean reward: 94.30 - Last mean reward per episode: 92.70
Num timesteps: 1585000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 1586000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 1587000
Best mean reward: 94.30 - Last mean reward per episode: 93.41
Num timesteps: 1588000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 1589000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 1590000
Best mean reward: 94.30 - Last mean reward per episode: 92.93
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 7.56     |
| reference_action_mean   | -0.302   |
| reference_action_std    | 0.935    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.71     |
| rollout/Q_mean          | 61       |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.44e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 92.9     |
| total/duration          | 4.07e+03 |
| total/episodes          | 1.44e+04 |
| total/epochs            | 1        |
| total/steps             | 1589998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 1.34     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1591000
Best mean reward: 94.30 - Last mean reward per episode: 92.93
Num timesteps: 1592000
Best mean reward: 94.30 - Last mean reward per episode: 92.97
Num timesteps: 1593000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 1594000
Best mean reward: 94.30 - Last mean reward per episode: 92.76
Num timesteps: 1595000
Best mean reward: 94.30 - Last mean reward per episode: 92.77
Num timesteps: 1596000
Best mean reward: 94.30 - Last mean reward per episode: 92.86
Num timesteps: 1597000
Best mean reward: 94.30 - Last mean reward per episode: 92.79
Num timesteps: 1598000
Best mean reward: 94.30 - Last mean reward per episode: 92.93
Num timesteps: 1599000
Best mean reward: 94.30 - Last mean reward per episode: 93.01
Num timesteps: 1600000
Best mean reward: 94.30 - Last mean reward per episode: 93.40
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 7.14     |
| reference_action_mean   | -0.432   |
| reference_action_std    | 0.874    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 7.24     |
| rollout/Q_mean          | 61.1     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.45e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 4.1e+03  |
| total/episodes          | 1.45e+04 |
| total/epochs            | 1        |
| total/steps             | 1599998  |
| total/steps_per_second  | 391      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 1.19     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1601000
Best mean reward: 94.30 - Last mean reward per episode: 93.09
Num timesteps: 1602000
Best mean reward: 94.30 - Last mean reward per episode: 93.17
Num timesteps: 1603000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 1604000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 1605000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1606000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1607000
Best mean reward: 94.30 - Last mean reward per episode: 92.00
Num timesteps: 1608000
Best mean reward: 94.30 - Last mean reward per episode: 91.99
Num timesteps: 1609000
Best mean reward: 94.30 - Last mean reward per episode: 92.05
Num timesteps: 1610000
Best mean reward: 94.30 - Last mean reward per episode: 92.44
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 7        |
| reference_action_mean   | -0.476   |
| reference_action_std    | 0.847    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 7.12     |
| rollout/Q_mean          | 61.1     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.46e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 92.4     |
| total/duration          | 4.13e+03 |
| total/episodes          | 1.46e+04 |
| total/epochs            | 1        |
| total/steps             | 1609998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.437    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1611000
Best mean reward: 94.30 - Last mean reward per episode: 92.42
Num timesteps: 1612000
Best mean reward: 94.30 - Last mean reward per episode: 90.78
Num timesteps: 1613000
Best mean reward: 94.30 - Last mean reward per episode: 90.77
Num timesteps: 1614000
Best mean reward: 94.30 - Last mean reward per episode: 90.75
Num timesteps: 1615000
Best mean reward: 94.30 - Last mean reward per episode: 90.70
Num timesteps: 1616000
Best mean reward: 94.30 - Last mean reward per episode: 92.06
Num timesteps: 1617000
Best mean reward: 94.30 - Last mean reward per episode: 91.98
Num timesteps: 1618000
Best mean reward: 94.30 - Last mean reward per episode: 91.94
Num timesteps: 1619000
Best mean reward: 94.30 - Last mean reward per episode: 91.88
Num timesteps: 1620000
Best mean reward: 94.30 - Last mean reward per episode: 91.76
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 6.75     |
| reference_action_mean   | -0.426   |
| reference_action_std    | 0.862    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 6.93     |
| rollout/Q_mean          | 61.2     |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.47e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 91.8     |
| total/duration          | 4.15e+03 |
| total/episodes          | 1.47e+04 |
| total/epochs            | 1        |
| total/steps             | 1619998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.398    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1621000
Best mean reward: 94.30 - Last mean reward per episode: 93.30
Num timesteps: 1622000
Best mean reward: 94.30 - Last mean reward per episode: 93.09
Num timesteps: 1623000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 1624000
Best mean reward: 94.30 - Last mean reward per episode: 93.07
Num timesteps: 1625000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 1626000
Best mean reward: 94.30 - Last mean reward per episode: 93.11
Num timesteps: 1627000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 1628000
Best mean reward: 94.30 - Last mean reward per episode: 93.05
Num timesteps: 1629000
Best mean reward: 94.30 - Last mean reward per episode: 93.04
Num timesteps: 1630000
Best mean reward: 94.30 - Last mean reward per episode: 93.07
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 6.25     |
| reference_action_mean   | -0.227   |
| reference_action_std    | 0.947    |
| reference_actor_Q_mean  | 51.8     |
| reference_actor_Q_std   | 6.74     |
| rollout/Q_mean          | 61.2     |
| rollout/actions_mean    | 0.148    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.48e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.1     |
| total/duration          | 4.18e+03 |
| total/episodes          | 1.48e+04 |
| total/epochs            | 1        |
| total/steps             | 1629998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.449    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1631000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 1632000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
Num timesteps: 1633000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
Num timesteps: 1634000
Best mean reward: 94.30 - Last mean reward per episode: 92.71
Num timesteps: 1635000
Best mean reward: 94.30 - Last mean reward per episode: 92.72
Num timesteps: 1636000
Best mean reward: 94.30 - Last mean reward per episode: 92.78
Num timesteps: 1637000
Best mean reward: 94.30 - Last mean reward per episode: 92.87
Num timesteps: 1638000
Best mean reward: 94.30 - Last mean reward per episode: 92.86
Num timesteps: 1639000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 1640000
Best mean reward: 94.30 - Last mean reward per episode: 92.94
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 6.42     |
| reference_action_mean   | 0.139    |
| reference_action_std    | 0.972    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 6.58     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.148    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.49e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 92.9     |
| total/duration          | 4.21e+03 |
| total/episodes          | 1.49e+04 |
| total/epochs            | 1        |
| total/steps             | 1639998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.444    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1641000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 1642000
Best mean reward: 94.30 - Last mean reward per episode: 92.96
Num timesteps: 1643000
Best mean reward: 94.30 - Last mean reward per episode: 93.30
Num timesteps: 1644000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
Num timesteps: 1645000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 1646000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
Num timesteps: 1647000
Best mean reward: 94.30 - Last mean reward per episode: 93.38
Num timesteps: 1648000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
Num timesteps: 1649000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 1650000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 6.11     |
| reference_action_mean   | -0.312   |
| reference_action_std    | 0.919    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 6.48     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.5e+04  |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.2     |
| total/duration          | 4.24e+03 |
| total/episodes          | 1.5e+04  |
| total/epochs            | 1        |
| total/steps             | 1649998  |
| total/steps_per_second  | 390      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 1.48     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1651000
Best mean reward: 94.30 - Last mean reward per episode: 93.38
Num timesteps: 1652000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 1653000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
Num timesteps: 1654000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 1655000
Best mean reward: 94.30 - Last mean reward per episode: 93.17
Num timesteps: 1656000
Best mean reward: 94.30 - Last mean reward per episode: 93.06
Num timesteps: 1657000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 1658000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 1659000
Best mean reward: 94.30 - Last mean reward per episode: 93.05
Num timesteps: 1660000
Best mean reward: 94.30 - Last mean reward per episode: 93.09
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 5.79     |
| reference_action_mean   | -0.399   |
| reference_action_std    | 0.873    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 5.92     |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.51e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.1     |
| total/duration          | 4.26e+03 |
| total/episodes          | 1.51e+04 |
| total/epochs            | 1        |
| total/steps             | 1659998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -69.4    |
| train/loss_critic       | 0.504    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1661000
Best mean reward: 94.30 - Last mean reward per episode: 92.94
Num timesteps: 1662000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 1663000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 1664000
Best mean reward: 94.30 - Last mean reward per episode: 91.66
Num timesteps: 1665000
Best mean reward: 94.30 - Last mean reward per episode: 91.69
Num timesteps: 1666000
Best mean reward: 94.30 - Last mean reward per episode: 91.42
Num timesteps: 1667000
Best mean reward: 94.30 - Last mean reward per episode: 91.13
Num timesteps: 1668000
Best mean reward: 94.30 - Last mean reward per episode: 91.12
Num timesteps: 1669000
Best mean reward: 94.30 - Last mean reward per episode: 89.63
Num timesteps: 1670000
Best mean reward: 94.30 - Last mean reward per episode: 89.56
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 5.92     |
| reference_action_mean   | -0.199   |
| reference_action_std    | 0.946    |
| reference_actor_Q_mean  | 53       |
| reference_actor_Q_std   | 5.8      |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.148    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.52e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 89.6     |
| total/duration          | 4.29e+03 |
| total/episodes          | 1.52e+04 |
| total/epochs            | 1        |
| total/steps             | 1669998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -68.9    |
| train/loss_critic       | 0.549    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1671000
Best mean reward: 94.30 - Last mean reward per episode: 89.31
Num timesteps: 1672000
Best mean reward: 94.30 - Last mean reward per episode: 89.26
Num timesteps: 1673000
Best mean reward: 94.30 - Last mean reward per episode: 87.65
Num timesteps: 1674000
Best mean reward: 94.30 - Last mean reward per episode: 87.74
Num timesteps: 1675000
Best mean reward: 94.30 - Last mean reward per episode: 87.81
Num timesteps: 1676000
Best mean reward: 94.30 - Last mean reward per episode: 87.87
Num timesteps: 1677000
Best mean reward: 94.30 - Last mean reward per episode: 89.25
Num timesteps: 1678000
Best mean reward: 94.30 - Last mean reward per episode: 89.39
Num timesteps: 1679000
Best mean reward: 94.30 - Last mean reward per episode: 89.65
Num timesteps: 1680000
Best mean reward: 94.30 - Last mean reward per episode: 91.16
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 5.75     |
| reference_action_mean   | -0.232   |
| reference_action_std    | 0.915    |
| reference_actor_Q_mean  | 53.7     |
| reference_actor_Q_std   | 5.4      |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.53e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 91.2     |
| total/duration          | 4.32e+03 |
| total/episodes          | 1.53e+04 |
| total/epochs            | 1        |
| total/steps             | 1679998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -68.6    |
| train/loss_critic       | 0.669    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1681000
Best mean reward: 94.30 - Last mean reward per episode: 91.23
Num timesteps: 1682000
Best mean reward: 94.30 - Last mean reward per episode: 91.39
Num timesteps: 1683000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 1684000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 1685000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
Num timesteps: 1686000
Best mean reward: 94.30 - Last mean reward per episode: 93.06
Num timesteps: 1687000
Best mean reward: 94.30 - Last mean reward per episode: 92.99
Num timesteps: 1688000
Best mean reward: 94.30 - Last mean reward per episode: 91.03
Num timesteps: 1689000
Best mean reward: 94.30 - Last mean reward per episode: 91.00
Num timesteps: 1690000
Best mean reward: 94.30 - Last mean reward per episode: 90.93
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 5.61     |
| reference_action_mean   | -0.345   |
| reference_action_std    | 0.85     |
| reference_actor_Q_mean  | 54.2     |
| reference_actor_Q_std   | 5.35     |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.54e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 90.9     |
| total/duration          | 4.34e+03 |
| total/episodes          | 1.54e+04 |
| total/epochs            | 1        |
| total/steps             | 1689998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 1.63     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1691000
Best mean reward: 94.30 - Last mean reward per episode: 90.87
Num timesteps: 1692000
Best mean reward: 94.30 - Last mean reward per episode: 90.94
Num timesteps: 1693000
Best mean reward: 94.30 - Last mean reward per episode: 91.07
Num timesteps: 1694000
Best mean reward: 94.30 - Last mean reward per episode: 91.15
Num timesteps: 1695000
Best mean reward: 94.30 - Last mean reward per episode: 91.12
Num timesteps: 1696000
Best mean reward: 94.30 - Last mean reward per episode: 91.11
Num timesteps: 1697000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 1698000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 1699000
Best mean reward: 94.30 - Last mean reward per episode: 93.01
Num timesteps: 1700000
Best mean reward: 94.30 - Last mean reward per episode: 93.11
--------------------------------------
| reference_Q_mean        | 52.8     |
| reference_Q_std         | 5.29     |
| reference_action_mean   | -0.413   |
| reference_action_std    | 0.815    |
| reference_actor_Q_mean  | 54.1     |
| reference_actor_Q_std   | 5.07     |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.55e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.1     |
| total/duration          | 4.37e+03 |
| total/episodes          | 1.55e+04 |
| total/epochs            | 1        |
| total/steps             | 1699998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -67.7    |
| train/loss_critic       | 1.47     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1701000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 1702000
Best mean reward: 94.30 - Last mean reward per episode: 91.23
Num timesteps: 1703000
Best mean reward: 94.30 - Last mean reward per episode: 90.72
Num timesteps: 1704000
Best mean reward: 94.30 - Last mean reward per episode: 90.34
Num timesteps: 1705000
Best mean reward: 94.30 - Last mean reward per episode: 90.18
Num timesteps: 1706000
Best mean reward: 94.30 - Last mean reward per episode: 89.85
Num timesteps: 1707000
Best mean reward: 94.30 - Last mean reward per episode: 89.56
Num timesteps: 1708000
Best mean reward: 94.30 - Last mean reward per episode: 89.32
Num timesteps: 1709000
Best mean reward: 94.30 - Last mean reward per episode: 89.04
Num timesteps: 1710000
Best mean reward: 94.30 - Last mean reward per episode: 88.74
--------------------------------------
| reference_Q_mean        | 52.8     |
| reference_Q_std         | 5.05     |
| reference_action_mean   | 0.186    |
| reference_action_std    | 0.96     |
| reference_actor_Q_mean  | 53.8     |
| reference_actor_Q_std   | 4.8      |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.55e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 88.7     |
| total/duration          | 4.4e+03  |
| total/episodes          | 1.55e+04 |
| total/epochs            | 1        |
| total/steps             | 1709998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -66.5    |
| train/loss_critic       | 1.44     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1711000
Best mean reward: 94.30 - Last mean reward per episode: 88.60
Num timesteps: 1712000
Best mean reward: 94.30 - Last mean reward per episode: 88.28
Num timesteps: 1713000
Best mean reward: 94.30 - Last mean reward per episode: 88.02
Num timesteps: 1714000
Best mean reward: 94.30 - Last mean reward per episode: 87.84
Num timesteps: 1715000
Best mean reward: 94.30 - Last mean reward per episode: 87.77
Num timesteps: 1716000
Best mean reward: 94.30 - Last mean reward per episode: 87.44
Num timesteps: 1717000
Best mean reward: 94.30 - Last mean reward per episode: 87.22
Num timesteps: 1718000
Best mean reward: 94.30 - Last mean reward per episode: 89.45
Num timesteps: 1719000
Best mean reward: 94.30 - Last mean reward per episode: 89.59
Num timesteps: 1720000
Best mean reward: 94.30 - Last mean reward per episode: 89.68
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 5.1      |
| reference_action_mean   | 0.114    |
| reference_action_std    | 0.951    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 4.89     |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.56e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 89.7     |
| total/duration          | 4.42e+03 |
| total/episodes          | 1.56e+04 |
| total/epochs            | 1        |
| total/steps             | 1719998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -64.7    |
| train/loss_critic       | 1.45     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1721000
Best mean reward: 94.30 - Last mean reward per episode: 89.77
Num timesteps: 1722000
Best mean reward: 94.30 - Last mean reward per episode: 89.81
Num timesteps: 1723000
Best mean reward: 94.30 - Last mean reward per episode: 89.92
Num timesteps: 1724000
Best mean reward: 94.30 - Last mean reward per episode: 89.93
Num timesteps: 1725000
Best mean reward: 94.30 - Last mean reward per episode: 89.74
Num timesteps: 1726000
Best mean reward: 94.30 - Last mean reward per episode: 89.76
Num timesteps: 1727000
Best mean reward: 94.30 - Last mean reward per episode: 88.21
Num timesteps: 1728000
Best mean reward: 94.30 - Last mean reward per episode: 88.19
Num timesteps: 1729000
Best mean reward: 94.30 - Last mean reward per episode: 88.02
Num timesteps: 1730000
Best mean reward: 94.30 - Last mean reward per episode: 87.90
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 5.14     |
| reference_action_mean   | 0.128    |
| reference_action_std    | 0.96     |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 5.05     |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.57e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 87.9     |
| total/duration          | 4.45e+03 |
| total/episodes          | 1.57e+04 |
| total/epochs            | 1        |
| total/steps             | 1729998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -62.9    |
| train/loss_critic       | 1.02     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1731000
Best mean reward: 94.30 - Last mean reward per episode: 88.08
Num timesteps: 1732000
Best mean reward: 94.30 - Last mean reward per episode: 87.18
Num timesteps: 1733000
Best mean reward: 94.30 - Last mean reward per episode: 86.25
Num timesteps: 1734000
Best mean reward: 94.30 - Last mean reward per episode: 85.19
Num timesteps: 1735000
Best mean reward: 94.30 - Last mean reward per episode: 84.10
Num timesteps: 1736000
Best mean reward: 94.30 - Last mean reward per episode: 83.12
Num timesteps: 1737000
Best mean reward: 94.30 - Last mean reward per episode: 83.11
Num timesteps: 1738000
Best mean reward: 94.30 - Last mean reward per episode: 82.11
Num timesteps: 1739000
Best mean reward: 94.30 - Last mean reward per episode: 81.02
Num timesteps: 1740000
Best mean reward: 94.30 - Last mean reward per episode: 80.82
--------------------------------------
| reference_Q_mean        | 50       |
| reference_Q_std         | 5.44     |
| reference_action_mean   | 0.335    |
| reference_action_std    | 0.914    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 5.42     |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.57e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 80.8     |
| total/duration          | 4.48e+03 |
| total/episodes          | 1.57e+04 |
| total/epochs            | 1        |
| total/steps             | 1739998  |
| total/steps_per_second  | 389      |
| train/loss_actor        | -58.8    |
| train/loss_critic       | 0.86     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1741000
Best mean reward: 94.30 - Last mean reward per episode: 80.63
Num timesteps: 1742000
Best mean reward: 94.30 - Last mean reward per episode: 80.48
Num timesteps: 1743000
Best mean reward: 94.30 - Last mean reward per episode: 80.35
Num timesteps: 1744000
Best mean reward: 94.30 - Last mean reward per episode: 80.19
Num timesteps: 1745000
Best mean reward: 94.30 - Last mean reward per episode: 80.05
Num timesteps: 1746000
Best mean reward: 94.30 - Last mean reward per episode: 79.87
Num timesteps: 1747000
Best mean reward: 94.30 - Last mean reward per episode: 79.73
Num timesteps: 1748000
Best mean reward: 94.30 - Last mean reward per episode: 79.66
Num timesteps: 1749000
Best mean reward: 94.30 - Last mean reward per episode: 79.39
Num timesteps: 1750000
Best mean reward: 94.30 - Last mean reward per episode: 79.34
--------------------------------------
| reference_Q_mean        | 49.4     |
| reference_Q_std         | 5.54     |
| reference_action_mean   | 0.382    |
| reference_action_std    | 0.9      |
| reference_actor_Q_mean  | 49.9     |
| reference_actor_Q_std   | 5.68     |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.57e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 79.3     |
| total/duration          | 4.5e+03  |
| total/episodes          | 1.57e+04 |
| total/epochs            | 1        |
| total/steps             | 1749998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -56.4    |
| train/loss_critic       | 0.922    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1751000
Best mean reward: 94.30 - Last mean reward per episode: 79.38
Num timesteps: 1752000
Best mean reward: 94.30 - Last mean reward per episode: 80.75
Num timesteps: 1753000
Best mean reward: 94.30 - Last mean reward per episode: 80.70
Num timesteps: 1754000
Best mean reward: 94.30 - Last mean reward per episode: 80.68
Num timesteps: 1755000
Best mean reward: 94.30 - Last mean reward per episode: 80.45
Num timesteps: 1756000
Best mean reward: 94.30 - Last mean reward per episode: 80.49
Num timesteps: 1757000
Best mean reward: 94.30 - Last mean reward per episode: 80.74
Num timesteps: 1758000
Best mean reward: 94.30 - Last mean reward per episode: 81.70
Num timesteps: 1759000
Best mean reward: 94.30 - Last mean reward per episode: 81.66
Num timesteps: 1760000
Best mean reward: 94.30 - Last mean reward per episode: 82.51
--------------------------------------
| reference_Q_mean        | 48.6     |
| reference_Q_std         | 6.02     |
| reference_action_mean   | 0.154    |
| reference_action_std    | 0.957    |
| reference_actor_Q_mean  | 48.9     |
| reference_actor_Q_std   | 6.05     |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.811    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.58e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 82.5     |
| total/duration          | 4.53e+03 |
| total/episodes          | 1.58e+04 |
| total/epochs            | 1        |
| total/steps             | 1759998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -54.3    |
| train/loss_critic       | 1.73     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1761000
Best mean reward: 94.30 - Last mean reward per episode: 84.48
Num timesteps: 1762000
Best mean reward: 94.30 - Last mean reward per episode: 85.44
Num timesteps: 1763000
Best mean reward: 94.30 - Last mean reward per episode: 86.05
Num timesteps: 1764000
Best mean reward: 94.30 - Last mean reward per episode: 87.03
Num timesteps: 1765000
Best mean reward: 94.30 - Last mean reward per episode: 85.71
Num timesteps: 1766000
Best mean reward: 94.30 - Last mean reward per episode: 86.03
Num timesteps: 1767000
Best mean reward: 94.30 - Last mean reward per episode: 86.07
Num timesteps: 1768000
Best mean reward: 94.30 - Last mean reward per episode: 86.22
Num timesteps: 1769000
Best mean reward: 94.30 - Last mean reward per episode: 86.63
Num timesteps: 1770000
Best mean reward: 94.30 - Last mean reward per episode: 86.65
--------------------------------------
| reference_Q_mean        | 47.5     |
| reference_Q_std         | 5.88     |
| reference_action_mean   | 0.496    |
| reference_action_std    | 0.846    |
| reference_actor_Q_mean  | 47.8     |
| reference_actor_Q_std   | 6.03     |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.811    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.58e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 86.7     |
| total/duration          | 4.56e+03 |
| total/episodes          | 1.58e+04 |
| total/epochs            | 1        |
| total/steps             | 1769998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -53.7    |
| train/loss_critic       | 1.76     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1771000
Best mean reward: 94.30 - Last mean reward per episode: 86.49
Num timesteps: 1772000
Best mean reward: 94.30 - Last mean reward per episode: 86.44
Num timesteps: 1773000
Best mean reward: 94.30 - Last mean reward per episode: 86.49
Num timesteps: 1774000
Best mean reward: 94.30 - Last mean reward per episode: 86.73
Num timesteps: 1775000
Best mean reward: 94.30 - Last mean reward per episode: 87.20
Num timesteps: 1776000
Best mean reward: 94.30 - Last mean reward per episode: 87.63
Num timesteps: 1777000
Best mean reward: 94.30 - Last mean reward per episode: 87.80
Num timesteps: 1778000
Best mean reward: 94.30 - Last mean reward per episode: 87.61
Num timesteps: 1779000
Best mean reward: 94.30 - Last mean reward per episode: 87.92
Num timesteps: 1780000
Best mean reward: 94.30 - Last mean reward per episode: 89.90
--------------------------------------
| reference_Q_mean        | 46.2     |
| reference_Q_std         | 6.12     |
| reference_action_mean   | 0.711    |
| reference_action_std    | 0.687    |
| reference_actor_Q_mean  | 46.4     |
| reference_actor_Q_std   | 6.19     |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.59e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 89.9     |
| total/duration          | 4.58e+03 |
| total/episodes          | 1.59e+04 |
| total/epochs            | 1        |
| total/steps             | 1779998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -53.1    |
| train/loss_critic       | 1.11     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1781000
Best mean reward: 94.30 - Last mean reward per episode: 89.86
Num timesteps: 1782000
Best mean reward: 94.30 - Last mean reward per episode: 89.95
Num timesteps: 1783000
Best mean reward: 94.30 - Last mean reward per episode: 89.92
Num timesteps: 1784000
Best mean reward: 94.30 - Last mean reward per episode: 90.15
Num timesteps: 1785000
Best mean reward: 94.30 - Last mean reward per episode: 90.19
Num timesteps: 1786000
Best mean reward: 94.30 - Last mean reward per episode: 90.64
Num timesteps: 1787000
Best mean reward: 94.30 - Last mean reward per episode: 90.75
Num timesteps: 1788000
Best mean reward: 94.30 - Last mean reward per episode: 90.65
Num timesteps: 1789000
Best mean reward: 94.30 - Last mean reward per episode: 90.64
Num timesteps: 1790000
Best mean reward: 94.30 - Last mean reward per episode: 90.60
--------------------------------------
| reference_Q_mean        | 44.6     |
| reference_Q_std         | 6.13     |
| reference_action_mean   | 0.714    |
| reference_action_std    | 0.69     |
| reference_actor_Q_mean  | 44.7     |
| reference_actor_Q_std   | 6.14     |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.156    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.6e+04  |
| rollout/return          | 91       |
| rollout/return_history  | 90.6     |
| total/duration          | 4.61e+03 |
| total/episodes          | 1.6e+04  |
| total/epochs            | 1        |
| total/steps             | 1789998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -56.2    |
| train/loss_critic       | 0.939    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1791000
Best mean reward: 94.30 - Last mean reward per episode: 90.77
Num timesteps: 1792000
Best mean reward: 94.30 - Last mean reward per episode: 90.71
Num timesteps: 1793000
Best mean reward: 94.30 - Last mean reward per episode: 90.62
Num timesteps: 1794000
Best mean reward: 94.30 - Last mean reward per episode: 90.66
Num timesteps: 1795000
Best mean reward: 94.30 - Last mean reward per episode: 90.69
Num timesteps: 1796000
Best mean reward: 94.30 - Last mean reward per episode: 90.54
Num timesteps: 1797000
Best mean reward: 94.30 - Last mean reward per episode: 90.49
Num timesteps: 1798000
Best mean reward: 94.30 - Last mean reward per episode: 88.98
Num timesteps: 1799000
Best mean reward: 94.30 - Last mean reward per episode: 88.99
Num timesteps: 1800000
Best mean reward: 94.30 - Last mean reward per episode: 88.93
--------------------------------------
| reference_Q_mean        | 43.9     |
| reference_Q_std         | 6.52     |
| reference_action_mean   | 0.719    |
| reference_action_std    | 0.69     |
| reference_actor_Q_mean  | 44.3     |
| reference_actor_Q_std   | 6.62     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.6e+04  |
| rollout/return          | 91       |
| rollout/return_history  | 88.9     |
| total/duration          | 4.64e+03 |
| total/episodes          | 1.6e+04  |
| total/epochs            | 1        |
| total/steps             | 1799998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -57      |
| train/loss_critic       | 0.857    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1801000
Best mean reward: 94.30 - Last mean reward per episode: 88.84
Num timesteps: 1802000
Best mean reward: 94.30 - Last mean reward per episode: 88.84
Num timesteps: 1803000
Best mean reward: 94.30 - Last mean reward per episode: 88.77
Num timesteps: 1804000
Best mean reward: 94.30 - Last mean reward per episode: 88.82
Num timesteps: 1805000
Best mean reward: 94.30 - Last mean reward per episode: 88.82
Num timesteps: 1806000
Best mean reward: 94.30 - Last mean reward per episode: 88.88
Num timesteps: 1807000
Best mean reward: 94.30 - Last mean reward per episode: 88.82
Num timesteps: 1808000
Best mean reward: 94.30 - Last mean reward per episode: 88.78
Num timesteps: 1809000
Best mean reward: 94.30 - Last mean reward per episode: 88.74
Num timesteps: 1810000
Best mean reward: 94.30 - Last mean reward per episode: 88.79
--------------------------------------
| reference_Q_mean        | 43.7     |
| reference_Q_std         | 6.98     |
| reference_action_mean   | 0.694    |
| reference_action_std    | 0.706    |
| reference_actor_Q_mean  | 44.3     |
| reference_actor_Q_std   | 7.08     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.159    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.61e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 88.8     |
| total/duration          | 4.66e+03 |
| total/episodes          | 1.61e+04 |
| total/epochs            | 1        |
| total/steps             | 1809998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -58.6    |
| train/loss_critic       | 0.871    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1811000
Best mean reward: 94.30 - Last mean reward per episode: 90.45
Num timesteps: 1812000
Best mean reward: 94.30 - Last mean reward per episode: 90.52
Num timesteps: 1813000
Best mean reward: 94.30 - Last mean reward per episode: 90.56
Num timesteps: 1814000
Best mean reward: 94.30 - Last mean reward per episode: 90.62
Num timesteps: 1815000
Best mean reward: 94.30 - Last mean reward per episode: 90.60
Num timesteps: 1816000
Best mean reward: 94.30 - Last mean reward per episode: 90.58
Num timesteps: 1817000
Best mean reward: 94.30 - Last mean reward per episode: 90.64
Num timesteps: 1818000
Best mean reward: 94.30 - Last mean reward per episode: 90.68
Num timesteps: 1819000
Best mean reward: 94.30 - Last mean reward per episode: 90.73
Num timesteps: 1820000
Best mean reward: 94.30 - Last mean reward per episode: 90.82
--------------------------------------
| reference_Q_mean        | 43.4     |
| reference_Q_std         | 7.49     |
| reference_action_mean   | 0.641    |
| reference_action_std    | 0.754    |
| reference_actor_Q_mean  | 44.3     |
| reference_actor_Q_std   | 7.67     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.161    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.62e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 90.8     |
| total/duration          | 4.69e+03 |
| total/episodes          | 1.62e+04 |
| total/epochs            | 1        |
| total/steps             | 1819998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -59.2    |
| train/loss_critic       | 0.951    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1821000
Best mean reward: 94.30 - Last mean reward per episode: 90.84
Num timesteps: 1822000
Best mean reward: 94.30 - Last mean reward per episode: 90.78
Num timesteps: 1823000
Best mean reward: 94.30 - Last mean reward per episode: 90.76
Num timesteps: 1824000
Best mean reward: 94.30 - Last mean reward per episode: 89.78
Num timesteps: 1825000
Best mean reward: 94.30 - Last mean reward per episode: 88.80
Num timesteps: 1826000
Best mean reward: 94.30 - Last mean reward per episode: 87.51
Num timesteps: 1827000
Best mean reward: 94.30 - Last mean reward per episode: 86.55
Num timesteps: 1828000
Best mean reward: 94.30 - Last mean reward per episode: 86.43
Num timesteps: 1829000
Best mean reward: 94.30 - Last mean reward per episode: 85.44
Num timesteps: 1830000
Best mean reward: 94.30 - Last mean reward per episode: 85.26
--------------------------------------
| reference_Q_mean        | 43.9     |
| reference_Q_std         | 8.05     |
| reference_action_mean   | 0.32     |
| reference_action_std    | 0.931    |
| reference_actor_Q_mean  | 44.4     |
| reference_actor_Q_std   | 8.08     |
| rollout/Q_mean          | 61.2     |
| rollout/actions_mean    | 0.161    |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.62e+04 |
| rollout/return          | 90.9     |
| rollout/return_history  | 85.3     |
| total/duration          | 4.72e+03 |
| total/episodes          | 1.62e+04 |
| total/epochs            | 1        |
| total/steps             | 1829998  |
| total/steps_per_second  | 388      |
| train/loss_actor        | -56.8    |
| train/loss_critic       | 0.778    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1831000
Best mean reward: 94.30 - Last mean reward per episode: 85.24
Num timesteps: 1832000
Best mean reward: 94.30 - Last mean reward per episode: 85.24
Num timesteps: 1833000
Best mean reward: 94.30 - Last mean reward per episode: 85.25
Num timesteps: 1834000
Best mean reward: 94.30 - Last mean reward per episode: 85.30
Num timesteps: 1835000
Best mean reward: 94.30 - Last mean reward per episode: 85.19
Num timesteps: 1836000
Best mean reward: 94.30 - Last mean reward per episode: 84.85
Num timesteps: 1837000
Best mean reward: 94.30 - Last mean reward per episode: 84.93
Num timesteps: 1838000
Best mean reward: 94.30 - Last mean reward per episode: 85.00
Num timesteps: 1839000
Best mean reward: 94.30 - Last mean reward per episode: 85.26
Num timesteps: 1840000
Best mean reward: 94.30 - Last mean reward per episode: 85.52
--------------------------------------
| reference_Q_mean        | 43.6     |
| reference_Q_std         | 8.58     |
| reference_action_mean   | -0.501   |
| reference_action_std    | 0.839    |
| reference_actor_Q_mean  | 43.8     |
| reference_actor_Q_std   | 8.55     |
| rollout/Q_mean          | 61.2     |
| rollout/actions_mean    | 0.161    |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.63e+04 |
| rollout/return          | 90.9     |
| rollout/return_history  | 85.5     |
| total/duration          | 4.75e+03 |
| total/episodes          | 1.63e+04 |
| total/epochs            | 1        |
| total/steps             | 1839998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -56.1    |
| train/loss_critic       | 2.06     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1841000
Best mean reward: 94.30 - Last mean reward per episode: 85.65
Num timesteps: 1842000
Best mean reward: 94.30 - Last mean reward per episode: 91.05
Num timesteps: 1843000
Best mean reward: 94.30 - Last mean reward per episode: 91.35
Num timesteps: 1844000
Best mean reward: 94.30 - Last mean reward per episode: 91.36
Num timesteps: 1845000
Best mean reward: 94.30 - Last mean reward per episode: 91.58
Num timesteps: 1846000
Best mean reward: 94.30 - Last mean reward per episode: 92.08
Num timesteps: 1847000
Best mean reward: 94.30 - Last mean reward per episode: 92.11
Num timesteps: 1848000
Best mean reward: 94.30 - Last mean reward per episode: 92.15
Num timesteps: 1849000
Best mean reward: 94.30 - Last mean reward per episode: 91.97
Num timesteps: 1850000
Best mean reward: 94.30 - Last mean reward per episode: 91.99
--------------------------------------
| reference_Q_mean        | 42.1     |
| reference_Q_std         | 8.47     |
| reference_action_mean   | -0.303   |
| reference_action_std    | 0.94     |
| reference_actor_Q_mean  | 42.5     |
| reference_actor_Q_std   | 8.37     |
| rollout/Q_mean          | 61.2     |
| rollout/actions_mean    | 0.159    |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.64e+04 |
| rollout/return          | 90.9     |
| rollout/return_history  | 92       |
| total/duration          | 4.78e+03 |
| total/episodes          | 1.64e+04 |
| total/epochs            | 1        |
| total/steps             | 1849998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -57.9    |
| train/loss_critic       | 1.21     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1851000
Best mean reward: 94.30 - Last mean reward per episode: 92.03
Num timesteps: 1852000
Best mean reward: 94.30 - Last mean reward per episode: 92.21
Num timesteps: 1853000
Best mean reward: 94.30 - Last mean reward per episode: 92.28
Num timesteps: 1854000
Best mean reward: 94.30 - Last mean reward per episode: 92.48
Num timesteps: 1855000
Best mean reward: 94.30 - Last mean reward per episode: 92.65
Num timesteps: 1856000
Best mean reward: 94.30 - Last mean reward per episode: 92.75
Num timesteps: 1857000
Best mean reward: 94.30 - Last mean reward per episode: 92.92
Num timesteps: 1858000
Best mean reward: 94.30 - Last mean reward per episode: 92.90
Num timesteps: 1859000
Best mean reward: 94.30 - Last mean reward per episode: 93.22
Num timesteps: 1860000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
--------------------------------------
| reference_Q_mean        | 41.9     |
| reference_Q_std         | 8.04     |
| reference_action_mean   | -0.242   |
| reference_action_std    | 0.961    |
| reference_actor_Q_mean  | 42.5     |
| reference_actor_Q_std   | 8.03     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.158    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 113      |
| rollout/episodes        | 1.65e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 93.3     |
| total/duration          | 4.8e+03  |
| total/episodes          | 1.65e+04 |
| total/epochs            | 1        |
| total/steps             | 1859998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -59.7    |
| train/loss_critic       | 1.96     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1861000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
Num timesteps: 1862000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 1863000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 1864000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 1865000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 1866000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 1867000
Best mean reward: 94.30 - Last mean reward per episode: 93.86
Num timesteps: 1868000
Best mean reward: 94.30 - Last mean reward per episode: 92.50
Num timesteps: 1869000
Best mean reward: 94.30 - Last mean reward per episode: 92.52
Num timesteps: 1870000
Best mean reward: 94.30 - Last mean reward per episode: 92.59
--------------------------------------
| reference_Q_mean        | 43.1     |
| reference_Q_std         | 8.31     |
| reference_action_mean   | -0.133   |
| reference_action_std    | 0.968    |
| reference_actor_Q_mean  | 43.8     |
| reference_actor_Q_std   | 8.21     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.66e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 92.6     |
| total/duration          | 4.83e+03 |
| total/episodes          | 1.66e+04 |
| total/epochs            | 1        |
| total/steps             | 1869998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -62.9    |
| train/loss_critic       | 2.03     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1871000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 1872000
Best mean reward: 94.30 - Last mean reward per episode: 92.03
Num timesteps: 1873000
Best mean reward: 94.30 - Last mean reward per episode: 92.04
Num timesteps: 1874000
Best mean reward: 94.30 - Last mean reward per episode: 91.98
Num timesteps: 1875000
Best mean reward: 94.30 - Last mean reward per episode: 91.94
Num timesteps: 1876000
Best mean reward: 94.30 - Last mean reward per episode: 91.91
Num timesteps: 1877000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 1878000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 1879000
Best mean reward: 94.30 - Last mean reward per episode: 94.04
Num timesteps: 1880000
Best mean reward: 94.30 - Last mean reward per episode: 93.96
--------------------------------------
| reference_Q_mean        | 44.6     |
| reference_Q_std         | 7.97     |
| reference_action_mean   | -0.433   |
| reference_action_std    | 0.889    |
| reference_actor_Q_mean  | 45.2     |
| reference_actor_Q_std   | 7.87     |
| rollout/Q_mean          | 61.3     |
| rollout/actions_mean    | 0.157    |
| rollout/actions_std     | 0.81     |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.67e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 94       |
| total/duration          | 4.86e+03 |
| total/episodes          | 1.67e+04 |
| total/epochs            | 1        |
| total/steps             | 1879998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -67.4    |
| train/loss_critic       | 1.56     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1881000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 1882000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 1883000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1884000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 1885000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 1886000
Best mean reward: 94.30 - Last mean reward per episode: 91.94
Num timesteps: 1887000
Best mean reward: 94.30 - Last mean reward per episode: 91.83
Num timesteps: 1888000
Best mean reward: 94.30 - Last mean reward per episode: 91.76
Num timesteps: 1889000
Best mean reward: 94.30 - Last mean reward per episode: 91.78
Num timesteps: 1890000
Best mean reward: 94.30 - Last mean reward per episode: 91.33
--------------------------------------
| reference_Q_mean        | 45.7     |
| reference_Q_std         | 8.01     |
| reference_action_mean   | -0.00316 |
| reference_action_std    | 0.988    |
| reference_actor_Q_mean  | 46.3     |
| reference_actor_Q_std   | 8        |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.811    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.68e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 91.3     |
| total/duration          | 4.89e+03 |
| total/episodes          | 1.68e+04 |
| total/epochs            | 1        |
| total/steps             | 1889998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.68     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1891000
Best mean reward: 94.30 - Last mean reward per episode: 91.57
Num timesteps: 1892000
Best mean reward: 94.30 - Last mean reward per episode: 91.50
Num timesteps: 1893000
Best mean reward: 94.30 - Last mean reward per episode: 91.08
Num timesteps: 1894000
Best mean reward: 94.30 - Last mean reward per episode: 90.87
Num timesteps: 1895000
Best mean reward: 94.30 - Last mean reward per episode: 90.90
Num timesteps: 1896000
Best mean reward: 94.30 - Last mean reward per episode: 90.73
Num timesteps: 1897000
Best mean reward: 94.30 - Last mean reward per episode: 92.24
Num timesteps: 1898000
Best mean reward: 94.30 - Last mean reward per episode: 92.36
Num timesteps: 1899000
Best mean reward: 94.30 - Last mean reward per episode: 92.81
Num timesteps: 1900000
Best mean reward: 94.30 - Last mean reward per episode: 92.81
--------------------------------------
| reference_Q_mean        | 44.8     |
| reference_Q_std         | 8.41     |
| reference_action_mean   | -0.31    |
| reference_action_std    | 0.932    |
| reference_actor_Q_mean  | 45.3     |
| reference_actor_Q_std   | 8.88     |
| rollout/Q_mean          | 61.4     |
| rollout/actions_mean    | 0.155    |
| rollout/actions_std     | 0.811    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.69e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 92.8     |
| total/duration          | 4.91e+03 |
| total/episodes          | 1.69e+04 |
| total/epochs            | 1        |
| total/steps             | 1899998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.93     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1901000
Best mean reward: 94.30 - Last mean reward per episode: 92.92
Num timesteps: 1902000
Best mean reward: 94.30 - Last mean reward per episode: 93.43
Num timesteps: 1903000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 1904000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 1905000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 1906000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 1907000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1908000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 1909000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1910000
Best mean reward: 94.30 - Last mean reward per episode: 92.19
--------------------------------------
| reference_Q_mean        | 45.2     |
| reference_Q_std         | 8.51     |
| reference_action_mean   | -0.229   |
| reference_action_std    | 0.957    |
| reference_actor_Q_mean  | 45.9     |
| reference_actor_Q_std   | 8.83     |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.71e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 92.2     |
| total/duration          | 4.94e+03 |
| total/episodes          | 1.71e+04 |
| total/epochs            | 1        |
| total/steps             | 1909998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 3.16     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1911000
Best mean reward: 94.30 - Last mean reward per episode: 92.28
Num timesteps: 1912000
Best mean reward: 94.30 - Last mean reward per episode: 92.23
Num timesteps: 1913000
Best mean reward: 94.30 - Last mean reward per episode: 92.19
Num timesteps: 1914000
Best mean reward: 94.30 - Last mean reward per episode: 92.22
Num timesteps: 1915000
Best mean reward: 94.30 - Last mean reward per episode: 92.15
Num timesteps: 1916000
Best mean reward: 94.30 - Last mean reward per episode: 92.23
Num timesteps: 1917000
Best mean reward: 94.30 - Last mean reward per episode: 92.27
Num timesteps: 1918000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 1919000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 1920000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
--------------------------------------
| reference_Q_mean        | 47.7     |
| reference_Q_std         | 8.7      |
| reference_action_mean   | -0.216   |
| reference_action_std    | 0.955    |
| reference_actor_Q_mean  | 49       |
| reference_actor_Q_std   | 8.83     |
| rollout/Q_mean          | 61.5     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.72e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 93.6     |
| total/duration          | 4.97e+03 |
| total/episodes          | 1.72e+04 |
| total/epochs            | 1        |
| total/steps             | 1919998  |
| total/steps_per_second  | 387      |
| train/loss_actor        | -71      |
| train/loss_critic       | 2.07     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1921000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 1922000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 1923000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 1924000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 1925000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 1926000
Best mean reward: 94.30 - Last mean reward per episode: 92.05
Num timesteps: 1927000
Best mean reward: 94.30 - Last mean reward per episode: 91.99
Num timesteps: 1928000
Best mean reward: 94.30 - Last mean reward per episode: 92.08
Num timesteps: 1929000
Best mean reward: 94.30 - Last mean reward per episode: 92.12
Num timesteps: 1930000
Best mean reward: 94.30 - Last mean reward per episode: 92.12
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 6.98     |
| reference_action_mean   | -0.22    |
| reference_action_std    | 0.952    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 7.02     |
| rollout/Q_mean          | 61.6     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.73e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 92.1     |
| total/duration          | 5e+03    |
| total/episodes          | 1.73e+04 |
| total/epochs            | 1        |
| total/steps             | 1929998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 2.46     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1931000
Best mean reward: 94.30 - Last mean reward per episode: 92.13
Num timesteps: 1932000
Best mean reward: 94.30 - Last mean reward per episode: 92.30
Num timesteps: 1933000
Best mean reward: 94.30 - Last mean reward per episode: 92.20
Num timesteps: 1934000
Best mean reward: 94.30 - Last mean reward per episode: 93.77
Num timesteps: 1935000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 1936000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 1937000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 1938000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 1939000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 1940000
Best mean reward: 94.30 - Last mean reward per episode: 93.71
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 6.13     |
| reference_action_mean   | -0.606   |
| reference_action_std    | 0.778    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 6.02     |
| rollout/Q_mean          | 61.6     |
| rollout/actions_mean    | 0.154    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.74e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.7     |
| total/duration          | 5.02e+03 |
| total/episodes          | 1.74e+04 |
| total/epochs            | 1        |
| total/steps             | 1939998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -69.8    |
| train/loss_critic       | 1.41     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1941000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 1942000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 1943000
Best mean reward: 94.30 - Last mean reward per episode: 93.71
Num timesteps: 1944000
Best mean reward: 94.30 - Last mean reward per episode: 92.24
Num timesteps: 1945000
Best mean reward: 94.30 - Last mean reward per episode: 92.04
Num timesteps: 1946000
Best mean reward: 94.30 - Last mean reward per episode: 91.83
Num timesteps: 1947000
Best mean reward: 94.30 - Last mean reward per episode: 91.92
Num timesteps: 1948000
Best mean reward: 94.30 - Last mean reward per episode: 92.00
Num timesteps: 1949000
Best mean reward: 94.30 - Last mean reward per episode: 91.83
Num timesteps: 1950000
Best mean reward: 94.30 - Last mean reward per episode: 91.82
--------------------------------------
| reference_Q_mean        | 51.3     |
| reference_Q_std         | 6.32     |
| reference_action_mean   | -0.749   |
| reference_action_std    | 0.646    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 6.45     |
| rollout/Q_mean          | 61.6     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.75e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 91.8     |
| total/duration          | 5.05e+03 |
| total/episodes          | 1.75e+04 |
| total/epochs            | 1        |
| total/steps             | 1949998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -69.2    |
| train/loss_critic       | 0.96     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1951000
Best mean reward: 94.30 - Last mean reward per episode: 91.66
Num timesteps: 1952000
Best mean reward: 94.30 - Last mean reward per episode: 91.82
Num timesteps: 1953000
Best mean reward: 94.30 - Last mean reward per episode: 91.84
Num timesteps: 1954000
Best mean reward: 94.30 - Last mean reward per episode: 91.92
Num timesteps: 1955000
Best mean reward: 94.30 - Last mean reward per episode: 91.76
Num timesteps: 1956000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 1957000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 1958000
Best mean reward: 94.30 - Last mean reward per episode: 93.09
Num timesteps: 1959000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 1960000
Best mean reward: 94.30 - Last mean reward per episode: 93.29
--------------------------------------
| reference_Q_mean        | 49.9     |
| reference_Q_std         | 6.83     |
| reference_action_mean   | 0.226    |
| reference_action_std    | 0.957    |
| reference_actor_Q_mean  | 50.5     |
| reference_actor_Q_std   | 6.6      |
| rollout/Q_mean          | 61.7     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.76e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.3     |
| total/duration          | 5.08e+03 |
| total/episodes          | 1.76e+04 |
| total/epochs            | 1        |
| total/steps             | 1959998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -69.3    |
| train/loss_critic       | 1.71     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1961000
Best mean reward: 94.30 - Last mean reward per episode: 93.07
Num timesteps: 1962000
Best mean reward: 94.30 - Last mean reward per episode: 92.82
Num timesteps: 1963000
Best mean reward: 94.30 - Last mean reward per episode: 90.97
Num timesteps: 1964000
Best mean reward: 94.30 - Last mean reward per episode: 90.65
Num timesteps: 1965000
Best mean reward: 94.30 - Last mean reward per episode: 90.44
Num timesteps: 1966000
Best mean reward: 94.30 - Last mean reward per episode: 90.24
Num timesteps: 1967000
Best mean reward: 94.30 - Last mean reward per episode: 90.16
Num timesteps: 1968000
Best mean reward: 94.30 - Last mean reward per episode: 90.04
Num timesteps: 1969000
Best mean reward: 94.30 - Last mean reward per episode: 89.90
Num timesteps: 1970000
Best mean reward: 94.30 - Last mean reward per episode: 88.25
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 6.57     |
| reference_action_mean   | 0.238    |
| reference_action_std    | 0.956    |
| reference_actor_Q_mean  | 51.4     |
| reference_actor_Q_std   | 6.6      |
| rollout/Q_mean          | 61.7     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.76e+04 |
| rollout/return          | 91       |
| rollout/return_history  | 88.3     |
| total/duration          | 5.1e+03  |
| total/episodes          | 1.76e+04 |
| total/epochs            | 1        |
| total/steps             | 1969998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -68.4    |
| train/loss_critic       | 2.95     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1971000
Best mean reward: 94.30 - Last mean reward per episode: 88.50
Num timesteps: 1972000
Best mean reward: 94.30 - Last mean reward per episode: 88.51
Num timesteps: 1973000
Best mean reward: 94.30 - Last mean reward per episode: 88.45
Num timesteps: 1974000
Best mean reward: 94.30 - Last mean reward per episode: 88.51
Num timesteps: 1975000
Best mean reward: 94.30 - Last mean reward per episode: 90.89
Num timesteps: 1976000
Best mean reward: 94.30 - Last mean reward per episode: 91.10
Num timesteps: 1977000
Best mean reward: 94.30 - Last mean reward per episode: 91.22
Num timesteps: 1978000
Best mean reward: 94.30 - Last mean reward per episode: 91.01
Num timesteps: 1979000
Best mean reward: 94.30 - Last mean reward per episode: 91.14
Num timesteps: 1980000
Best mean reward: 94.30 - Last mean reward per episode: 92.79
--------------------------------------
| reference_Q_mean        | 50.6     |
| reference_Q_std         | 5.85     |
| reference_action_mean   | 0.225    |
| reference_action_std    | 0.952    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 5.93     |
| rollout/Q_mean          | 61.7     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 112      |
| rollout/episodes        | 1.77e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 92.8     |
| total/duration          | 5.13e+03 |
| total/episodes          | 1.77e+04 |
| total/epochs            | 1        |
| total/steps             | 1979998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -67.8    |
| train/loss_critic       | 1.57     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1981000
Best mean reward: 94.30 - Last mean reward per episode: 92.84
Num timesteps: 1982000
Best mean reward: 94.30 - Last mean reward per episode: 92.87
Num timesteps: 1983000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 1984000
Best mean reward: 94.30 - Last mean reward per episode: 93.00
Num timesteps: 1985000
Best mean reward: 94.30 - Last mean reward per episode: 93.11
Num timesteps: 1986000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 1987000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 1988000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 1989000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 1990000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 5.53     |
| reference_action_mean   | -0.641   |
| reference_action_std    | 0.741    |
| reference_actor_Q_mean  | 50.7     |
| reference_actor_Q_std   | 5.59     |
| rollout/Q_mean          | 61.8     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.79e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.6     |
| total/duration          | 5.16e+03 |
| total/episodes          | 1.79e+04 |
| total/epochs            | 1        |
| total/steps             | 1989998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -67.9    |
| train/loss_critic       | 1.79     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 1991000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 1992000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
Num timesteps: 1993000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 1994000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 1995000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 1996000
Best mean reward: 94.30 - Last mean reward per episode: 93.17
Num timesteps: 1997000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 1998000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 1999000
Best mean reward: 94.30 - Last mean reward per episode: 93.77
Num timesteps: 2000000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 5.6      |
| reference_action_mean   | -0.263   |
| reference_action_std    | 0.934    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 5.68     |
| rollout/Q_mean          | 61.8     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.8e+04  |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.7     |
| total/duration          | 5.18e+03 |
| total/episodes          | 1.8e+04  |
| total/epochs            | 1        |
| total/steps             | 1999998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 0.788    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2001000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 2002000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
Num timesteps: 2003000
Best mean reward: 94.30 - Last mean reward per episode: 93.88
Num timesteps: 2004000
Best mean reward: 94.30 - Last mean reward per episode: 93.91
Num timesteps: 2005000
Best mean reward: 94.30 - Last mean reward per episode: 93.94
Num timesteps: 2006000
Best mean reward: 94.30 - Last mean reward per episode: 93.98
Num timesteps: 2007000
Best mean reward: 94.30 - Last mean reward per episode: 93.98
Num timesteps: 2008000
Best mean reward: 94.30 - Last mean reward per episode: 92.43
Num timesteps: 2009000
Best mean reward: 94.30 - Last mean reward per episode: 92.48
Num timesteps: 2010000
Best mean reward: 94.30 - Last mean reward per episode: 92.42
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 5.53     |
| reference_action_mean   | -0.292   |
| reference_action_std    | 0.941    |
| reference_actor_Q_mean  | 51.7     |
| reference_actor_Q_std   | 5.67     |
| rollout/Q_mean          | 61.9     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.81e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 92.4     |
| total/duration          | 5.21e+03 |
| total/episodes          | 1.81e+04 |
| total/epochs            | 1        |
| total/steps             | 2009998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -69      |
| train/loss_critic       | 0.628    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2011000
Best mean reward: 94.30 - Last mean reward per episode: 92.39
Num timesteps: 2012000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 2013000
Best mean reward: 94.30 - Last mean reward per episode: 92.03
Num timesteps: 2014000
Best mean reward: 94.30 - Last mean reward per episode: 91.97
Num timesteps: 2015000
Best mean reward: 94.30 - Last mean reward per episode: 92.06
Num timesteps: 2016000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 2017000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 2018000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 2019000
Best mean reward: 94.30 - Last mean reward per episode: 93.07
Num timesteps: 2020000
Best mean reward: 94.30 - Last mean reward per episode: 92.88
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 5.82     |
| reference_action_mean   | -0.197   |
| reference_action_std    | 0.97     |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 5.97     |
| rollout/Q_mean          | 61.9     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.82e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 92.9     |
| total/duration          | 5.24e+03 |
| total/episodes          | 1.82e+04 |
| total/epochs            | 1        |
| total/steps             | 2019998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.755    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2021000
Best mean reward: 94.30 - Last mean reward per episode: 93.14
Num timesteps: 2022000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 2023000
Best mean reward: 94.30 - Last mean reward per episode: 93.14
Num timesteps: 2024000
Best mean reward: 94.30 - Last mean reward per episode: 93.09
Num timesteps: 2025000
Best mean reward: 94.30 - Last mean reward per episode: 92.97
Num timesteps: 2026000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
Num timesteps: 2027000
Best mean reward: 94.30 - Last mean reward per episode: 93.29
Num timesteps: 2028000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 2029000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 2030000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 5.88     |
| reference_action_mean   | -0.219   |
| reference_action_std    | 0.973    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.01     |
| rollout/Q_mean          | 62       |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.83e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.4     |
| total/duration          | 5.26e+03 |
| total/episodes          | 1.83e+04 |
| total/epochs            | 1        |
| total/steps             | 2029998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -71.1    |
| train/loss_critic       | 1.55     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2031000
Best mean reward: 94.30 - Last mean reward per episode: 93.43
Num timesteps: 2032000
Best mean reward: 94.30 - Last mean reward per episode: 93.41
Num timesteps: 2033000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 2034000
Best mean reward: 94.30 - Last mean reward per episode: 93.09
Num timesteps: 2035000
Best mean reward: 94.30 - Last mean reward per episode: 93.06
Num timesteps: 2036000
Best mean reward: 94.30 - Last mean reward per episode: 93.07
Num timesteps: 2037000
Best mean reward: 94.30 - Last mean reward per episode: 92.99
Num timesteps: 2038000
Best mean reward: 94.30 - Last mean reward per episode: 93.22
Num timesteps: 2039000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 2040000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
--------------------------------------
| reference_Q_mean        | 53.2     |
| reference_Q_std         | 5.6      |
| reference_action_mean   | -0.245   |
| reference_action_std    | 0.959    |
| reference_actor_Q_mean  | 53.7     |
| reference_actor_Q_std   | 5.65     |
| rollout/Q_mean          | 62       |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.84e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 93.2     |
| total/duration          | 5.29e+03 |
| total/episodes          | 1.84e+04 |
| total/epochs            | 1        |
| total/steps             | 2039998  |
| total/steps_per_second  | 386      |
| train/loss_actor        | -71      |
| train/loss_critic       | 0.957    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2041000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 2042000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 2043000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 2044000
Best mean reward: 94.30 - Last mean reward per episode: 91.95
Num timesteps: 2045000
Best mean reward: 94.30 - Last mean reward per episode: 91.73
Num timesteps: 2046000
Best mean reward: 94.30 - Last mean reward per episode: 91.74
Num timesteps: 2047000
Best mean reward: 94.30 - Last mean reward per episode: 91.65
Num timesteps: 2048000
Best mean reward: 94.30 - Last mean reward per episode: 91.40
Num timesteps: 2049000
Best mean reward: 94.30 - Last mean reward per episode: 91.27
Num timesteps: 2050000
Best mean reward: 94.30 - Last mean reward per episode: 91.21
--------------------------------------
| reference_Q_mean        | 54.2     |
| reference_Q_std         | 4.66     |
| reference_action_mean   | -0.0446  |
| reference_action_std    | 0.973    |
| reference_actor_Q_mean  | 54.5     |
| reference_actor_Q_std   | 4.6      |
| rollout/Q_mean          | 62       |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.85e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 91.2     |
| total/duration          | 5.32e+03 |
| total/episodes          | 1.85e+04 |
| total/epochs            | 1        |
| total/steps             | 2049998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.737    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2051000
Best mean reward: 94.30 - Last mean reward per episode: 90.67
Num timesteps: 2052000
Best mean reward: 94.30 - Last mean reward per episode: 90.05
Num timesteps: 2053000
Best mean reward: 94.30 - Last mean reward per episode: 90.00
Num timesteps: 2054000
Best mean reward: 94.30 - Last mean reward per episode: 89.98
Num timesteps: 2055000
Best mean reward: 94.30 - Last mean reward per episode: 89.89
Num timesteps: 2056000
Best mean reward: 94.30 - Last mean reward per episode: 91.48
Num timesteps: 2057000
Best mean reward: 94.30 - Last mean reward per episode: 91.65
Num timesteps: 2058000
Best mean reward: 94.30 - Last mean reward per episode: 91.58
Num timesteps: 2059000
Best mean reward: 94.30 - Last mean reward per episode: 91.53
Num timesteps: 2060000
Best mean reward: 94.30 - Last mean reward per episode: 91.43
--------------------------------------
| reference_Q_mean        | 53.5     |
| reference_Q_std         | 4.94     |
| reference_action_mean   | -0.409   |
| reference_action_std    | 0.9      |
| reference_actor_Q_mean  | 53.8     |
| reference_actor_Q_std   | 5.19     |
| rollout/Q_mean          | 62.1     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.86e+04 |
| rollout/return          | 91.1     |
| rollout/return_history  | 91.4     |
| total/duration          | 5.35e+03 |
| total/episodes          | 1.86e+04 |
| total/epochs            | 1        |
| total/steps             | 2059998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.66     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2061000
Best mean reward: 94.30 - Last mean reward per episode: 91.75
Num timesteps: 2062000
Best mean reward: 94.30 - Last mean reward per episode: 92.91
Num timesteps: 2063000
Best mean reward: 94.30 - Last mean reward per episode: 92.82
Num timesteps: 2064000
Best mean reward: 94.30 - Last mean reward per episode: 92.88
Num timesteps: 2065000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 2066000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 2067000
Best mean reward: 94.30 - Last mean reward per episode: 93.15
Num timesteps: 2068000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 2069000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 2070000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 5.32     |
| reference_action_mean   | -0.566   |
| reference_action_std    | 0.809    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 5.48     |
| rollout/Q_mean          | 62.1     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.814    |
| rollout/episode_steps   | 111      |
| rollout/episodes        | 1.87e+04 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.7     |
| total/duration          | 5.38e+03 |
| total/episodes          | 1.87e+04 |
| total/epochs            | 1        |
| total/steps             | 2069998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 1.75     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2071000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 2072000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
Num timesteps: 2073000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 2074000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 2075000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 2076000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2077000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2078000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 2079000
Best mean reward: 94.30 - Last mean reward per episode: 93.88
Num timesteps: 2080000
Best mean reward: 94.30 - Last mean reward per episode: 93.91
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 6.11     |
| reference_action_mean   | -0.15    |
| reference_action_std    | 0.974    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 6.41     |
| rollout/Q_mean          | 62.2     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.815    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.88e+04 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.9     |
| total/duration          | 5.4e+03  |
| total/episodes          | 1.88e+04 |
| total/epochs            | 1        |
| total/steps             | 2079998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.551    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2081000
Best mean reward: 94.30 - Last mean reward per episode: 93.88
Num timesteps: 2082000
Best mean reward: 94.30 - Last mean reward per episode: 93.87
Num timesteps: 2083000
Best mean reward: 94.30 - Last mean reward per episode: 93.90
Num timesteps: 2084000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2085000
Best mean reward: 94.30 - Last mean reward per episode: 93.21
Num timesteps: 2086000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
Num timesteps: 2087000
Best mean reward: 94.30 - Last mean reward per episode: 93.21
Num timesteps: 2088000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 2089000
Best mean reward: 94.30 - Last mean reward per episode: 92.93
Num timesteps: 2090000
Best mean reward: 94.30 - Last mean reward per episode: 92.92
--------------------------------------
| reference_Q_mean        | 52.6     |
| reference_Q_std         | 6.81     |
| reference_action_mean   | -0.0749  |
| reference_action_std    | 0.988    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 6.95     |
| rollout/Q_mean          | 62.2     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.815    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.89e+04 |
| rollout/return          | 91.2     |
| rollout/return_history  | 92.9     |
| total/duration          | 5.43e+03 |
| total/episodes          | 1.89e+04 |
| total/epochs            | 1        |
| total/steps             | 2089998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.1    |
| train/loss_critic       | 0.624    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2091000
Best mean reward: 94.30 - Last mean reward per episode: 92.92
Num timesteps: 2092000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 2093000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 2094000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2095000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 2096000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 2097000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 2098000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 2099000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 2100000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
--------------------------------------
| reference_Q_mean        | 53.4     |
| reference_Q_std         | 7.16     |
| reference_action_mean   | 0.128    |
| reference_action_std    | 0.979    |
| reference_actor_Q_mean  | 53.9     |
| reference_actor_Q_std   | 6.98     |
| rollout/Q_mean          | 62.2     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.815    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.91e+04 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 5.46e+03 |
| total/episodes          | 1.91e+04 |
| total/epochs            | 1        |
| total/steps             | 2099998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.857    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2101000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 2102000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 2103000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 2104000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
Num timesteps: 2105000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 2106000
Best mean reward: 94.30 - Last mean reward per episode: 93.40
Num timesteps: 2107000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 2108000
Best mean reward: 94.30 - Last mean reward per episode: 91.95
Num timesteps: 2109000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 2110000
Best mean reward: 94.30 - Last mean reward per episode: 91.99
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 6.93     |
| reference_action_mean   | -0.573   |
| reference_action_std    | 0.797    |
| reference_actor_Q_mean  | 52.8     |
| reference_actor_Q_std   | 6.83     |
| rollout/Q_mean          | 62.3     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.92e+04 |
| rollout/return          | 91.2     |
| rollout/return_history  | 92       |
| total/duration          | 5.48e+03 |
| total/episodes          | 1.92e+04 |
| total/epochs            | 1        |
| total/steps             | 2109998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.9    |
| train/loss_critic       | 1.58     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2111000
Best mean reward: 94.30 - Last mean reward per episode: 91.73
Num timesteps: 2112000
Best mean reward: 94.30 - Last mean reward per episode: 91.70
Num timesteps: 2113000
Best mean reward: 94.30 - Last mean reward per episode: 91.71
Num timesteps: 2114000
Best mean reward: 94.30 - Last mean reward per episode: 91.66
Num timesteps: 2115000
Best mean reward: 94.30 - Last mean reward per episode: 91.77
Num timesteps: 2116000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 2117000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 2118000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 2119000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 2120000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 6.72     |
| reference_action_mean   | -0.0157  |
| reference_action_std    | 0.988    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.62     |
| rollout/Q_mean          | 62.3     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.93e+04 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 5.51e+03 |
| total/episodes          | 1.93e+04 |
| total/epochs            | 1        |
| total/steps             | 2119998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -71      |
| train/loss_critic       | 0.867    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2121000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 2122000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 2123000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2124000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 2125000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 2126000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 2127000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2128000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
Num timesteps: 2129000
Best mean reward: 94.30 - Last mean reward per episode: 93.55
Num timesteps: 2130000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
--------------------------------------
| reference_Q_mean        | 52.6     |
| reference_Q_std         | 6.21     |
| reference_action_mean   | -0.399   |
| reference_action_std    | 0.895    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.35     |
| rollout/Q_mean          | 62.4     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.94e+04 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.5     |
| total/duration          | 5.54e+03 |
| total/episodes          | 1.94e+04 |
| total/epochs            | 1        |
| total/steps             | 2129998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.769    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2131000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 2132000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 2133000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 2134000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 2135000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 2136000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 2137000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 2138000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2139000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 2140000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
--------------------------------------
| reference_Q_mean        | 52.6     |
| reference_Q_std         | 6.25     |
| reference_action_mean   | -0.352   |
| reference_action_std    | 0.914    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 6.29     |
| rollout/Q_mean          | 62.4     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 110      |
| rollout/episodes        | 1.95e+04 |
| rollout/return          | 91.2     |
| rollout/return_history  | 93.4     |
| total/duration          | 5.56e+03 |
| total/episodes          | 1.95e+04 |
| total/epochs            | 1        |
| total/steps             | 2139998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.64     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2141000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 2142000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2143000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2144000
Best mean reward: 94.30 - Last mean reward per episode: 93.55
Num timesteps: 2145000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 2146000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 2147000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 2148000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 2149000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 2150000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
--------------------------------------
| reference_Q_mean        | 52.2     |
| reference_Q_std         | 5.93     |
| reference_action_mean   | -0.41    |
| reference_action_std    | 0.89     |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 6.03     |
| rollout/Q_mean          | 62.4     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 1.96e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.7     |
| total/duration          | 5.59e+03 |
| total/episodes          | 1.96e+04 |
| total/epochs            | 1        |
| total/steps             | 2149998  |
| total/steps_per_second  | 385      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.507    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2151000
Best mean reward: 94.30 - Last mean reward per episode: 92.45
Num timesteps: 2152000
Best mean reward: 94.30 - Last mean reward per episode: 92.43
Num timesteps: 2153000
Best mean reward: 94.30 - Last mean reward per episode: 92.48
Num timesteps: 2154000
Best mean reward: 94.30 - Last mean reward per episode: 92.12
Num timesteps: 2155000
Best mean reward: 94.30 - Last mean reward per episode: 92.05
Num timesteps: 2156000
Best mean reward: 94.30 - Last mean reward per episode: 92.06
Num timesteps: 2157000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 2158000
Best mean reward: 94.30 - Last mean reward per episode: 92.10
Num timesteps: 2159000
Best mean reward: 94.30 - Last mean reward per episode: 92.05
Num timesteps: 2160000
Best mean reward: 94.30 - Last mean reward per episode: 93.31
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 5.76     |
| reference_action_mean   | -0.431   |
| reference_action_std    | 0.881    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 5.87     |
| rollout/Q_mean          | 62.5     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.816    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 1.98e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.3     |
| total/duration          | 5.62e+03 |
| total/episodes          | 1.98e+04 |
| total/epochs            | 1        |
| total/steps             | 2159998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -71      |
| train/loss_critic       | 0.549    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2161000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 2162000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
Num timesteps: 2163000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 2164000
Best mean reward: 94.30 - Last mean reward per episode: 93.79
Num timesteps: 2165000
Best mean reward: 94.30 - Last mean reward per episode: 93.79
Num timesteps: 2166000
Best mean reward: 94.30 - Last mean reward per episode: 93.84
Num timesteps: 2167000
Best mean reward: 94.30 - Last mean reward per episode: 93.86
Num timesteps: 2168000
Best mean reward: 94.30 - Last mean reward per episode: 93.92
Num timesteps: 2169000
Best mean reward: 94.30 - Last mean reward per episode: 93.93
Num timesteps: 2170000
Best mean reward: 94.30 - Last mean reward per episode: 92.46
--------------------------------------
| reference_Q_mean        | 52.8     |
| reference_Q_std         | 5.35     |
| reference_action_mean   | -0.324   |
| reference_action_std    | 0.913    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 5.49     |
| rollout/Q_mean          | 62.5     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 1.99e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 92.5     |
| total/duration          | 5.65e+03 |
| total/episodes          | 1.99e+04 |
| total/epochs            | 1        |
| total/steps             | 2169998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.8    |
| train/loss_critic       | 0.452    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2171000
Best mean reward: 94.30 - Last mean reward per episode: 92.48
Num timesteps: 2172000
Best mean reward: 94.30 - Last mean reward per episode: 92.01
Num timesteps: 2173000
Best mean reward: 94.30 - Last mean reward per episode: 91.94
Num timesteps: 2174000
Best mean reward: 94.30 - Last mean reward per episode: 91.89
Num timesteps: 2175000
Best mean reward: 94.30 - Last mean reward per episode: 91.72
Num timesteps: 2176000
Best mean reward: 94.30 - Last mean reward per episode: 91.68
Num timesteps: 2177000
Best mean reward: 94.30 - Last mean reward per episode: 91.22
Num timesteps: 2178000
Best mean reward: 94.30 - Last mean reward per episode: 91.17
Num timesteps: 2179000
Best mean reward: 94.30 - Last mean reward per episode: 91.18
Num timesteps: 2180000
Best mean reward: 94.30 - Last mean reward per episode: 91.14
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 5.76     |
| reference_action_mean   | -0.648   |
| reference_action_std    | 0.728    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 5.71     |
| rollout/Q_mean          | 62.6     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 2e+04    |
| rollout/return          | 91.3     |
| rollout/return_history  | 91.1     |
| total/duration          | 5.67e+03 |
| total/episodes          | 2e+04    |
| total/epochs            | 1        |
| total/steps             | 2179998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.517    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2181000
Best mean reward: 94.30 - Last mean reward per episode: 92.54
Num timesteps: 2182000
Best mean reward: 94.30 - Last mean reward per episode: 92.99
Num timesteps: 2183000
Best mean reward: 94.30 - Last mean reward per episode: 93.07
Num timesteps: 2184000
Best mean reward: 94.30 - Last mean reward per episode: 93.28
Num timesteps: 2185000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 2186000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 2187000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 2188000
Best mean reward: 94.30 - Last mean reward per episode: 93.71
Num timesteps: 2189000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 2190000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 6.1      |
| reference_action_mean   | -0.556   |
| reference_action_std    | 0.811    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 6.1      |
| rollout/Q_mean          | 62.6     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 2.01e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.8     |
| total/duration          | 5.7e+03  |
| total/episodes          | 2.01e+04 |
| total/epochs            | 1        |
| total/steps             | 2189998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.7    |
| train/loss_critic       | 0.495    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2191000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 2192000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 2193000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2194000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 2195000
Best mean reward: 94.30 - Last mean reward per episode: 93.61
Num timesteps: 2196000
Best mean reward: 94.30 - Last mean reward per episode: 93.67
Num timesteps: 2197000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 2198000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 2199000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 2200000
Best mean reward: 94.30 - Last mean reward per episode: 93.71
--------------------------------------
| reference_Q_mean        | 50.7     |
| reference_Q_std         | 6.21     |
| reference_action_mean   | -0.736   |
| reference_action_std    | 0.636    |
| reference_actor_Q_mean  | 51.3     |
| reference_actor_Q_std   | 6.17     |
| rollout/Q_mean          | 62.6     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 2.02e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.7     |
| total/duration          | 5.73e+03 |
| total/episodes          | 2.02e+04 |
| total/epochs            | 1        |
| total/steps             | 2199998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.462    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2201000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 2202000
Best mean reward: 94.30 - Last mean reward per episode: 93.76
Num timesteps: 2203000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 2204000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 2205000
Best mean reward: 94.30 - Last mean reward per episode: 93.68
Num timesteps: 2206000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 2207000
Best mean reward: 94.30 - Last mean reward per episode: 93.84
Num timesteps: 2208000
Best mean reward: 94.30 - Last mean reward per episode: 93.76
Num timesteps: 2209000
Best mean reward: 94.30 - Last mean reward per episode: 93.77
Num timesteps: 2210000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 6.16     |
| reference_action_mean   | -0.573   |
| reference_action_std    | 0.802    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 6.1      |
| rollout/Q_mean          | 62.7     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 2.03e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.8     |
| total/duration          | 5.76e+03 |
| total/episodes          | 2.03e+04 |
| total/epochs            | 1        |
| total/steps             | 2209998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.54     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2211000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 2212000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 2213000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 2214000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 2215000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 2216000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 2217000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
Num timesteps: 2218000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 2219000
Best mean reward: 94.30 - Last mean reward per episode: 93.43
Num timesteps: 2220000
Best mean reward: 94.30 - Last mean reward per episode: 93.38
--------------------------------------
| reference_Q_mean        | 50.1     |
| reference_Q_std         | 6.02     |
| reference_action_mean   | -0.423   |
| reference_action_std    | 0.902    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 5.96     |
| rollout/Q_mean          | 62.7     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 2.04e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.4     |
| total/duration          | 5.78e+03 |
| total/episodes          | 2.04e+04 |
| total/epochs            | 1        |
| total/steps             | 2219998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.664    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2221000
Best mean reward: 94.30 - Last mean reward per episode: 93.34
Num timesteps: 2222000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 2223000
Best mean reward: 94.30 - Last mean reward per episode: 93.91
Num timesteps: 2224000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 2225000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
Num timesteps: 2226000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 2227000
Best mean reward: 94.30 - Last mean reward per episode: 93.77
Num timesteps: 2228000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 2229000
Best mean reward: 94.30 - Last mean reward per episode: 93.76
Num timesteps: 2230000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
--------------------------------------
| reference_Q_mean        | 50.4     |
| reference_Q_std         | 5.61     |
| reference_action_mean   | -0.539   |
| reference_action_std    | 0.828    |
| reference_actor_Q_mean  | 50.9     |
| reference_actor_Q_std   | 5.64     |
| rollout/Q_mean          | 62.7     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 109      |
| rollout/episodes        | 2.05e+04 |
| rollout/return          | 91.3     |
| rollout/return_history  | 93.9     |
| total/duration          | 5.81e+03 |
| total/episodes          | 2.05e+04 |
| total/epochs            | 1        |
| total/steps             | 2229998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 0.496    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2231000
Best mean reward: 94.30 - Last mean reward per episode: 93.80
Num timesteps: 2232000
Best mean reward: 94.30 - Last mean reward per episode: 93.74
Num timesteps: 2233000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
Num timesteps: 2234000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 2235000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 2236000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 2237000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2238000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 2239000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2240000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
--------------------------------------
| reference_Q_mean        | 51.1     |
| reference_Q_std         | 5.55     |
| reference_action_mean   | -0.462   |
| reference_action_std    | 0.873    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 5.71     |
| rollout/Q_mean          | 62.8     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.07e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.7     |
| total/duration          | 5.84e+03 |
| total/episodes          | 2.07e+04 |
| total/epochs            | 1        |
| total/steps             | 2239998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.6    |
| train/loss_critic       | 0.465    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2241000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 2242000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 2243000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 2244000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 2245000
Best mean reward: 94.30 - Last mean reward per episode: 93.65
Num timesteps: 2246000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 2247000
Best mean reward: 94.30 - Last mean reward per episode: 93.63
Num timesteps: 2248000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 2249000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 2250000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 5.68     |
| reference_action_mean   | -0.536   |
| reference_action_std    | 0.805    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 5.88     |
| rollout/Q_mean          | 62.8     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.08e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.7     |
| total/duration          | 5.86e+03 |
| total/episodes          | 2.08e+04 |
| total/epochs            | 1        |
| total/steps             | 2249998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.584    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2251000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
Num timesteps: 2252000
Best mean reward: 94.30 - Last mean reward per episode: 93.55
Num timesteps: 2253000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 2254000
Best mean reward: 94.30 - Last mean reward per episode: 93.45
Num timesteps: 2255000
Best mean reward: 94.30 - Last mean reward per episode: 93.47
Num timesteps: 2256000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
Num timesteps: 2257000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 2258000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 2259000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 2260000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 5.72     |
| reference_action_mean   | -0.308   |
| reference_action_std    | 0.924    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 5.91     |
| rollout/Q_mean          | 62.8     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.09e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.6     |
| total/duration          | 5.89e+03 |
| total/episodes          | 2.09e+04 |
| total/epochs            | 1        |
| total/steps             | 2259998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.5    |
| train/loss_critic       | 1.16     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2261000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 2262000
Best mean reward: 94.30 - Last mean reward per episode: 93.75
Num timesteps: 2263000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
Num timesteps: 2264000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 2265000
Best mean reward: 94.30 - Last mean reward per episode: 93.72
Num timesteps: 2266000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 2267000
Best mean reward: 94.30 - Last mean reward per episode: 93.21
Num timesteps: 2268000
Best mean reward: 94.30 - Last mean reward per episode: 93.07
Num timesteps: 2269000
Best mean reward: 94.30 - Last mean reward per episode: 93.04
Num timesteps: 2270000
Best mean reward: 94.30 - Last mean reward per episode: 93.01
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 5.53     |
| reference_action_mean   | -0.0748  |
| reference_action_std    | 0.978    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 5.63     |
| rollout/Q_mean          | 62.9     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.1e+04  |
| rollout/return          | 91.4     |
| rollout/return_history  | 93       |
| total/duration          | 5.92e+03 |
| total/episodes          | 2.1e+04  |
| total/epochs            | 1        |
| total/steps             | 2269998  |
| total/steps_per_second  | 384      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 1.04     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2271000
Best mean reward: 94.30 - Last mean reward per episode: 93.07
Num timesteps: 2272000
Best mean reward: 94.30 - Last mean reward per episode: 92.91
Num timesteps: 2273000
Best mean reward: 94.30 - Last mean reward per episode: 92.88
Num timesteps: 2274000
Best mean reward: 94.30 - Last mean reward per episode: 92.96
Num timesteps: 2275000
Best mean reward: 94.30 - Last mean reward per episode: 92.94
Num timesteps: 2276000
Best mean reward: 94.30 - Last mean reward per episode: 92.96
Num timesteps: 2277000
Best mean reward: 94.30 - Last mean reward per episode: 91.62
Num timesteps: 2278000
Best mean reward: 94.30 - Last mean reward per episode: 91.93
Num timesteps: 2279000
Best mean reward: 94.30 - Last mean reward per episode: 91.87
Num timesteps: 2280000
Best mean reward: 94.30 - Last mean reward per episode: 91.85
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 5.2      |
| reference_action_mean   | -0.104   |
| reference_action_std    | 0.979    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 5.28     |
| rollout/Q_mean          | 62.9     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.11e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 91.8     |
| total/duration          | 5.95e+03 |
| total/episodes          | 2.11e+04 |
| total/epochs            | 1        |
| total/steps             | 2279998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 0.938    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2281000
Best mean reward: 94.30 - Last mean reward per episode: 90.43
Num timesteps: 2282000
Best mean reward: 94.30 - Last mean reward per episode: 88.93
Num timesteps: 2283000
Best mean reward: 94.30 - Last mean reward per episode: 88.86
Num timesteps: 2284000
Best mean reward: 94.30 - Last mean reward per episode: 88.94
Num timesteps: 2285000
Best mean reward: 94.30 - Last mean reward per episode: 89.09
Num timesteps: 2286000
Best mean reward: 94.30 - Last mean reward per episode: 89.06
Num timesteps: 2287000
Best mean reward: 94.30 - Last mean reward per episode: 88.84
Num timesteps: 2288000
Best mean reward: 94.30 - Last mean reward per episode: 88.85
Num timesteps: 2289000
Best mean reward: 94.30 - Last mean reward per episode: 90.42
Num timesteps: 2290000
Best mean reward: 94.30 - Last mean reward per episode: 89.99
--------------------------------------
| reference_Q_mean        | 51.6     |
| reference_Q_std         | 5.47     |
| reference_action_mean   | -0.455   |
| reference_action_std    | 0.88     |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 5.64     |
| rollout/Q_mean          | 62.9     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.12e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 90       |
| total/duration          | 5.97e+03 |
| total/episodes          | 2.12e+04 |
| total/epochs            | 1        |
| total/steps             | 2289998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -69.7    |
| train/loss_critic       | 1.48     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2291000
Best mean reward: 94.30 - Last mean reward per episode: 89.99
Num timesteps: 2292000
Best mean reward: 94.30 - Last mean reward per episode: 92.90
Num timesteps: 2293000
Best mean reward: 94.30 - Last mean reward per episode: 92.95
Num timesteps: 2294000
Best mean reward: 94.30 - Last mean reward per episode: 92.98
Num timesteps: 2295000
Best mean reward: 94.30 - Last mean reward per episode: 92.98
Num timesteps: 2296000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
Num timesteps: 2297000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 2298000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 2299000
Best mean reward: 94.30 - Last mean reward per episode: 93.14
Num timesteps: 2300000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 5.59     |
| reference_action_mean   | -0.718   |
| reference_action_std    | 0.66     |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 5.71     |
| rollout/Q_mean          | 62.9     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.13e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.5     |
| total/duration          | 6e+03    |
| total/episodes          | 2.13e+04 |
| total/epochs            | 1        |
| total/steps             | 2299998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -69.5    |
| train/loss_critic       | 0.869    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2301000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 2302000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 2303000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 2304000
Best mean reward: 94.30 - Last mean reward per episode: 93.53
Num timesteps: 2305000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2306000
Best mean reward: 94.30 - Last mean reward per episode: 93.58
Num timesteps: 2307000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 2308000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 2309000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 2310000
Best mean reward: 94.30 - Last mean reward per episode: 93.76
--------------------------------------
| reference_Q_mean        | 52.4     |
| reference_Q_std         | 5.52     |
| reference_action_mean   | -0.487   |
| reference_action_std    | 0.831    |
| reference_actor_Q_mean  | 52.9     |
| reference_actor_Q_std   | 5.67     |
| rollout/Q_mean          | 63       |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.14e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.8     |
| total/duration          | 6.03e+03 |
| total/episodes          | 2.14e+04 |
| total/epochs            | 1        |
| total/steps             | 2309998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 1.16     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2311000
Best mean reward: 94.30 - Last mean reward per episode: 93.70
Num timesteps: 2312000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 2313000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 2314000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 2315000
Best mean reward: 94.30 - Last mean reward per episode: 93.40
Num timesteps: 2316000
Best mean reward: 94.30 - Last mean reward per episode: 93.20
Num timesteps: 2317000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 2318000
Best mean reward: 94.30 - Last mean reward per episode: 92.98
Num timesteps: 2319000
Best mean reward: 94.30 - Last mean reward per episode: 92.87
Num timesteps: 2320000
Best mean reward: 94.30 - Last mean reward per episode: 92.82
--------------------------------------
| reference_Q_mean        | 53.5     |
| reference_Q_std         | 5.57     |
| reference_action_mean   | -0.00774 |
| reference_action_std    | 0.968    |
| reference_actor_Q_mean  | 54.3     |
| reference_actor_Q_std   | 5.64     |
| rollout/Q_mean          | 63       |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.15e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 92.8     |
| total/duration          | 6.06e+03 |
| total/episodes          | 2.15e+04 |
| total/epochs            | 1        |
| total/steps             | 2319998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -70.3    |
| train/loss_critic       | 0.903    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2321000
Best mean reward: 94.30 - Last mean reward per episode: 92.65
Num timesteps: 2322000
Best mean reward: 94.30 - Last mean reward per episode: 92.84
Num timesteps: 2323000
Best mean reward: 94.30 - Last mean reward per episode: 92.55
Num timesteps: 2324000
Best mean reward: 94.30 - Last mean reward per episode: 92.36
Num timesteps: 2325000
Best mean reward: 94.30 - Last mean reward per episode: 92.24
Num timesteps: 2326000
Best mean reward: 94.30 - Last mean reward per episode: 92.37
Num timesteps: 2327000
Best mean reward: 94.30 - Last mean reward per episode: 92.33
Num timesteps: 2328000
Best mean reward: 94.30 - Last mean reward per episode: 92.44
Num timesteps: 2329000
Best mean reward: 94.30 - Last mean reward per episode: 92.41
Num timesteps: 2330000
Best mean reward: 94.30 - Last mean reward per episode: 92.39
--------------------------------------
| reference_Q_mean        | 54.5     |
| reference_Q_std         | 5.69     |
| reference_action_mean   | -0.0419  |
| reference_action_std    | 0.967    |
| reference_actor_Q_mean  | 55       |
| reference_actor_Q_std   | 5.73     |
| rollout/Q_mean          | 63       |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.817    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.16e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 92.4     |
| total/duration          | 6.09e+03 |
| total/episodes          | 2.16e+04 |
| total/epochs            | 1        |
| total/steps             | 2329998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -70.4    |
| train/loss_critic       | 0.99     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2331000
Best mean reward: 94.30 - Last mean reward per episode: 92.59
Num timesteps: 2332000
Best mean reward: 94.30 - Last mean reward per episode: 92.64
Num timesteps: 2333000
Best mean reward: 94.30 - Last mean reward per episode: 92.74
Num timesteps: 2334000
Best mean reward: 94.30 - Last mean reward per episode: 92.92
Num timesteps: 2335000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
Num timesteps: 2336000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
Num timesteps: 2337000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
Num timesteps: 2338000
Best mean reward: 94.30 - Last mean reward per episode: 93.35
Num timesteps: 2339000
Best mean reward: 94.30 - Last mean reward per episode: 93.39
Num timesteps: 2340000
Best mean reward: 94.30 - Last mean reward per episode: 93.37
--------------------------------------
| reference_Q_mean        | 53.7     |
| reference_Q_std         | 5.74     |
| reference_action_mean   | 0.357    |
| reference_action_std    | 0.888    |
| reference_actor_Q_mean  | 54.3     |
| reference_actor_Q_std   | 5.74     |
| rollout/Q_mean          | 63.1     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.17e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.4     |
| total/duration          | 6.11e+03 |
| total/episodes          | 2.17e+04 |
| total/epochs            | 1        |
| total/steps             | 2339998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -71.3    |
| train/loss_critic       | 0.844    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2341000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 2342000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 2343000
Best mean reward: 94.30 - Last mean reward per episode: 93.30
Num timesteps: 2344000
Best mean reward: 94.30 - Last mean reward per episode: 93.36
Num timesteps: 2345000
Best mean reward: 94.30 - Last mean reward per episode: 93.32
Num timesteps: 2346000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 2347000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
Num timesteps: 2348000
Best mean reward: 94.30 - Last mean reward per episode: 92.95
Num timesteps: 2349000
Best mean reward: 94.30 - Last mean reward per episode: 92.84
Num timesteps: 2350000
Best mean reward: 94.30 - Last mean reward per episode: 92.71
--------------------------------------
| reference_Q_mean        | 52.6     |
| reference_Q_std         | 5.9      |
| reference_action_mean   | 0.169    |
| reference_action_std    | 0.976    |
| reference_actor_Q_mean  | 53.1     |
| reference_actor_Q_std   | 5.82     |
| rollout/Q_mean          | 63.1     |
| rollout/actions_mean    | 0.153    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.18e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 92.7     |
| total/duration          | 6.14e+03 |
| total/episodes          | 2.18e+04 |
| total/epochs            | 1        |
| total/steps             | 2349998  |
| total/steps_per_second  | 383      |
| train/loss_actor        | -71.2    |
| train/loss_critic       | 0.857    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2351000
Best mean reward: 94.30 - Last mean reward per episode: 92.66
Num timesteps: 2352000
Best mean reward: 94.30 - Last mean reward per episode: 92.88
Num timesteps: 2353000
Best mean reward: 94.30 - Last mean reward per episode: 92.77
Num timesteps: 2354000
Best mean reward: 94.30 - Last mean reward per episode: 92.83
Num timesteps: 2355000
Best mean reward: 94.30 - Last mean reward per episode: 92.96
Num timesteps: 2356000
Best mean reward: 94.30 - Last mean reward per episode: 92.91
Num timesteps: 2357000
Best mean reward: 94.30 - Last mean reward per episode: 93.05
Num timesteps: 2358000
Best mean reward: 94.30 - Last mean reward per episode: 92.96
Num timesteps: 2359000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 2360000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
--------------------------------------
| reference_Q_mean        | 52.7     |
| reference_Q_std         | 6.23     |
| reference_action_mean   | -0.146   |
| reference_action_std    | 0.972    |
| reference_actor_Q_mean  | 53.2     |
| reference_actor_Q_std   | 6.2      |
| rollout/Q_mean          | 63.2     |
| rollout/actions_mean    | 0.152    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.19e+04 |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.3     |
| total/duration          | 6.17e+03 |
| total/episodes          | 2.19e+04 |
| total/epochs            | 1        |
| total/steps             | 2359998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.7    |
| train/loss_critic       | 0.711    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2361000
Best mean reward: 94.30 - Last mean reward per episode: 93.22
Num timesteps: 2362000
Best mean reward: 94.30 - Last mean reward per episode: 93.27
Num timesteps: 2363000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
Num timesteps: 2364000
Best mean reward: 94.30 - Last mean reward per episode: 93.18
Num timesteps: 2365000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 2366000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
Num timesteps: 2367000
Best mean reward: 94.30 - Last mean reward per episode: 93.34
Num timesteps: 2368000
Best mean reward: 94.30 - Last mean reward per episode: 93.23
Num timesteps: 2369000
Best mean reward: 94.30 - Last mean reward per episode: 93.21
Num timesteps: 2370000
Best mean reward: 94.30 - Last mean reward per episode: 93.25
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 6.58     |
| reference_action_mean   | -0.0831  |
| reference_action_std    | 0.973    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 6.77     |
| rollout/Q_mean          | 63.2     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.2e+04  |
| rollout/return          | 91.4     |
| rollout/return_history  | 93.2     |
| total/duration          | 6.2e+03  |
| total/episodes          | 2.2e+04  |
| total/epochs            | 1        |
| total/steps             | 2369998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -72.7    |
| train/loss_critic       | 0.984    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2371000
Best mean reward: 94.30 - Last mean reward per episode: 93.26
Num timesteps: 2372000
Best mean reward: 94.30 - Last mean reward per episode: 93.06
Num timesteps: 2373000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
Num timesteps: 2374000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
Num timesteps: 2375000
Best mean reward: 94.30 - Last mean reward per episode: 93.01
Num timesteps: 2376000
Best mean reward: 94.30 - Last mean reward per episode: 93.05
Num timesteps: 2377000
Best mean reward: 94.30 - Last mean reward per episode: 93.02
Num timesteps: 2378000
Best mean reward: 94.30 - Last mean reward per episode: 92.91
Num timesteps: 2379000
Best mean reward: 94.30 - Last mean reward per episode: 92.81
Num timesteps: 2380000
Best mean reward: 94.30 - Last mean reward per episode: 92.69
--------------------------------------
| reference_Q_mean        | 51.8     |
| reference_Q_std         | 7.13     |
| reference_action_mean   | 0.25     |
| reference_action_std    | 0.948    |
| reference_actor_Q_mean  | 52       |
| reference_actor_Q_std   | 7.22     |
| rollout/Q_mean          | 63.3     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.21e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 92.7     |
| total/duration          | 6.23e+03 |
| total/episodes          | 2.21e+04 |
| total/epochs            | 1        |
| total/steps             | 2379998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -72.6    |
| train/loss_critic       | 0.632    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2381000
Best mean reward: 94.30 - Last mean reward per episode: 92.78
Num timesteps: 2382000
Best mean reward: 94.30 - Last mean reward per episode: 92.66
Num timesteps: 2383000
Best mean reward: 94.30 - Last mean reward per episode: 92.43
Num timesteps: 2384000
Best mean reward: 94.30 - Last mean reward per episode: 92.37
Num timesteps: 2385000
Best mean reward: 94.30 - Last mean reward per episode: 92.47
Num timesteps: 2386000
Best mean reward: 94.30 - Last mean reward per episode: 92.58
Num timesteps: 2387000
Best mean reward: 94.30 - Last mean reward per episode: 92.79
Num timesteps: 2388000
Best mean reward: 94.30 - Last mean reward per episode: 92.80
Num timesteps: 2389000
Best mean reward: 94.30 - Last mean reward per episode: 92.96
Num timesteps: 2390000
Best mean reward: 94.30 - Last mean reward per episode: 93.13
--------------------------------------
| reference_Q_mean        | 52.5     |
| reference_Q_std         | 7.34     |
| reference_action_mean   | -0.623   |
| reference_action_std    | 0.749    |
| reference_actor_Q_mean  | 52.6     |
| reference_actor_Q_std   | 7.4      |
| rollout/Q_mean          | 63.3     |
| rollout/actions_mean    | 0.151    |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.22e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.1     |
| total/duration          | 6.26e+03 |
| total/episodes          | 2.22e+04 |
| total/epochs            | 1        |
| total/steps             | 2389998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -72.7    |
| train/loss_critic       | 0.477    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2391000
Best mean reward: 94.30 - Last mean reward per episode: 93.12
Num timesteps: 2392000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2393000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 2394000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 2395000
Best mean reward: 94.30 - Last mean reward per episode: 93.10
Num timesteps: 2396000
Best mean reward: 94.30 - Last mean reward per episode: 92.71
Num timesteps: 2397000
Best mean reward: 94.30 - Last mean reward per episode: 92.52
Num timesteps: 2398000
Best mean reward: 94.30 - Last mean reward per episode: 92.34
Num timesteps: 2399000
Best mean reward: 94.30 - Last mean reward per episode: 92.36
Num timesteps: 2400000
Best mean reward: 94.30 - Last mean reward per episode: 92.36
--------------------------------------
| reference_Q_mean        | 53.2     |
| reference_Q_std         | 6.47     |
| reference_action_mean   | -0.868   |
| reference_action_std    | 0.487    |
| reference_actor_Q_mean  | 53.5     |
| reference_actor_Q_std   | 6.52     |
| rollout/Q_mean          | 63.3     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 108      |
| rollout/episodes        | 2.23e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 92.4     |
| total/duration          | 6.28e+03 |
| total/episodes          | 2.23e+04 |
| total/epochs            | 1        |
| total/steps             | 2399998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -71.3    |
| train/loss_critic       | 0.599    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2401000
Best mean reward: 94.30 - Last mean reward per episode: 92.22
Num timesteps: 2402000
Best mean reward: 94.30 - Last mean reward per episode: 92.36
Num timesteps: 2403000
Best mean reward: 94.30 - Last mean reward per episode: 92.36
Num timesteps: 2404000
Best mean reward: 94.30 - Last mean reward per episode: 92.44
Num timesteps: 2405000
Best mean reward: 94.30 - Last mean reward per episode: 92.95
Num timesteps: 2406000
Best mean reward: 94.30 - Last mean reward per episode: 93.42
Num timesteps: 2407000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 2408000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 2409000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 2410000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
--------------------------------------
| reference_Q_mean        | 52.1     |
| reference_Q_std         | 6.41     |
| reference_action_mean   | -0.218   |
| reference_action_std    | 0.974    |
| reference_actor_Q_mean  | 52.5     |
| reference_actor_Q_std   | 6.37     |
| rollout/Q_mean          | 63.4     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.24e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.6     |
| total/duration          | 6.31e+03 |
| total/episodes          | 2.24e+04 |
| total/epochs            | 1        |
| total/steps             | 2409998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -70.2    |
| train/loss_critic       | 0.586    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2411000
Best mean reward: 94.30 - Last mean reward per episode: 93.41
Num timesteps: 2412000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 2413000
Best mean reward: 94.30 - Last mean reward per episode: 93.59
Num timesteps: 2414000
Best mean reward: 94.30 - Last mean reward per episode: 93.64
Num timesteps: 2415000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 2416000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2417000
Best mean reward: 94.30 - Last mean reward per episode: 93.62
Num timesteps: 2418000
Best mean reward: 94.30 - Last mean reward per episode: 93.57
Num timesteps: 2419000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2420000
Best mean reward: 94.30 - Last mean reward per episode: 93.66
--------------------------------------
| reference_Q_mean        | 52       |
| reference_Q_std         | 5.92     |
| reference_action_mean   | -0.222   |
| reference_action_std    | 0.971    |
| reference_actor_Q_mean  | 52.4     |
| reference_actor_Q_std   | 5.9      |
| rollout/Q_mean          | 63.4     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.26e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.7     |
| total/duration          | 6.34e+03 |
| total/episodes          | 2.26e+04 |
| total/epochs            | 1        |
| total/steps             | 2419998  |
| total/steps_per_second  | 382      |
| train/loss_actor        | -70      |
| train/loss_critic       | 0.595    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2421000
Best mean reward: 94.30 - Last mean reward per episode: 93.60
Num timesteps: 2422000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2423000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 2424000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 2425000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 2426000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 2427000
Best mean reward: 94.30 - Last mean reward per episode: 93.46
Num timesteps: 2428000
Best mean reward: 94.30 - Last mean reward per episode: 93.50
Num timesteps: 2429000
Best mean reward: 94.30 - Last mean reward per episode: 93.49
Num timesteps: 2430000
Best mean reward: 94.30 - Last mean reward per episode: 93.38
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 5.45     |
| reference_action_mean   | -0.247   |
| reference_action_std    | 0.964    |
| reference_actor_Q_mean  | 52.3     |
| reference_actor_Q_std   | 5.55     |
| rollout/Q_mean          | 63.4     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.27e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.4     |
| total/duration          | 6.37e+03 |
| total/episodes          | 2.27e+04 |
| total/epochs            | 1        |
| total/steps             | 2429998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.416    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2431000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 2432000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 2433000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 2434000
Best mean reward: 94.30 - Last mean reward per episode: 93.31
Num timesteps: 2435000
Best mean reward: 94.30 - Last mean reward per episode: 93.33
Num timesteps: 2436000
Best mean reward: 94.30 - Last mean reward per episode: 93.34
Num timesteps: 2437000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 2438000
Best mean reward: 94.30 - Last mean reward per episode: 93.48
Num timesteps: 2439000
Best mean reward: 94.30 - Last mean reward per episode: 93.56
Num timesteps: 2440000
Best mean reward: 94.30 - Last mean reward per episode: 93.85
--------------------------------------
| reference_Q_mean        | 51.9     |
| reference_Q_std         | 5.2      |
| reference_action_mean   | -0.168   |
| reference_action_std    | 0.981    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 5.34     |
| rollout/Q_mean          | 63.4     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.819    |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.28e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.9     |
| total/duration          | 6.4e+03  |
| total/episodes          | 2.28e+04 |
| total/epochs            | 1        |
| total/steps             | 2439998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -69.6    |
| train/loss_critic       | 0.358    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2441000
Best mean reward: 94.30 - Last mean reward per episode: 93.83
Num timesteps: 2442000
Best mean reward: 94.30 - Last mean reward per episode: 93.89
Num timesteps: 2443000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 2444000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 2445000
Best mean reward: 94.30 - Last mean reward per episode: 93.81
Num timesteps: 2446000
Best mean reward: 94.30 - Last mean reward per episode: 93.82
Num timesteps: 2447000
Best mean reward: 94.30 - Last mean reward per episode: 93.78
Num timesteps: 2448000
Best mean reward: 94.30 - Last mean reward per episode: 93.77
Num timesteps: 2449000
Best mean reward: 94.30 - Last mean reward per episode: 93.73
Num timesteps: 2450000
Best mean reward: 94.30 - Last mean reward per episode: 93.69
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 5.16     |
| reference_action_mean   | -0.192   |
| reference_action_std    | 0.969    |
| reference_actor_Q_mean  | 51.9     |
| reference_actor_Q_std   | 5.28     |
| rollout/Q_mean          | 63.5     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.29e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.7     |
| total/duration          | 6.43e+03 |
| total/episodes          | 2.29e+04 |
| total/epochs            | 1        |
| total/steps             | 2449998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.339    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2451000
Best mean reward: 94.30 - Last mean reward per episode: 93.44
Num timesteps: 2452000
Best mean reward: 94.30 - Last mean reward per episode: 93.51
Num timesteps: 2453000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2454000
Best mean reward: 94.30 - Last mean reward per episode: 93.54
Num timesteps: 2455000
Best mean reward: 94.30 - Last mean reward per episode: 93.52
Num timesteps: 2456000
Best mean reward: 94.30 - Last mean reward per episode: 93.41
Num timesteps: 2457000
Best mean reward: 94.30 - Last mean reward per episode: 93.19
Num timesteps: 2458000
Best mean reward: 94.30 - Last mean reward per episode: 93.11
Num timesteps: 2459000
Best mean reward: 94.30 - Last mean reward per episode: 92.94
Num timesteps: 2460000
Best mean reward: 94.30 - Last mean reward per episode: 93.16
--------------------------------------
| reference_Q_mean        | 51.7     |
| reference_Q_std         | 5.14     |
| reference_action_mean   | 0.334    |
| reference_action_std    | 0.931    |
| reference_actor_Q_mean  | 52.2     |
| reference_actor_Q_std   | 5.11     |
| rollout/Q_mean          | 63.5     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.3e+04  |
| rollout/return          | 91.5     |
| rollout/return_history  | 93.2     |
| total/duration          | 6.46e+03 |
| total/episodes          | 2.3e+04  |
| total/epochs            | 1        |
| total/steps             | 2459998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -69.9    |
| train/loss_critic       | 0.41     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2461000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 2462000
Best mean reward: 94.30 - Last mean reward per episode: 93.08
Num timesteps: 2463000
Best mean reward: 94.30 - Last mean reward per episode: 91.28
Num timesteps: 2464000
Best mean reward: 94.30 - Last mean reward per episode: 91.20
Num timesteps: 2465000
Best mean reward: 94.30 - Last mean reward per episode: 91.22
Num timesteps: 2466000
Best mean reward: 94.30 - Last mean reward per episode: 91.09
Num timesteps: 2467000
Best mean reward: 94.30 - Last mean reward per episode: 91.26
Num timesteps: 2468000
Best mean reward: 94.30 - Last mean reward per episode: 91.23
Num timesteps: 2469000
Best mean reward: 94.30 - Last mean reward per episode: 91.27
Num timesteps: 2470000
Best mean reward: 94.30 - Last mean reward per episode: 91.28
--------------------------------------
| reference_Q_mean        | 51.5     |
| reference_Q_std         | 5.56     |
| reference_action_mean   | 0.113    |
| reference_action_std    | 0.978    |
| reference_actor_Q_mean  | 52.1     |
| reference_actor_Q_std   | 5.54     |
| rollout/Q_mean          | 63.5     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.31e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 91.3     |
| total/duration          | 6.49e+03 |
| total/episodes          | 2.31e+04 |
| total/epochs            | 1        |
| total/steps             | 2469998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -69.1    |
| train/loss_critic       | 0.518    |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2471000
Best mean reward: 94.30 - Last mean reward per episode: 91.09
Num timesteps: 2472000
Best mean reward: 94.30 - Last mean reward per episode: 92.86
Num timesteps: 2473000
Best mean reward: 94.30 - Last mean reward per episode: 92.79
Num timesteps: 2474000
Best mean reward: 94.30 - Last mean reward per episode: 91.32
Num timesteps: 2475000
Best mean reward: 94.30 - Last mean reward per episode: 90.94
Num timesteps: 2476000
Best mean reward: 94.30 - Last mean reward per episode: 90.86
Num timesteps: 2477000
Best mean reward: 94.30 - Last mean reward per episode: 90.69
Num timesteps: 2478000
Best mean reward: 94.30 - Last mean reward per episode: 90.87
Num timesteps: 2479000
Best mean reward: 94.30 - Last mean reward per episode: 89.16
Num timesteps: 2480000
Best mean reward: 94.30 - Last mean reward per episode: 89.21
--------------------------------------
| reference_Q_mean        | 51       |
| reference_Q_std         | 5.91     |
| reference_action_mean   | 0.269    |
| reference_action_std    | 0.954    |
| reference_actor_Q_mean  | 51.6     |
| reference_actor_Q_std   | 5.99     |
| rollout/Q_mean          | 63.5     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.32e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 89.2     |
| total/duration          | 6.51e+03 |
| total/episodes          | 2.32e+04 |
| total/epochs            | 1        |
| total/steps             | 2479998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -68      |
| train/loss_critic       | 0.56     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2481000
Best mean reward: 94.30 - Last mean reward per episode: 89.14
Num timesteps: 2482000
Best mean reward: 94.30 - Last mean reward per episode: 88.94
Num timesteps: 2483000
Best mean reward: 94.30 - Last mean reward per episode: 88.71
Num timesteps: 2484000
Best mean reward: 94.30 - Last mean reward per episode: 88.70
Num timesteps: 2485000
Best mean reward: 94.30 - Last mean reward per episode: 88.42
Num timesteps: 2486000
Best mean reward: 94.30 - Last mean reward per episode: 88.42
Num timesteps: 2487000
Best mean reward: 94.30 - Last mean reward per episode: 88.50
Num timesteps: 2488000
Best mean reward: 94.30 - Last mean reward per episode: 88.73
Num timesteps: 2489000
Best mean reward: 94.30 - Last mean reward per episode: 87.28
Num timesteps: 2490000
Best mean reward: 94.30 - Last mean reward per episode: 87.06
--------------------------------------
| reference_Q_mean        | 50.8     |
| reference_Q_std         | 6.19     |
| reference_action_mean   | -0.0628  |
| reference_action_std    | 0.984    |
| reference_actor_Q_mean  | 51.5     |
| reference_actor_Q_std   | 6.29     |
| rollout/Q_mean          | 63.5     |
| rollout/actions_mean    | 0.149    |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.33e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 87.1     |
| total/duration          | 6.54e+03 |
| total/episodes          | 2.33e+04 |
| total/epochs            | 1        |
| total/steps             | 2489998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -67.7    |
| train/loss_critic       | 1.34     |
| train/param_noise_di... | 0        |
--------------------------------------

Num timesteps: 2491000
Best mean reward: 94.30 - Last mean reward per episode: 87.16
Num timesteps: 2492000
Best mean reward: 94.30 - Last mean reward per episode: 89.00
Num timesteps: 2493000
Best mean reward: 94.30 - Last mean reward per episode: 89.16
Num timesteps: 2494000
Best mean reward: 94.30 - Last mean reward per episode: 89.34
Num timesteps: 2495000
Best mean reward: 94.30 - Last mean reward per episode: 89.67
Num timesteps: 2496000
Best mean reward: 94.30 - Last mean reward per episode: 90.13
Num timesteps: 2497000
Best mean reward: 94.30 - Last mean reward per episode: 90.13
Num timesteps: 2498000
Best mean reward: 94.30 - Last mean reward per episode: 93.24
Num timesteps: 2499000
Best mean reward: 94.30 - Last mean reward per episode: 92.98
Num timesteps: 2500000
Best mean reward: 94.30 - Last mean reward per episode: 92.96
--------------------------------------
| reference_Q_mean        | 50.3     |
| reference_Q_std         | 6.16     |
| reference_action_mean   | -0.212   |
| reference_action_std    | 0.966    |
| reference_actor_Q_mean  | 51       |
| reference_actor_Q_std   | 6.26     |
| rollout/Q_mean          | 63.6     |
| rollout/actions_mean    | 0.15     |
| rollout/actions_std     | 0.82     |
| rollout/episode_steps   | 107      |
| rollout/episodes        | 2.34e+04 |
| rollout/return          | 91.5     |
| rollout/return_history  | 93       |
| total/duration          | 6.57e+03 |
| total/episodes          | 2.34e+04 |
| total/epochs            | 1        |
| total/steps             | 2499998  |
| total/steps_per_second  | 381      |
| train/loss_actor        | -67.3    |
| train/loss_critic       | 1.46     |
| train/param_noise_di... | 0        |
--------------------------------------

